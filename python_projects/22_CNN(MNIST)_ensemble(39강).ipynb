{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "[CPU_ENV]",
      "language": "python",
      "name": "cpu_env"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.9"
    },
    "colab": {
      "name": "22_CNN(MNIST)_ensemble(39강).ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "bK62WY0Wfchb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "## ※ contrib.은 접어라(버전은 니가 업데이트해라)\n",
        "import tensorflow.compat.v1 as tf\n",
        "tf.disable_v2_behavior()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "id": "hSRumuAlfYqe",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "## 이전 강의 예제 복습 \n",
        "## 강사님 예제풀이 - 기존 설명위주의 코드에서 좀 더 유려한 것으로 \n",
        "## 지금까지 배운 내용을 기초로 tensorflow의 mnist example을 CNN으로 구현해보자 \n",
        "from tensorflow.examples.tutorials.mnist import input_data\n",
        "import pandas as pd\n",
        "import numpy as np \n",
        "\n",
        "# Data Loading \n",
        "mnist = input_data.read_data_sets(\"./data/mnist\", one_hot=True)\n",
        "\n",
        "## 전처리 단계(결측치, 이상치, 정규화, feature engineering)\n",
        "## mnist 예제에서는 이미 전처리가 끝난 상태여서 따로 할 건 없어요! \n",
        "\n",
        "# Model 정의 (Tensorflow graph 생성)\n",
        "tf.reset_default_graph()\n",
        "\n",
        "# 2.1 placeholder\n",
        "X = tf.placeholder(shape=[None,784], dtype=tf.float32)\n",
        "Y = tf.placeholder(shape=[None,10], dtype=tf.float32)\n",
        "#※ tensorflow 버전에 따라서 dropout을 사용하는 속성이 변경 \n",
        "#  최신버전(버리는쪽) -> rate =0.8 : drop 시키는 노드의 비율 \n",
        "#  이전버전(살리는쪽) -> keep_prob = 0.8 : 살리는 노드의 비율  \n",
        "keep_rate = tf.placeholder(dtype=tf.float32)\n",
        "\n",
        "# 2.2 Convolution Layer \n",
        "# 입력데이터는 4차원 \n",
        "x_img = tf.reshape(X,[-1,28,28,1])\n",
        "L1 = tf.layers.conv2d(inputs=x_img, filters=32,\n",
        "                      kernel_size = [3,3], \n",
        "                      padding = \"SAME\", #기존 적었던 내용들을 함수의 인자들로\n",
        "                      strides = 1, \n",
        "                      activation =tf.nn.relu)   \n",
        "L1 = tf.layers.max_pooling2d(inputs = L1, \n",
        "                             pool_size=[2,2],\n",
        "                             padding = \"SAME\", \n",
        "                             strides=2)\n",
        "L1 = tf.layers.dropout(inputs = L1, rate= 1- keep_rate)\n",
        "'''이하 주석와 동일\n",
        "W1 = tf.Variable(tf.random_normal([3,3,1,32])) #채널수(1)는 항상 일치시켜야 conv이 발생한다 \n",
        "\n",
        "L1 = tf.nn.conv2d(x_img,W1,strides=[1,1,1,1], \n",
        "                  padding = \"SAME\") #합성곱 연산단계\n",
        "L1 = tf.nn.relu(L1) #relu단계 \n",
        "L1 = tf.nn.max_pool(L1, ksize=[1,2,2,1], strides=[1,2,2,1],\n",
        "                    padding=\"SAME\") #pooling 단계\n",
        "'''\n",
        "\n",
        "\n",
        "L2 = tf.layers.conv2d(inputs=L1, filters=64,\n",
        "                      kernel_size = [3,3], padding = \"SAME\", #기존 적었던 내용들을 함수의 인자들로\n",
        "                      strides = 1, activation =tf.nn.relu)   \n",
        "L2= tf.layers.max_pooling2d(inputs = L2, pool_size=[2,2],\n",
        "                             padding = \"SAME\", strides=2)\n",
        "L2= tf.layers.dropout(inputs = L2, rate= 1 - keep_rate)\n",
        "'''이하 주석과 동일 \n",
        "W2 = tf.Variable(tf.random_normal([3,3,32,64])) #32는 받는것이므로 고정, 64는 내가 정한 임의값 \n",
        "\n",
        "L2 = tf.nn.conv2d(L1,W2,strides=[1,1,1,1], \n",
        "                  padding = \"SAME\") #합성곱 연산단계\n",
        "L2 = tf.nn.relu(L2) #relu단계 \n",
        "L1 = tf.nn.max_pool(L2, ksize=[1,2,2,1], strides=[1,2,2,1],\n",
        "                    padding=\"SAME\") #pooling 딘계\n",
        "'''\n",
        "\n",
        "# 2.3 FC layer (=dense layer)\n",
        "# dense 함수 설정하면 세줄짜리 코드를 한줄분량으로 설정 가능 \n",
        "L2 = tf.reshape(L2,[-1,7*7*64])\n",
        "dense1 = tf.layers.dense(inputs = L2, units=256, \n",
        "                         activation = tf.nn.relu)\n",
        "dense1 = tf.layers.dropout(inputs=dense1, rate= 1 - keep_rate)\n",
        "\n",
        "dense2= tf.layers.dense(inputs = dense1, units=128, \n",
        "                         activation = tf.nn.relu)\n",
        "dense2= tf.layers.dropout(inputs=dense2, rate= 1 - keep_rate)\n",
        "\n",
        "dense3 = tf.layers.dense(inputs = dense2, units=512, \n",
        "                         activation = tf.nn.relu)\n",
        "dense3 = tf.layers.dropout(inputs=dense3, rate= 1 - keep_rate)\n",
        "'''이하 주석과 동일 \n",
        "W3 = tf.get_variable(\"weight3\", shape=[7*7*64, 256],\n",
        "                    initializer = tf.contrib.layers.xavier_initializer())#Xavier쓸거니깐 get_variable선언\n",
        "b3 = tf.Variable(tf.random_normal([256]), name=\"bias3\")\n",
        "_layer1 = tf.nn.relu(tf.matmul(L2,W3) + b3)\n",
        "layer1 = tf.nn.dropout(_layer1, keep_prob = dropout_rate)\n",
        "\n",
        "W4 = tf.get_variable(\"weight4\", shape=[256,512],\n",
        "                    initializer = tf.contrib.layers.xavier_initializer())\n",
        "b4 = tf.Variable(tf.random_normal([512]), name=\"bias4\")\n",
        "_layer2 = tf.nn.relu(tf.matmul(layer1,W4) + b3)\n",
        "layer2 = tf.nn.dropout(_layer2, keep_prob = dropout_rate)\n",
        "'''\n",
        "\n",
        "#Hypothesis \n",
        "H = tf.layers.dense(inputs = dense3, units=10)\n",
        "'''이하 주석과 동일                     \n",
        "W5 = tf.get_variable(\"weight5\", shape=[512,10],\n",
        "                    initializer = tf.contrib.layers.xavier_initializer())\n",
        "b5 = tf.Variable(tf.random_normal([10]), name=\"bias5\")\n",
        "\n",
        "logit = tf.matmul(layer2, W5) + b5\n",
        "H = tf.nn.softmax(logit)\n",
        "'''\n",
        "                    \n",
        "#Cost\n",
        "cost = tf.losses.softmax_cross_entropy(Y,H)\n",
        "'''이하 주석과 동일 \n",
        "cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits_v2(logits= logit\n",
        "                                                                 labels= Y))\n",
        "'''\n",
        "\n",
        "#train\n",
        "train = tf.train.AdamOptimizer(learning_rate=0.001).minimize(cost)\n",
        "\n",
        "#session 초기화 \n",
        "sess = tf.Session()\n",
        "sess.run(tf.global_variables_initializer())\n",
        "\n",
        "#학습 \n",
        "num_of_epoch = 30 \n",
        "batch_size = 100 \n",
        "\n",
        "for step in range(num_of_epoch):\n",
        "    num_of_iter = int(mnist.train.num_examples / batch_size)\n",
        "    cost_val = 0 # 코스트 초기화(국룰)\n",
        "    \n",
        "    for i in range(num_of_iter):\n",
        "        batch_x,batch_y = mnist.train.next_batch(batch_size)\n",
        "        _, cost_val = sess.run([train,cost], feed_dict={X:batch_x, \n",
        "                                                        Y:batch_y,\n",
        "                                                        keep_rate:0.7 })\n",
        "    if step % 3 == 0 :\n",
        "        print(\"Cost는 : {}\".format(cost_val))\n",
        "\n",
        "#정확도\n",
        "predict = tf.argmax(H,1)\n",
        "correct = tf.equal(predict, tf.argmax(Y,1))\n",
        "accuracy = tf.reduce_mean(tf.cast(correct, dtype=tf.float32))\n",
        "\n",
        "print(\"정확도는 : {}\".format(sess.run(accuracy, \n",
        "                                   feed_dict={X:mnist.test.images, \n",
        "                                              Y:mnist.test.labels,\n",
        "                                              keep_rate:1.0})))\n",
        "#prediction\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KfotPetpfYqk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "###Ensemble(앙상블)\n",
        "## 결국 우리의 MNIST예제는 multinomial 예제\n",
        "## 이미지 1개에 대한 예측값이 H의 도출값은 [0.5, 0.3, 0.2, 0.001, 0.99, 0.44 ..] \n",
        "## 원소를 다 더하면 1이 나오는 예측값. \n",
        "\n",
        "## ensenble은 모델을 여러개 만든다. (10개의 모델)\n",
        "## 이미지 1개에 대한 각 모델의 예측값 \n",
        "## H1 -> [0.5, 0.3, 0.2, 0.001, 0.99, 0.44 ...]\n",
        "## H3 -> [0.4, 0.2, 0.3, 0.011, 0.90, 0.64 ...]\n",
        "## H3 -> [0.8, 0.3, 0.4, 0.005, 0.78, 0.84 ...]\n",
        "## Hn -> [...]\n",
        "\n",
        "## SUM -> [1.7, 0.8, 0.9, 0.017, ...]\n",
        "## 동일한 학습 알고리즘을 사용해 여러(두 가지 이상)모델을 학습하는 형태 \n",
        "## 한 사람의 전문가한테 의견을 구하는 게 아니라 \n",
        "## 여러사람의 전문가에게 의견을 구해서 가장 그럴듯한 대답을 찾아내는 방식 \n",
        "\n",
        "## 앞선 mnist multinomial 을 재활용해보자! "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p5BYNqR9vM7k",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "## 다완's\n",
        "##tensorflow의 mnist example을 CNN으로 구현해보자 \n",
        "from tensorflow.examples.tutorials.mnist import input_data\n",
        "import pandas as pd\n",
        "import numpy as np \n",
        "\n",
        "# Data Loading \n",
        "mnist = input_data.read_data_sets(\"./data/mnist\", one_hot=True)\n",
        "\n",
        "## 전처리 단계(결측치, 이상치, 정규화, feature engineering)\n",
        "## mnist 예제에서는 이미 전처리가 끝난 상태여서 따로 할 건 없어요! \n",
        "\n",
        "# Model 정의 (Tensorflow graph 생성)\n",
        "tf.reset_default_graph()\n",
        "\n",
        "# 2.1 placeholder\n",
        "X = tf.placeholder(shape=[None,784], dtype=tf.float32)\n",
        "Y = tf.placeholder(shape=[None,10], dtype=tf.float32)\n",
        "#※ tensorflow 버전에 따라서 dropout을 사용하는 속성이 변경 \n",
        "#  최신버전(버리는쪽) -> rate =0.8 : drop 시키는 노드의 비율 \n",
        "#  이전버전(살리는쪽) -> keep_prob = 0.8 : 살리는 노드의 비율  \n",
        "keep_rate = tf.placeholder(dtype=tf.float32)\n",
        "\n",
        "# 2.2 Convolution Layer\n",
        "# 입력데이터는 4차원!\n",
        "x_img = tf.reshape(X,[-1,28,28,1])  # 입력데이터 \n",
        "L1 = tf.layers.conv2d(inputs=x_img, filters = 32,\n",
        "                     kernel_size = [3,3],\n",
        "                     padding = 'SAME',\n",
        "                     strides = 1,\n",
        "                     activation = tf.nn.relu)\n",
        "# W1 = tf.Variable(tf.random_normal([3,3,1,32]))                   # 필터\n",
        "# L1 = tf.nn.conv2d(x_img,W1, strides=[1,1,1,1],                   # 합성곱 작업!\n",
        "#                   padding=\"SAME\")\n",
        "# L1 = tf.nn.relu(L1)\n",
        "# 이만큼의 작업을 위처럼 바꾸면 한번에 처리!\n",
        "\n",
        "L1 = tf.layers.max_pooling2d(inputs=L1,\n",
        "                            pool_size = [2,2],\n",
        "                            padding = 'SAME',\n",
        "                            strides = 2)\n",
        "\n",
        "L1 = tf.layers.dropout(inputs=L1, rate = 1-keep_rate)\n",
        "\n",
        "L2 = tf.layers.conv2d(inputs=L1, filters = 64,\n",
        "                     kernel_size = [3,3],\n",
        "                     padding = 'SAME',\n",
        "                     strides = 1,\n",
        "                     activation = tf.nn.relu)\n",
        "L2 = tf.layers.max_pooling2d(inputs=L2,\n",
        "                            pool_size = [2,2],\n",
        "                            padding = 'SAME',\n",
        "                            strides = 2)\n",
        "\n",
        "L2 = tf.layers.dropout(inputs=L2, rate = 1-keep_rate)\n",
        "\n",
        "\n",
        "\n",
        "# # 2.3 FC Layer (Dense Layer)\n",
        "L2 = tf.reshape(L2,[-1,7*7*64])\n",
        "dense1 = tf.layers.dense(inputs = L2, units = 256,\n",
        "                        activation = tf.nn.relu)\n",
        "# dense함수 이용하면 아래 3줄짜리 코드를 위처럼 한 줄로 설정 가능!!\n",
        "\n",
        "# W3 = tf.get_variable('weight3',shape=[7*7*64,256],\n",
        "#                     initializer=tf.contrib.layers.xavier_initializer())\n",
        "# b3 = tf.Variable(tf.random_normal([256]),name = 'bias3')\n",
        "# _layer1 = tf.nn.relu(tf.matmul(L2,W3)+b3)\n",
        "dense1 = tf.layers.dropout(inputs = dense1, rate = 1-keep_rate)\n",
        "# layer1 = tf.nn.dropout(_layer1, keep_prob=keep_rate)\n",
        "\n",
        "dense2 = tf.layers.dense(inputs = dense1, units = 128,\n",
        "                        activation = tf.nn.relu)\n",
        "dense2 = tf.layers.dropout(inputs = dense2, rate = 1-keep_rate)\n",
        "\n",
        "dense3 = tf.layers.dense(inputs = dense2, units = 512,\n",
        "                        activation = tf.nn.relu)\n",
        "dense3 = tf.layers.dropout(inputs = dense3, rate = 1-keep_rate)\n",
        "# W4 = tf.get_variable('weight4',shape=[256,512],\n",
        "#                     initializer=tf.contrib.layers.xavier_initializer())\n",
        "# b4 = tf.Variable(tf.random_normal([512]),name='bias4')\n",
        "\n",
        "# Hypothesis\n",
        "H = tf.layers.dense(inputs= dense3, units = 10)\n",
        "# Cost\n",
        "cost = tf.losses.softmax_cross_entropy(Y,H)  # 순서 바뀐거 중요!!\n",
        "\n",
        "train = tf.train.AdamOptimizer(learning_rate = 0.001).minimize(cost)\n",
        "\n",
        "# 초기화\n",
        "sess = tf.Session()\n",
        "sess.run(tf.global_variables_initializer())\n",
        "\n",
        "# 학습 ( 데이터 사이즈가 커졌기 때문에 batch 필요! )\n",
        "\n",
        "num_of_epoch = 30\n",
        "batch_size = 100\n",
        "\n",
        "for step in range(num_of_epoch):\n",
        "    num_of_iter = int(mnist.train.num_examples / batch_size)\n",
        "    cost_val = 0\n",
        "    for i in range(num_of_iter):\n",
        "        batch_x,batch_y = mnist.train.next_batch(batch_size)\n",
        "        _,cost_val = sess.run([train, cost],feed_dict = {X:batch_x,\n",
        "                                                         Y:batch_y,\n",
        "                                                        keep_rate:0.7})\n",
        "    if step % 3 == 0:\n",
        "        print('cost : {}'.format(cost_val))\n",
        "        \n",
        "# predition, accuracy측정\n",
        "\n",
        "predict = tf.argmax(H,1)\n",
        "correct = tf.equal(predict,tf.argmax(Y,1))\n",
        "accuracy = tf.reduce_mean(tf.cast(correct,dtype=tf.float32))\n",
        "\n",
        "print('정확도 ;{}'.format(sess.run(accuracy,feed_dict={X: mnist.test.images,\n",
        "                                                   Y:mnist.test.labels,\n",
        "                                                   keep_rate:1.0})))\n",
        "\n",
        "\n",
        "#ensamble(10번)\n",
        "for i in range(10):\n",
        "    tf.reset_default_graph() #런\n",
        "    num_of_epoch = 30\n",
        "    batch_size = 100\n",
        "    \n",
        "    for step in range(num_of_epoch):\n",
        "            num_of_iter = int(mnist.train.num_examples / batch_size)\n",
        "            cost_val = 0\n",
        "            for j in range(num_of_iter):\n",
        "                batch_x, batch_y = mnist.train.next_batch(batch_size)\n",
        "                _,H_val,cost_val = sess.run([train,H,cost], feed_dict={X : batch_x,\n",
        "                                                       Y : batch_y, \n",
        "                                                       keep_rate : 0.7})\n",
        "    print(cost_val)    \n",
        "    H_val = sess.run(H, feed_dict={X: mnist.test.images})\n",
        "    if i == 0:\n",
        "        H_result = H_val\n",
        "    else:\n",
        "        H_result = np.add(H_result,H_val)\n",
        "\n",
        "predict = np.argmax(H_result,1)\n",
        "Y = mnist.test.labels\n",
        "correct = np.equal(predict, np.argmax(Y,1))\n",
        "accuracy = np.mean(correct)\n",
        "print(\"앙상블 정확도:\", accuracy)"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}