{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### 딥러닝을 시행하기 위해 GPU를 사용해보자 \n",
    "## 지금 사용하고 있는 tensorflow는 cpu 버전 \n",
    "## deep learning으로 가면 더 많은 수학적 연산(matrix 연산)이 필요하다\n",
    "## GPU를 이용하면 더 빠른 계산이 가능하다 \n",
    "## 새로운 가상환경을 만들어서 tensorflow GPU버전을 이용해 보자 (NVIDIA GeForce)\n",
    "## CPU_ENV -> GPU_ENV\n",
    "\n",
    "## 1. 새로운 가상환경을 하나 생성해야 해요\n",
    "## conda create -n gpu_env python=3.6 openssl\n",
    "## 2. 새로운 가상환경 실행 \n",
    "## activate gpu_env \n",
    "## 3. nb_conda 설치 \n",
    "## conda install nb_conda \n",
    "## 4. python -m ipykernel install --user --name=gpu_env --display-name=[GPU_ENV]\n",
    "## Installed kernelspec gpu_env in C:\\Users\\student\\AppData\\Roaming\\jupyter\\kernels\\gpu_env \n",
    "## 이런식으로 뜨면 OK\n",
    "## 5. 최신 비디오 드라이버 설치 \n",
    "## 441.87-desktop-win10-64bit-international-whql.exe \n",
    "## 6. cuda 설치(NVIDIA)\n",
    "## cuda_10.0.130_411.31_win10.exe 실행\n",
    "## 7. cuDNN 압축풀어서 덮어쓰기(cuda설치된 경로에)\n",
    "## 8. pip install pandas\n",
    "## pip install numpy \n",
    "## pip install matplotlib \n",
    "## pip install tensorflow--gpu==1.15"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 개념 복습\n",
    "## 1. learning rate \n",
    "## 정해져 있는 기준은 없다! \n",
    "## 통상 0.01을 기준으로 cost값을 보고 learning rate 를 조절 \n",
    "## weight = (learning rate * 기울기)\n",
    "## 만약 learning rate 가 크다면 ? → overshooting(발산) 현상이 발생(이차함수 반대편으로 널뛰기)\n",
    "## 만약 learning rate 가 작다면 ? → local minimum(극소) 문제가 발생(바닥인줄 알았는데 옆에 더 깊은 바닥이)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 2. 입력데이터의 preprocessing \n",
    "## feature engineering을 포함해서 각 feature의 데이터의 범주와 크기를 살펴보아야 한다. \n",
    "## 정규화(Normalization) - MinMaxScale을 이용한다. (최대최소를 이용해서 0~1사이로 스케일링) \n",
    "##  -> 데이터를 학습에 용이한 형태로 \n",
    "## 표준화(Standardization) - 평균과 표준편차를 이용해서 -1~1사이의 값으로 scale하는 방식 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 3. overfitting \n",
    "## 모델을 만들어서 학습을 하는데 학습데이터에 너무 잘 들어맞는 모델이 생성되는 경우 (재현성 저해)\n",
    "## 실제 데이터를 적용할 때 결과값 예측이 잘 안되는 경우를 의미 \n",
    "## overfitting을 피하려면 \n",
    "## 1) 많은 training data set이 있어야 한다. (모든 문제는 데이터가 없어서) \n",
    "## 2) column의 수(=feature의 수)를 가능한 줄여햐 한다. -> 중복, 또는 필요없는 col을 삭제 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 4. 학습과정 \n",
    "## 일반적으로 training data set의 크기가 굉장히 크다 \n",
    "## -> 1 epoch을 수행하는 시간이 오래걸린다 \n",
    "## batch 처리를 이용해서 실제 학습을 진행 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 5. 정확도 측정 \n",
    "## 일반적으로 raw data set을 우리가 얻게되면 training data set, test data set으로 분리(7:3. 8:2)\n",
    "## 평가가 되지 않으면 잘 만들었는지 알 도리가 없다! 반드시 시행 \n",
    "## n fold cross validation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 기술적 화두 \n",
    "## 소프트웨어 측면 - 인공지능(AI)\n",
    "## 하드웨어 측면 - 양자이론, 양자컴퓨터 \n",
    "\n",
    "## 인공지능은 CS에서 궁극의 목표 중 하나 (경제, 의료 문화 등 우리가 문제시 했던 대부분의 대상들에 해결이 가능함)\n",
    "## 문제도 많다! -> 대표적으로 일자리문제(20년만 지나면 일자리가 절반으로 줄어들 것이다라는 예측)\n",
    "\n",
    "## 빅뱅의 시작을 1년으로 잡으면..\n",
    "## 인류의 탄생은 2일전\n",
    "## 산업혁명은 2초전 \n",
    "## 기술의 발전속도는 기하급수적으로 증가하고 있다. \n",
    "## 언젠가는 우리가 만드는 프로그램이 사람의 지능을 앞서는 순간이 올거라고 예측 할 수 있다. \n",
    "## -> 이 시점을 특이점(Singularity) 라고 지칭 \n",
    "## 그 시점을 사람들이 예측해보건데,, 약 2045 년 예측 \n",
    "## 많은 학자들 중 일부는 특이점이 오는 시기가 인류가 멸망하는 시기라고도 함\n",
    "\n",
    "## 프로그래밍을 통해서 AI를 제어할 수 있을까? \n",
    "## 뇌과학자 -> AI가 개발이 되면 인공지능은 전자회로 속도로 학습을 하고 사람은 생화학적 회로로 학습(100만배차이)\n",
    "## ex/ MIT의 AI개발자들이 만약 인공지능이 만들어지면 인공지능이 1주일동안 할 수 있을 MIT AI 개발자들이 2만년정도 걸림 \n",
    "## 즉, 어떤 회사가 시장에 1주일이라도 먼저 인공지능을 내놓는다면 2만년의 시간가치를 얻음 \n",
    "## 일론머스크는 AI가 핵무기보다 위험하다고 말함 \n",
    "\n",
    "## 현 시점에 가장 빠른 슈퍼컴퓨터가 미국 - IBM이 만든 서밋(summit)\n",
    "## 농구코트 2배정도되는 크기에 캐비넷 깔아놓고 그 안에 컴퓨터를 가득 채워넣은 크기 (고성능 cpu 9200개, gpu 27000개)\n",
    "## 우주개발 시뮬레이션, 기후 예측 등의 목적으로 쓰임 \n",
    "## 작년 10월 23일, 구글이 양자우위를 달성했다고 발표(nature) : 기존 슈퍼컴퓨터의 연산속도를 압도한다고 발표 \n",
    "## 54개의 큐비트를 탑재 시커모어(Sycamore)라는 양자 컴퓨터용 프로세서 -> 서밋을 압살한다고 발표 \n",
    "## 단, 굉장히 국한된 특정 분야에만 국한된 연산에나 해당 (ex/ 인간의 게놈지도 연구)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 인공지능(AI)\n",
    "## CS 분야에서 궁극의 목표 중 하나 -> 1960년대부터 꾸준히 연구 개발 \n",
    "## 인간의 뇌를 연구하기 시작 : 입력데이터 -> 뉴런의 가중치를 줘서 받아들이는(활성화되는) 부분\n",
    "## 인체가 받아들이는 수 많은 입력데이터는 가중치를 통해 시냅스에 합쳐진다 \n",
    "## 이후 activation 여부 결정 (다음 뉴런으로 데이터를 전파할지 말지)\n",
    "## 이런 뇌와 신경세포의 매커니즘을 답습해서 '퍼셉트론(Perceptron)'의 개념 생성(가상의 학습시계)\n",
    "## 퍼셉트론(뉴런의 동작방식) 1960년대 처음 대두 (여러개의 퍼셉트론을 유기적으로 잘 연결하면 인공지능을 만들 수 있지 않을까 ?\n",
    "## 1958년에 퍼셉트론을 모델링한 기계를 실제로 구현암(뉴욕타입즈에 기사 실림) . \n",
    "## -> 조금만 있으면 스스로 말하고, 듣고, 쓰고, 창조가 가능한 프로그램을 만다"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\ny_data = [[0],\\n          [1],\\n          [1],\\n          [1]]\\n'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## AND / OR에 대한 logistic regression -> percpetron\n",
    "## 진리표를 이용한 학습 \n",
    "## 여기서 특정 게이트에 대한 제한 발견 \n",
    "import tensorflow as tf\n",
    "\n",
    "\n",
    "# XOR 연산 진리표\n",
    "x_data = [[0,0],\n",
    "          [0,1],\n",
    "          [1,0],\n",
    "          [1,1]]\n",
    "\n",
    "\n",
    "y_data = [[0],\n",
    "          [1],\n",
    "          [1],\n",
    "          [0]]\n",
    "\n",
    "\n",
    "# OR 연산 진리표 \n",
    "'''\n",
    "y_data = [[0],\n",
    "          [1],\n",
    "          [1],\n",
    "          [1]]\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cost값은 : 1.1713627576828003\n",
      "Cost값은 : 0.6937274932861328\n",
      "Cost값은 : 0.693170428276062\n",
      "Cost값은 : 0.693148672580719\n",
      "Cost값은 : 0.6931473016738892\n",
      "Cost값은 : 0.6931471824645996\n",
      "Cost값은 : 0.6931471824645996\n",
      "Cost값은 : 0.6931471824645996\n",
      "Cost값은 : 0.6931471824645996\n",
      "Cost값은 : 0.6931471824645996\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[0.]], dtype=float32)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# tensorflow로 machine learning\n",
    "# placeholder\n",
    "X = tf.placeholder(shape=[None,2], dtype=tf.float32)\n",
    "Y = tf.placeholder(shape=[None,1], dtype=tf.float32)\n",
    "\n",
    "# Weight & bias\n",
    "W = tf.Variable(tf.random_normal([2,1]), name=\"weight\")\n",
    "b = tf.Variable(tf.random_normal([1]), name=\"bias\")\n",
    "\n",
    "# Hypothesis\n",
    "logit = tf.matmul(X,W) + b\n",
    "H = tf.sigmoid(logit)\n",
    "\n",
    "# Cost(Loss) function \n",
    "cost = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(logits=logit, labels=Y))\n",
    "\n",
    "#train\n",
    "optimizer = tf.train.GradientDescentOptimizer(learning_rate = 0.01)\n",
    "train = optimizer.minimize(cost)\n",
    "\n",
    "# session, 초기화\n",
    "sess = tf.Session()\n",
    "sess.run(tf.global_variables_initializer())\n",
    "\n",
    "# 학습 \n",
    "for step in range(30000):\n",
    "    _, cost_val = sess.run([train, cost], feed_dict={ X : x_data, \n",
    "                                                      Y : y_data})\n",
    "    if step % 3000 == 0:\n",
    "        print(\"Cost값은 : {}\".format(cost_val))\n",
    "        \n",
    "predict = tf.cast(H>0.5, dtype = tf.float32)\n",
    "sess.run(predict, feed_dict={X :[[0,0]]}) #[0,1],[1,0],[1,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Perceptron(logistic)으로 AND/OR는 구현이 가능 \n",
    "## XOR(Exclusive OR)는 Perceptron으로 구현이 안되요 \n",
    "## 많은 사람들이 XOR를 어떻게 Perceptron으로 구현할 수 있을 까 고민함 \n",
    "## 1969년에 마빈 민스키라는 사람이 논문을 하나 발표(MIT AI lab창시자) \n",
    "## -> 요지는 XOR연산은 하나의 perceptron 으로 연산이 불가능해(수학적으로 증명)\n",
    "## -> 한개로는 안 되지만 여러개 MLP(multi layer perceptron)로는 가능하다(XOR 논리게이트 검색)\n",
    "## -> 근데 MLP은 학습이 너무 어려워서 지구상에 있는 누구라도 이 학습을 시킬 수 가 없다 \n",
    "## 이후로 AI는 일시적인 침채기(당분간 활활타오르다 아무도 안함)\n",
    "## 당시로서는 학습할 수 있는 방법을 찾기가 너무 어렵다는 것이 문제 \n",
    "## 1974년도 Paul이라는 박사과정 학생이 Backpropagation(역전파) 이라는 알고리즘, 하나의 방법을 고안 \n",
    "## 시간이 더 지나서(접을 수는 없으니) 1982년도에 비슷한 내용으로 발표하나, 또 묻혀버림\n",
    "## 1986년 Hinton 교수가 논문을 발표 -> 그때 다시 주목을 받기 시작함 \n",
    "\n",
    "## 1995년쯤에 BackPropagation 방식이 안되는 건 아니지만 더 복잡한 문제를 학습하는데 어렵다는 사실 발견 \n",
    "## -> 간단한 문제는 해결이 가능하지만 복잡한 문제는 또 해결이 불가능함\n",
    "## 이 시기에 다른 여러가지 알고리즘들이 마구잡이로 쏟아져 나옴 \n",
    "## SVM, 나이브 베이지언, Decision Tree, ... \n",
    "## LECUN교수, 기존의 BackPropagation 보다 다른 알고리즘이 우수하다는 것을 증명하고, perceptron이 또 망함 \n",
    "## 다시 침체기..(흥망성쇠를 거듭)\n",
    "\n",
    "## 캐나다가 국책 연구기관을 설립 (Canadian Institute For Advanced Research (CIFAR) 발음이..)\n",
    "## 국책이기에 현재 돈이 안되더라도 기반기술 개발을 위해 사람들을 끌어모음 <- 거기에 Hinton 교수도 참여 \n",
    "## 1987년 Hinton교수가 캐나다로 건너가서 AI 연구를 지속 \n",
    "## 그리하여 2006, 2007년도에 Hinton교수의 연구진들이 2개의 논문을 발표(망한 이유를 찾았어!)\n",
    "## -> 망한이유 : BackPropagation이 안된 이유를 찾아서 수학적으로 증명 \n",
    "## -> w,b의 초기값을 random으로 주면 안돼(이거 초기값을 잘 주면 학습을 잘 할 수 있어)\n",
    "## -> 2007년, 초기값에 대한 증명에 대한 논문 추가 제출, layer를 더 많이 사용할 수록 복잡한 문제를 해결할 수 있다고 발표\n",
    "\n",
    "## 문제는 사람들이 반응이 차디참!!(두번 속냐)\n",
    "## 신분세탁... 브랜드 라벨을 바꾸어보자 -> '딥 러닝(deeplearning)' 으로 (새로운 기법인 것처럼)\n",
    "## 사실 AI, neural network에 대한 연구는 컴퓨터에 대한 역사와 상응할 만큼 상대적으로 김 \n",
    "\n",
    "##############################\n",
    "\n",
    "## 여러개의 perceptron으로 neural network를 연결해서, 기존에 학습이 안 되던 것을 학습해보자 \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "## XOR연산을 다시 시행해보자! \n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "\n",
    "# XOR 연산 진리표\n",
    "x_data = [[0,0],\n",
    "          [0,1],\n",
    "          [1,0],\n",
    "          [1,1]]\n",
    "\n",
    "\n",
    "y_data = [[0],\n",
    "          [1],\n",
    "          [1],\n",
    "          [0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cost값은 : 0.8071404695510864\n",
      "Cost값은 : 0.6624747514724731\n",
      "Cost값은 : 0.5953502655029297\n",
      "Cost값은 : 0.47640061378479004\n",
      "Cost값은 : 0.3287293314933777\n",
      "Cost값은 : 0.20723536610603333\n",
      "Cost값은 : 0.1327245533466339\n",
      "Cost값은 : 0.09057068824768066\n",
      "Cost값은 : 0.06588168442249298\n",
      "Cost값은 : 0.050473980605602264\n"
     ]
    }
   ],
   "source": [
    "# tensorflow로 machine learning 이어서\n",
    "# placeholder\n",
    "X = tf.placeholder(shape=[None,2], dtype=tf.float32)\n",
    "Y = tf.placeholder(shape=[None,1], dtype=tf.float32)\n",
    "\n",
    "# Weight & bias\n",
    "W1 = tf.Variable(tf.random_normal([2,20]), name=\"weight1\") #내보낼 logistic의 개수 증대(앞에건 설정한 placeholder대로)\n",
    "b1 = tf.Variable(tf.random_normal([20]), name=\"bias1\")\n",
    "layer1 = tf.sigmoid(tf.matmul(X,W1) + b1)\n",
    "\n",
    "W2 = tf.Variable(tf.random_normal([20,1]), name=\"weight2\")\n",
    "b2 = tf.Variable(tf.random_normal([1]), name=\"bias2\")\n",
    "\n",
    "# Hypothesis\n",
    "logit = tf.matmul(layer1,W2) + b2\n",
    "H = tf.sigmoid(logit)\n",
    "\n",
    "# Cost(Loss) function \n",
    "cost = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(logits=logit, labels=Y))\n",
    "\n",
    "#train\n",
    "optimizer = tf.train.GradientDescentOptimizer(learning_rate = 0.01)\n",
    "train = optimizer.minimize(cost)\n",
    "\n",
    "# session, 초기화\n",
    "sess = tf.Session()\n",
    "sess.run(tf.global_variables_initializer())\n",
    "\n",
    "# 학습 \n",
    "for step in range(30000):\n",
    "    _, cost_val = sess.run([train, cost], feed_dict={ X : x_data, \n",
    "                                                      Y : y_data})\n",
    "    if step % 3000 == 0:\n",
    "        print(\"Cost값은 : {}\".format(cost_val))\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1.]], dtype=float32)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#예측\n",
    "predict = tf.cast(H>0.5, dtype = tf.float32)\n",
    "sess.run(predict, feed_dict={X :[[0,1]]}) #[0,1],[1,0],[1,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cost값은 : 1.0553686618804932\n",
      "Cost값은 : 0.0914183259010315\n",
      "Cost값은 : 0.028093203902244568\n",
      "Cost값은 : 0.01488403044641018\n",
      "Cost값은 : 0.009751594625413418\n",
      "Cost값은 : 0.007120845839381218\n",
      "Cost값은 : 0.005549508612602949\n",
      "Cost값은 : 0.004515777807682753\n",
      "Cost값은 : 0.0037890183739364147\n",
      "Cost값은 : 0.0032527174334973097\n"
     ]
    }
   ],
   "source": [
    "## layer를 하나 더 늘여보자  \n",
    "import tensorflow as tf\n",
    "\n",
    "x_data = [[0,0],\n",
    "          [0,1],\n",
    "          [1,0],\n",
    "          [1,1]]\n",
    "y_data = [[0],\n",
    "          [1],\n",
    "          [1],\n",
    "          [0]]\n",
    "\n",
    "# placeholder\n",
    "X = tf.placeholder(shape=[None,2], dtype=tf.float32)\n",
    "Y = tf.placeholder(shape=[None,1], dtype=tf.float32)\n",
    "\n",
    "# Weight, bias\n",
    "W1 = tf.Variable(tf.random_normal([2,10]), name=\"weight1\")\n",
    "# W1 = tf.Variable(tf.random_normal([2,logistic에 들어갈 x 갯수]), name=\"weight1\")\n",
    "b1 = tf.Variable(tf.random_normal([10]), name=\"bias\")\n",
    "# b1 = tf.Variable(tf.random_normal([logistic에 들어갈 x 갯수]]), name=\"bias\")\n",
    "layer1 = tf.sigmoid(tf.matmul(X,W1) + b1)\n",
    "\n",
    "W2 = tf.Variable(tf.random_normal([10,256]), name=\"weight2\")\n",
    "# W2 = tf.Variable(tf.random_normal([logistic에 들어갈 x 갯수,1]), name=\"weight2\")\n",
    "b2 = tf.Variable(tf.random_normal([256]), name=\"bias2\")\n",
    "layer2 = tf.sigmoid(tf.matmul(layer1,W2) + b2)\n",
    "\n",
    "W3 = tf.Variable(tf.random_normal([256,1]), name=\"weight3\")\n",
    "b3 = tf.Variable(tf.random_normal([1]), name=\"bias3\")\n",
    "\n",
    "# Hypothesis\n",
    "logit = tf.matmul(layer2,W3) + b3\n",
    "H = tf.sigmoid(logit)\n",
    "\n",
    "# Cost\n",
    "cost = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(logits=logit,\n",
    "                                                              labels=Y))\n",
    "\n",
    "# train\n",
    "optimizer = tf.train.GradientDescentOptimizer(learning_rate = 0.1)\n",
    "train = optimizer.minimize(cost)\n",
    "\n",
    "# session, 초기화\n",
    "sess= tf.Session()\n",
    "sess.run(tf.global_variables_initializer())\n",
    "\n",
    "# nodes\n",
    "for step in range(3000): #  코스트값을 3000번 줄이겠다 는 뜻\n",
    "    _, cost_val=sess.run([train, cost], feed_dict = {X : x_data,\n",
    "                                                     Y : y_data} ) # 먹이를 줘야지 그데이터가지고 학습해 \n",
    "    if step % 300 == 0:\n",
    "        print(\"Cost값은 : {}\".format(cost_val))\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.]], dtype=float32)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#예측\n",
    "predict = tf.cast(H>0.5, dtype = tf.float32)\n",
    "sess.run(predict, feed_dict={X :[[1,1]]}) #[0,1],[1,0],[1,1]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "[GPU_ENV]",
   "language": "python",
   "name": "gpu_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
