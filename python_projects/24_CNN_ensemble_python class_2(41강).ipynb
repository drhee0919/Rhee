{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"24_CNN_ensemble_python class_2(41강).ipynb","provenance":[],"collapsed_sections":[],"authorship_tag":"ABX9TyPAKZ32whqK1OOxrEXtj1F1"},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"cell_type":"code","metadata":{"id":"KGVZX7O04ACE","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":161},"outputId":"c02564a6-8ecd-40f8-d0f5-3cf4758b65a0","executionInfo":{"status":"ok","timestamp":1579225208197,"user_tz":-540,"elapsed":1095,"user":{"displayName":"hyuck jae rhee","photoUrl":"","userId":"06690120640185291275"}}},"source":["# class에 대해서 조금 만 더 알아보자 \n","# class는 객체를 만들기 위해서 필요한 틀 \n","# class안에 기술된 정보를 바탕으로 일정 메모리 영역을 확봘 수 있고 이 영역이 객체\n","\n","class Car:\n","    # constructor(생성자) -> 함수 \n","    # 객체가 처음 만들어질 때 무조건 호출된다. \n","    # 객체가 가지는 변수를 특정 값으로 초기화 \n","    def __init__(self,name):\n","        print(\"객체가 생성되요\")\n","        # class로무터 파생된 객체가 가질 수 있는 data 는 \n","        # 여기에 명시한다. \n","        #self.car_name = \"소나타\" \n","        self.car_name = name\n","        self.fuel = 100 #연관성을 갖는 객체 하나 선언 \n","                        #객체내 변수와 기능은 연관성이 있다. \n","                        #따로따로 노는 것이 아닌 기능은 변수를 제어하는 용도 \n","                        #(꼭 그런것은 아니지만 거의 예외없이)\n","\n","    # 기능을 위한 함수(여러개 나올 수 있다. )\n","    def go_front(self):\n","        my_car = 100 # self가 있어 함수 안에 국한되는 변수로 인식하지않는다\n","                     # 객체가 갖는 변수로 인식  \n","        print(\"자동차가 전진해요!\") \n","        self.fuel = self.fuel - 5 #self를 안쓴다면 함수내부로 국한됨 \n","        #fuel = fuel - 5          #애초에 이름도 다름 (self.fuel != fuel) \n","\n","    def backward(self):\n","        print(\"자동차가 후진해요!\")\n","        self.fuel = self.fuel - 3  \n","\n","    def get_fuel(self): #잔여 연료량을 알려주는 함수 #self꼭 기재 \n","        return self.fuel \n","\n","\n","\n","\n","car1 = Car(\"소나타\") #car1이 하나의 객체 \n","car1.go_front()\n","car1.backward()\n","print(car1.car_name)\n","#car1.car_name = \"그랜저\"\n","car2 = Car(\"그랜저\") #car2는 하나의 객체 \n","car2.backward() #\".\" -> dot operator(점 연산자): '이 객체가 가지고 있는 기능'으로 쓰임 \n","print(car2.car_name)\n","print(car1.get_fuel())"],"execution_count":17,"outputs":[{"output_type":"stream","text":["객체가 생성되요\n","자동차가 전진해요!\n","자동차가 후진해요!\n","소나타\n","객체가 생성되요\n","자동차가 후진해요!\n","그랜저\n","92\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"2dZ80UENGYzu","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":1000},"outputId":"e800cd99-331e-4b5c-8e2b-83076435b8d9","executionInfo":{"status":"ok","timestamp":1579244470551,"user_tz":-540,"elapsed":605354,"user":{"displayName":"hyuck jae rhee","photoUrl":"","userId":"06690120640185291275"}}},"source":["### CNN(ensemble)을 class를 이용해서 구현해 보아요! \n","## ensemble은 여러개의 CNN model을 만들어서 이용 \n","## 객체를 만들어서 각 개체에 CNN model을 하나씩 저장 \n","\n","import tensorflow as tf \n","from tensorflow.examples.tutorials.mnist import input_data \n","import pandas as pd \n","import numpy as np \n","\n","# 그래프 초기화 \n","tf.reset_default_graph()\n","\n","class CnnModel:\n","    def __init__(self,session, data): #class로 만들시 session을 init 파트에서 실시한다\n","        print(\"객체가 생성되었어요!\")\n","        self.build_graph()\n","        self.sess = session\n","        self.mnist = data \n","\n","    def build_graph(self):\n","        print(\"텐서플로우 그래프를 그려요!\")\n","        # 직접쓰는 코드와 내부에서만 쓰는 코드 구분하여 self를 붙이자 \n","        # 2.1 placeholder\n","        self.X = tf.placeholder(shape=[None,784], dtype=tf.float32)\n","        self.Y = tf.placeholder(shape=[None,10], dtype=tf.float32)\n","        #※ tensorflow 버전에 따라서 dropout을 사용하는 속성이 변경 \n","        #  최신버전(버리는쪽) -> rate =0.8 : drop 시키는 노드의 비율 \n","        #  이전버전(살리는쪽) -> keep_prob = 0.8 : 살리는 노드의 비율  \n","        self.keep_rate = tf.placeholder(dtype=tf.float32)\n","\n","        # 2.2 Convolution Layer \n","        # 입력데이터는 4차원 \n","        x_img = tf.reshape(self.X,[-1,28,28,1])\n","        L1 = tf.layers.conv2d(inputs=x_img, filters=32,\n","                              kernel_size = [3,3], \n","                              padding = \"SAME\", #기존 적었던 내용들을 함수의 인자들로\n","                              strides = 1, \n","                              activation =tf.nn.relu)   \n","        L1 = tf.layers.max_pooling2d(inputs = L1, \n","                                    pool_size=[2,2],\n","                                    padding = \"SAME\", \n","                                    strides=2)\n","        L1 = tf.layers.dropout(inputs = L1, rate= 1- self.keep_rate)\n","\n","\n","\n","        L2 = tf.layers.conv2d(inputs=L1, filters=64,\n","                            kernel_size = [3,3], padding = \"SAME\", #기존 적었던 내용들을 함수의 인자들로\n","                            strides = 1, activation =tf.nn.relu)   \n","        L2= tf.layers.max_pooling2d(inputs = L2, pool_size=[2,2],\n","                                    padding = \"SAME\", strides=2)\n","        L2= tf.layers.dropout(inputs = L2, rate= 1 - self.keep_rate)\n","\n","\n","\n","        # 2.3 FC layer (=dense layer)\n","        # dense 함수 설정하면 세줄짜리 코드를 한줄분량으로 설정 가능 \n","        L2 = tf.reshape(L2,[-1,7*7*64])\n","        dense1 = tf.layers.dense(inputs = L2, units=256, \n","                         activation = tf.nn.relu)\n","        dense1 = tf.layers.dropout(inputs=dense1, rate= 1 - self.keep_rate)\n","\n","\n","        dense2= tf.layers.dense(inputs = dense1, units=128, \n","                         activation = tf.nn.relu)\n","        dense2= tf.layers.dropout(inputs=dense2, rate= 1 - self.keep_rate)\n","\n","\n","        dense3 = tf.layers.dense(inputs = dense2, units=512, \n","                         activation = tf.nn.relu)\n","        dense3 = tf.layers.dropout(inputs=dense3, rate= 1 - self.keep_rate)\n","\n","\n","        #Hypothesis \n","        self.H = tf.layers.dense(inputs = dense3, units=10)\n","\n","                    \n","        #Cost\n","        self.cost = tf.losses.softmax_cross_entropy(self.Y,self.H)\n","\n","        #train\n","        self.train = tf.train.AdamOptimizer(learning_rate=0.001).minimize(self.cost)\n","    \n","    def train_graph(self):\n","        print(\"텐서플로우 그래프를 학습 시켜요\") \n","        #학습\n","        self.sess.run(tf.global_variables_initializer())\n","        num_of_epoch = 30 \n","        batch_size = 100 \n","\n","        for step in range(num_of_epoch):\n","            num_of_iter = int(mnist.train.num_examples / batch_size)\n","            cost_val = 0 # 코스트 초기화(국룰)\n","            \n","            for i in range(num_of_iter):\n","                batch_x,batch_y = mnist.train.next_batch(batch_size)\n","                _, cost_val = self.sess.run([self.train,self.cost], \n","                                            feed_dict={self.X:batch_x, \n","                                                        self.Y:batch_y,\n","                                                        self.keep_rate:0.7 })\n","            if step % 3 == 0 :\n","                print(\"Cost는 : {}\".format(cost_val))\n","\n","    def get_accuracy(self):\n","        print(\"정확도를 구해요!\")\n","        predict = tf.argmax(self.H,1)\n","        correct = tf.equal(predict, tf.argmax(self.Y,1))\n","        accuracy = tf.reduce_mean(tf.cast(correct, dtype=tf.float32))\n","\n","        print(\"정확도는 : {}\".format(sess.run(accuracy, \n","                                        feed_dict={self.X:mnist.test.images, \n","                                                   self.Y:mnist.test.labels,\n","                                                   self.keep_rate:1.0})))\n","    '''\n","    def get_accuracy(self, x_test, y_test, keep_prop=1.0):\n","\n","        return self.sess.run(self.accuracy, feed_dict={self.X: x_test, self.Y: y_test, self.keep_prob: keep_prop})\n","    '''\n","    '''\n","    def get_prediction(self):\n","        print(\"예측값을 구해요!\")\n","        return self.sess.run(self.logits,\n","                             feed_dict = {self.X: \n","                                          self.Y:\n","                                          self.keep_rate: 1.0})\n","    '''\n","# 모델 10개를 만들꺼예요! : 객체 10개를 만들거예요 \n","'''\n","model1= CnnModel()\n","model2 = CnnModel()\n","model3 = CnnModel()\n","model4 = CnnModel()\n","'''\n","\n","# Ensamble\n","sess = tf.Session()\n","mnist = input_data.read_data_sets(\"./data/mnist\", one_hot=True)\n","num_of_model = 10\n","models = [CnnModel(sess, mnist) for x in range(num_of_model)] #CnnModel()를 호출해서 list안에 하나씩 집어넣는다 \n","\n","for i in range(num_of_model):\n","    models[i].train_graph()\n","    models[i].get_accuracy()"],"execution_count":29,"outputs":[{"output_type":"stream","text":["Extracting ./data/mnist/train-images-idx3-ubyte.gz\n","Extracting ./data/mnist/train-labels-idx1-ubyte.gz\n","Extracting ./data/mnist/t10k-images-idx3-ubyte.gz\n","Extracting ./data/mnist/t10k-labels-idx1-ubyte.gz\n","객체가 생성되었어요!\n","텐서플로우 그래프를 그려요!\n","객체가 생성되었어요!\n","텐서플로우 그래프를 그려요!\n","객체가 생성되었어요!\n","텐서플로우 그래프를 그려요!\n","객체가 생성되었어요!\n","텐서플로우 그래프를 그려요!\n","객체가 생성되었어요!\n","텐서플로우 그래프를 그려요!\n","객체가 생성되었어요!\n","텐서플로우 그래프를 그려요!\n","객체가 생성되었어요!\n","텐서플로우 그래프를 그려요!\n","객체가 생성되었어요!\n","텐서플로우 그래프를 그려요!\n","객체가 생성되었어요!\n","텐서플로우 그래프를 그려요!\n","객체가 생성되었어요!\n","텐서플로우 그래프를 그려요!\n","텐서플로우 그래프를 학습 시켜요\n","Cost는 : 0.02375377155840397\n","Cost는 : 0.01411296334117651\n","Cost는 : 0.005204381421208382\n","Cost는 : 0.00036485790042206645\n","Cost는 : 0.045848846435546875\n","Cost는 : 0.0041394843719899654\n","Cost는 : 8.219357550842687e-05\n","Cost는 : 1.89319525816245e-05\n","Cost는 : 6.355872756103054e-05\n","Cost는 : 0.00013122310338076204\n","정확도를 구해요!\n","정확도는 : 0.9915000200271606\n","텐서플로우 그래프를 학습 시켜요\n","Cost는 : 0.0443902388215065\n","Cost는 : 0.022927463054656982\n","Cost는 : 0.00038397058960981667\n","Cost는 : 0.002552641090005636\n","Cost는 : 0.0009932605316862464\n","Cost는 : 0.0003491039969958365\n","Cost는 : 0.006375987082719803\n","Cost는 : 1.7142066326414351e-06\n","Cost는 : 1.03134616438183e-05\n","Cost는 : 0.0002711550914682448\n","정확도를 구해요!\n","정확도는 : 0.991599977016449\n","텐서플로우 그래프를 학습 시켜요\n","Cost는 : 0.035586658865213394\n","Cost는 : 0.011845852248370647\n","Cost는 : 0.0032196915708482265\n","Cost는 : 0.05862199887633324\n","Cost는 : 0.0028201360255479813\n","Cost는 : 5.895931099075824e-05\n","Cost는 : 0.010171054862439632\n","Cost는 : 0.0001014458539430052\n","Cost는 : 0.00024475340615026653\n","Cost는 : 9.300652891397476e-06\n","정확도를 구해요!\n","정확도는 : 0.9929999709129333\n","텐서플로우 그래프를 학습 시켜요\n","Cost는 : 0.02824551798403263\n","Cost는 : 0.016631804406642914\n","Cost는 : 0.03967772796750069\n","Cost는 : 0.000271028809947893\n","Cost는 : 9.813182259676978e-05\n","Cost는 : 0.0004448687541298568\n","Cost는 : 4.6251034291344695e-06\n","Cost는 : 0.0002644271880853921\n","Cost는 : 0.00038803249481134117\n","Cost는 : 3.360647679073736e-05\n","정확도를 구해요!\n","정확도는 : 0.9901999831199646\n","텐서플로우 그래프를 학습 시켜요\n","Cost는 : 0.05974338948726654\n","Cost는 : 0.06869632005691528\n","Cost는 : 0.0022474979050457478\n","Cost는 : 0.010790890082716942\n","Cost는 : 0.004004360176622868\n","Cost는 : 0.0005812530871480703\n","Cost는 : 0.0005966601311229169\n","Cost는 : 0.0005089907208457589\n","Cost는 : 0.000557517574634403\n","Cost는 : 1.091948547582433e-06\n","정확도를 구해요!\n","정확도는 : 0.9904999732971191\n","텐서플로우 그래프를 학습 시켜요\n","Cost는 : 0.14432111382484436\n","Cost는 : 0.013912254944443703\n","Cost는 : 0.0005239625461399555\n","Cost는 : 0.0038887702394276857\n","Cost는 : 4.56629968539346e-05\n","Cost는 : 0.01256822980940342\n","Cost는 : 0.0001873142464319244\n","Cost는 : 0.0013570627197623253\n","Cost는 : 1.1503556379466318e-05\n","Cost는 : 0.012845589779317379\n","정확도를 구해요!\n","정확도는 : 0.9921000003814697\n","텐서플로우 그래프를 학습 시켜요\n","Cost는 : 0.013036783784627914\n","Cost는 : 0.03376634418964386\n","Cost는 : 0.005697780754417181\n","Cost는 : 0.0024729829747229815\n","Cost는 : 0.005954960361123085\n","Cost는 : 0.003412993624806404\n","Cost는 : 0.0031774193048477173\n","Cost는 : 0.00011887565051438287\n","Cost는 : 0.0054054646752774715\n","Cost는 : 1.0472100257175043e-05\n","정확도를 구해요!\n","정확도는 : 0.9925000071525574\n","텐서플로우 그래프를 학습 시켜요\n","Cost는 : 0.07261156290769577\n","Cost는 : 0.0062244851142168045\n","Cost는 : 0.0042891125194728374\n","Cost는 : 0.0030542146414518356\n","Cost는 : 0.0007678244728595018\n","Cost는 : 0.00018927585915662348\n","Cost는 : 4.7262630687328056e-05\n","Cost는 : 0.002303405199199915\n","Cost는 : 0.000342192011885345\n","Cost는 : 0.004565911367535591\n","정확도를 구해요!\n","정확도는 : 0.9914000034332275\n","텐서플로우 그래프를 학습 시켜요\n","Cost는 : 0.023801157251000404\n","Cost는 : 0.04548496752977371\n","Cost는 : 0.06883437186479568\n","Cost는 : 0.0018349338788539171\n","Cost는 : 0.027591116726398468\n","Cost는 : 0.006295634899288416\n","Cost는 : 1.884634912130423e-05\n","Cost는 : 0.0008418938377872109\n","Cost는 : 0.0017658867873251438\n","Cost는 : 6.298003427218646e-05\n","정확도를 구해요!\n","정확도는 : 0.9908000230789185\n","텐서플로우 그래프를 학습 시켜요\n","Cost는 : 0.10380223393440247\n","Cost는 : 0.001716799451969564\n","Cost는 : 0.10197648406028748\n","Cost는 : 0.0002010197058552876\n","Cost는 : 0.005678245797753334\n","Cost는 : 0.0008666805224493146\n","Cost는 : 0.0013704708544537425\n","Cost는 : 0.003919633105397224\n","Cost는 : 1.8353533960180357e-05\n","Cost는 : 0.03422518074512482\n","정확도를 구해요!\n","정확도는 : 0.9916999936103821\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"u3njsXOqI170","colab_type":"code","colab":{}},"source":["class Model:\n","\n","\n","\n","    def __init__(self, sess, name):\n","\n","        self.sess = sess\n","\n","        self.name = name\n","\n","        self._build_net()\n","\n","\n","\n","    def _build_net(self):\n","\n","        with tf.variable_scope(self.name):\n","\n","            # dropout (keep_prob) rate  0.7~0.5 on training, but should be 1\n","\n","            # for testing\n","\n","            self.keep_prob = tf.placeholder(tf.float32)\n","\n","\n","\n","            # input place holders\n","\n","            self.X = tf.placeholder(tf.float32, [None, 784])\n","\n","            # img 28x28x1 (black/white)\n","\n","            X_img = tf.reshape(self.X, [-1, 28, 28, 1])\n","\n","            self.Y = tf.placeholder(tf.float32, [None, 10])\n","\n","\n","\n","            # L1 ImgIn shape=(?, 28, 28, 1)\n","\n","            W1 = tf.Variable(tf.random_normal([3, 3, 1, 32], stddev=0.01))\n","\n","            #    Conv     -> (?, 28, 28, 32)\n","\n","            #    Pool     -> (?, 14, 14, 32)\n","\n","            L1 = tf.nn.conv2d(X_img, W1, strides=[1, 1, 1, 1], padding='SAME')\n","\n","            L1 = tf.nn.relu(L1)\n","\n","            L1 = tf.nn.max_pool(L1, ksize=[1, 2, 2, 1],\n","\n","                                strides=[1, 2, 2, 1], padding='SAME')\n","\n","            L1 = tf.nn.dropout(L1, keep_prob=self.keep_prob)\n","\n","            '''\n","\n","            Tensor(\"Conv2D:0\", shape=(?, 28, 28, 32), dtype=float32)\n","\n","            Tensor(\"Relu:0\", shape=(?, 28, 28, 32), dtype=float32)\n","\n","            Tensor(\"MaxPool:0\", shape=(?, 14, 14, 32), dtype=float32)\n","\n","            Tensor(\"dropout/mul:0\", shape=(?, 14, 14, 32), dtype=float32)\n","\n","            '''\n","\n","\n","\n","            # L2 ImgIn shape=(?, 14, 14, 32)\n","\n","            W2 = tf.Variable(tf.random_normal([3, 3, 32, 64], stddev=0.01))\n","\n","            #    Conv      ->(?, 14, 14, 64)\n","\n","            #    Pool      ->(?, 7, 7, 64)\n","\n","            L2 = tf.nn.conv2d(L1, W2, strides=[1, 1, 1, 1], padding='SAME')\n","\n","            L2 = tf.nn.relu(L2)\n","\n","            L2 = tf.nn.max_pool(L2, ksize=[1, 2, 2, 1],\n","\n","                                strides=[1, 2, 2, 1], padding='SAME')\n","\n","            L2 = tf.nn.dropout(L2, keep_prob=self.keep_prob)\n","\n","            '''\n","\n","            Tensor(\"Conv2D_1:0\", shape=(?, 14, 14, 64), dtype=float32)\n","\n","            Tensor(\"Relu_1:0\", shape=(?, 14, 14, 64), dtype=float32)\n","\n","            Tensor(\"MaxPool_1:0\", shape=(?, 7, 7, 64), dtype=float32)\n","\n","            Tensor(\"dropout_1/mul:0\", shape=(?, 7, 7, 64), dtype=float32)\n","\n","            '''\n","\n","\n","\n","            # L3 ImgIn shape=(?, 7, 7, 64)\n","\n","            W3 = tf.Variable(tf.random_normal([3, 3, 64, 128], stddev=0.01))\n","\n","            #    Conv      ->(?, 7, 7, 128)\n","\n","            #    Pool      ->(?, 4, 4, 128)\n","\n","            #    Reshape   ->(?, 4 * 4 * 128) # Flatten them for FC\n","\n","            L3 = tf.nn.conv2d(L2, W3, strides=[1, 1, 1, 1], padding='SAME')\n","\n","            L3 = tf.nn.relu(L3)\n","\n","            L3 = tf.nn.max_pool(L3, ksize=[1, 2, 2, 1], strides=[\n","\n","                                1, 2, 2, 1], padding='SAME')\n","\n","            L3 = tf.nn.dropout(L3, keep_prob=self.keep_prob)\n","\n","\n","\n","            L3_flat = tf.reshape(L3, [-1, 128 * 4 * 4])\n","\n","            '''\n","\n","            Tensor(\"Conv2D_2:0\", shape=(?, 7, 7, 128), dtype=float32)\n","\n","            Tensor(\"Relu_2:0\", shape=(?, 7, 7, 128), dtype=float32)\n","\n","            Tensor(\"MaxPool_2:0\", shape=(?, 4, 4, 128), dtype=float32)\n","\n","            Tensor(\"dropout_2/mul:0\", shape=(?, 4, 4, 128), dtype=float32)\n","\n","            Tensor(\"Reshape_1:0\", shape=(?, 2048), dtype=float32)\n","\n","            '''\n","\n","\n","\n","            # L4 FC 4x4x128 inputs -> 625 outputs\n","\n","            W4 = tf.get_variable(\"W4\", shape=[128 * 4 * 4, 625],\n","\n","                                 initializer=tf.contrib.layers.xavier_initializer())\n","\n","            b4 = tf.Variable(tf.random_normal([625]))\n","\n","            L4 = tf.nn.relu(tf.matmul(L3_flat, W4) + b4)\n","\n","            L4 = tf.nn.dropout(L4, keep_prob=self.keep_prob)\n","\n","            '''\n","\n","            Tensor(\"Relu_3:0\", shape=(?, 625), dtype=float32)\n","\n","            Tensor(\"dropout_3/mul:0\", shape=(?, 625), dtype=float32)\n","\n","            '''\n","\n","\n","\n","            # L5 Final FC 625 inputs -> 10 outputs\n","\n","            W5 = tf.get_variable(\"W5\", shape=[625, 10],\n","\n","                                 initializer=tf.contrib.layers.xavier_initializer())\n","\n","            b5 = tf.Variable(tf.random_normal([10]))\n","\n","            self.logits = tf.matmul(L4, W5) + b5\n","\n","            '''\n","\n","            Tensor(\"add_1:0\", shape=(?, 10), dtype=float32)\n","\n","            '''\n","\n","\n","\n","        # define cost/loss & optimizer\n","\n","        self.cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(\n","\n","            logits=self.logits, labels=self.Y))\n","\n","        self.optimizer = tf.train.AdamOptimizer(\n","\n","            learning_rate=learning_rate).minimize(self.cost)\n","\n","\n","\n","        correct_prediction = tf.equal(\n","\n","            tf.argmax(self.logits, 1), tf.argmax(self.Y, 1))\n","\n","        self.accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n","\n","\n","\n","    def predict(self, x_test, keep_prop=1.0):\n","\n","        return self.sess.run(self.logits, feed_dict={self.X: x_test, self.keep_prob: keep_prop})\n","\n","\n","\n","    def get_accuracy(self, x_test, y_test, keep_prop=1.0):\n","\n","        return self.sess.run(self.accuracy, feed_dict={self.X: x_test, self.Y: y_test, self.keep_prob: keep_prop})\n","\n","\n","\n","    def train(self, x_data, y_data, keep_prop=0.7):\n","\n","        return self.sess.run([self.cost, self.optimizer], feed_dict={\n","\n","            self.X: x_data, self.Y: y_data, self.keep_prob: keep_prop})\n","\n","\n"],"execution_count":0,"outputs":[]}]}