## MNIST with Neural Network(DEEP Learning) 
## MNIST 문제를 딥러닝을 사용해서 정확도를 높혀보자
## tensorflow가 기본으로 제공하는 예제를 이용해서 구현하기 

import tensorflow as tf 
from tensorflow.examples.tutorials.mnist import input_data
import warnings 

warnings.filterwarnings(action = "ignore") # warning 출력 방지 

# Data Loading 
mnist = input_data.read_data_sets("./data/mnist",one_hot=True)


# Placeholder 
X = tf.placeholder(shape=[None, 784], dtype = tf.float32)
Y = tf.placeholder(shape=[None, 10], dtype = tf.float32)

# Weight & bias( Deep & Wide ) 
W1 = tf.Variable(tf.random_normal([784,256]), name = "weight") # 뒷'열'(perceptron)은 내가 잡고 싶은 대로 조정
b1 = tf.Variable(tf.random_normal([256]), name = "bias")       # 단 depth 가 깊으면 깊을수록 처리하기 곤란해진다
layer1 = tf.sigmoid(tf.matmul(X,W1) + b1)

W2 = tf.Variable(tf.random_normal([256,256]), name = "weight2") 
b2 = tf.Variable(tf.random_normal([256]), name = "bias2")      
layer2 = tf.sigmoid(tf.matmul(layer1,W2) + b2)

W3 = tf.Variable(tf.random_normal([256,10]), name = "weight3") 
b3 = tf.Variable(tf.random_normal([10]), name = "bias3")      
layer3 = tf.sigmoid(tf.matmul(layer2,W3) + b3)


# Hypothesis 
logit = tf.matmul(layer2,W3) + b3 
H = tf.nn.softmax(logit) #or tf.sigmoid()

# cost
cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits_v2(logits = logit,
                                                                 labels = Y))

# train 
train = tf.train.GradientDescentOptimizer(learning_rate = 0.01).minimize(cost)

# session, 초기화 
sess = tf.Session()
sess.run(tf.global_variables_initializer())

# 학습 
num_of_epoch = 300 
batch_size = 100 

for step in range(num_of_epoch):
    num_of_iter = int(mnist.train.num_examples / batch_size)
    cost_val = 0 # 코스트 초기화(국룰)
    
    for i in range(num_of_iter):
        batch_x,batch_y = mnist.train.next_batch(batch_size)
        _, cost_val = sess.run([train,cost], feed_dict={X:batch_x, 
                                                        Y:batch_y})
    if step % 30 == 0 :
        print("Cost는 : {}".format(cost_val))



predict = tf.argmax(H,1)
correct = tf.equal(predict, tf.argmax(Y,1))
accuracy = tf.reduce_mean(tf.cast(correct, dtype=tf.float32))

print("정확도는 : {}".format(sess.run(accuracy, 
                                   feed_dict={X:mnist.test.images, 
                                              Y:mnist.test.labels})))
# 정확도가 뛰어나지 않다?




## 생각보다 정확도가 많이 향상되지 않았다! 
## Hinton교수가 원인을 파악하고자 노력했다. 
## deep learning이 좀더 학습이 잘 되기 위해선 layer를 추가하고 각 layer에 많은 perceptron을 추가해서 구현 
import tensorflow as tf 
from tensorflow.examples.tutorials.mnist import input_data
import warnings 

warnings.filterwarnings(action = "ignore") # warning 출력 방지 

# Data Loading 
mnist = input_data.read_data_sets("./data/mnist",one_hot=True)



#그래프 초기화 
tf.reset_default_graph()

# Placeholder 
X = tf.placeholder(shape=[None, 784], dtype = tf.float32)
Y = tf.placeholder(shape=[None, 10], dtype = tf.float32)

# Weight & bias( Deep & Wide ) 
W1 = tf.Variable(tf.random_normal([784,256]), name = "weight") # 뒷'열'(perceptron)은 내가 잡고 싶은 대로 조정
b1 = tf.Variable(tf.random_normal([256]), name = "bias")       # 단 depth 가 깊으면 깊을수록 처리하기 곤란해진다
layer1 = tf.nn.relu(tf.matmul(X,W1) + b1)

W2 = tf.Variable(tf.random_normal([256,256]), name = "weight2") 
b2 = tf.Variable(tf.random_normal([256]), name = "bias2")      
layer2 = tf.nn.relu(tf.matmul(layer1,W2) + b2)

W3 = tf.Variable(tf.random_normal([256,10]), name = "weight3") 
b3 = tf.Variable(tf.random_normal([10]), name = "bias3")      
layer3 = tf.nn.relu(tf.matmul(layer2,W3) + b3)


# Hypothesis 
logit = tf.matmul(layer2,W3) + b3 
H = tf.nn.relu(logit) #or tf.sigmoid()

# cost
cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits_v2(logits = logit,
                                                                 labels = Y))

# train 
train = tf.train.GradientDescentOptimizer(learning_rate = 0.01).minimize(cost)

# session, 초기화 
sess = tf.Session()
sess.run(tf.global_variables_initializer())

# 학습 
num_of_epoch = 30
batch_size = 100 

for step in range(num_of_epoch):
    num_of_iter = int(mnist.train.num_examples / batch_size)
    cost_val = 0 # 코스트 초기화(국룰)
    
    for i in range(num_of_iter):
        batch_x,batch_y = mnist.train.next_batch(batch_size)
        _, cost_val = sess.run([train,cost], feed_dict={X:batch_x, 
                                                        Y:batch_y})
    if step % 3 == 0 :
        print("Cost는 : {}".format(cost_val))



predict = tf.argmax(H,1)
correct = tf.equal(predict, tf.argmax(Y,1))
accuracy = tf.reduce_mean(tf.cast(correct, dtype=tf.float32))

print("정확도는 : {}".format(sess.run(accuracy, 
                                   feed_dict={X:mnist.test.images, 
                                              Y:mnist.test.labels})))

###################################
## Hinton 교수님이 중요하게 여기는 또 하나의 요건은 W의 초기값
## 초기에는 RBM이라는 방법을 이용하여 초기화를 진행 
## 2010 년도에 Xavier 초기화라는 방식으로 논문으로 발표 
## 2015 년에는 He's 초기화라는 방식의 논문으로 발표 
import tensorflow as tf 
from tensorflow.examples.tutorials.mnist import input_data
import warnings 

warnings.filterwarnings(action = "ignore") # warning 출력 방지 

# Data Loading 
mnist = input_data.read_data_sets("./data/mnist",one_hot=True)

## Overfitting(과적합)
## 학습한 모델이 training data set에 최적화되어 있는 상태 
## 테스트 데이터에는 잘 들어맞지 않는 상태를 지칭

## 학습한 모델이 training data set에는 약 98% 이상 정확도를 가지지만
## test data set에 대해서는 85% 정도 밖에 나오지 않는 다면 그건 overfitting 

## 1. 일단 학습하는 데이터 수 가 많아야 한다. 
## 2. 필요없는 feature들은 학습에서 제외 
##    중복되는 feature들은 단일화 시켜야한다. 
## 3. 학습하는 과정에서 overfitting을 피할 수 있다. 
## (2014년 논문, logistic을 선택적으로 줄이면 overfitting을 피할 수 있다고 주장 -> dropout)
##  -> 이 역시 tensorflow에서 제공된다. 


# 그래프 초기화 
tf.reset_default_graph()

# Placeholder 
X = tf.placeholder(shape=[None, 784], dtype = tf.float32)
Y = tf.placeholder(shape=[None, 10], dtype = tf.float32)
dout_rate =tf.placeholder(dtype=tf.float32)

# Weight & bias( Deep & Wide ) 
W1 = tf.get_variable("Weight1", shape=[784, 256], initializer=tf.contrib.layers.xavier_initializer())
b1 = tf.Variable(tf.random_normal([256]), name = "bias1")        
_layer1 = tf.nn.relu(tf.matmul(X,W1) + b1)
layer1 = tf.nn.dropout(_layer1, rate =dout_rate ) #'죽이는 비율'설정 -> 과적합 방지 (학습시 수치 설정)
                                                  # test data로 검사결과(정확도)뽑을 때는 당연 빼야한다 
W2 = tf.get_variable("Weight2", shape=[256, 256], initializer=tf.contrib.layers.xavier_initializer())
b2 = tf.Variable(tf.random_normal([256]), name = "bias2")      
_layer2 = tf.nn.relu(tf.matmul(layer1,W2) + b2)
layer2 = tf.nn.dropout(_layer2, rate = dout_rate )

W3 = tf.get_variable("Weight3", shape=[256, 10], initializer=tf.contrib.layers.xavier_initializer())
b3 = tf.Variable(tf.random_normal([10]), name = "bias3")      
_layer3 = tf.nn.relu(tf.matmul(layer2,W2) + b2)
layer3 = tf.nn.dropout(_layer3, rate = dout_rate )


# Hypothesis 
logit = tf.matmul(layer2,W3) + b3 
H = tf.nn.relu(logit) #or tf.sigmoid()

# cost
cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits_v2(logits = logit,
                                                                 labels = Y))

# train 
# AdamOptimizer 사용해보기 
train = tf.train.AdamOptimizer(learning_rate = 0.01).minimize(cost)
#train = tf.train.GradientDescentOptimizer(learning_rate = 0.01).minimize(cost)

# session, 초기화 
sess = tf.Session()
sess.run(tf.global_variables_initializer())

# 학습 
num_of_epoch = 30
batch_size = 100 

for step in range(num_of_epoch):
    num_of_iter = int(mnist.train.num_examples / batch_size)
    cost_val = 0 # 코스트 초기화(국룰)
    
    for i in range(num_of_iter):
        batch_x,batch_y = mnist.train.next_batch(batch_size)
        _, cost_val = sess.run([train,cost], feed_dict={X:batch_x, 
                                                        Y:batch_y,
                                                        dout_rate:0.3})
    if step % 3 == 0 :
        print("Cost는 : {}".format(cost_val))
		
predict = tf.argmax(H,1)
correct = tf.equal(predict, tf.argmax(Y,1))
accuracy = tf.reduce_mean(tf.cast(correct, dtype=tf.float32))

print("정확도는 : {}".format(sess.run(accuracy, 
                                   feed_dict={X:mnist.test.images, 
                                              Y:mnist.test.labels,
                                              dout_rate:0})))




