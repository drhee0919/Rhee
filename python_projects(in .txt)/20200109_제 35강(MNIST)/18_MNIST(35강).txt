### multinominal classification exercise 
## -> MNIST 예제를 이용해서 보완해야 하는 부분들을 알아보자 
## -> MNIST 는 '이미지를 학습하고 prediction' 하는 예제(28pixel*28pixel크기의 이미지들이 주어진다.)
## -> 각 이미지들은 사람이 '수기로 쓴 숫자' 들이 들어있다. 
## -> 1명당 0~9까지 숫자를 쓴 내용이 5만명 분이 들어있다. (50,000set x 10 = 500,000images)
## -> MNIST 의 결과물 multinomial 개수는 10개의 logistic을 가짐(0~9) 

# 필요한 module import 
import tensorflow as tf 
import pandas as pd 
import numpy as np 
import matplotlib.pyplot as plt 
from tensorflow.examples.tutorials.mnist import input_data #데이터셋 불러오기용(.gz파일, unix계열 압축파일)

# Data Loading 
mnist = input_data.read_data_sets("./data/mnist", one_hot=True) #one_hot encoding상태로 받아올 수 있다! 
                                              #단, tf가 제공하는 형태이기 때문에 가능한것(다 되는건 아니다) 


# Data Loading이어서 
mnist.train.images # 학습용 데이터 불러오기 (2차원 array)
mnist.train.images.shape #(55000, 784)

#이미지데이터 -> x train data로 불러오기 

train_x_data_df = pd.DataFrame(mnist.train.images) #데이터 프레임으로 만들어
train_x_data_df.to_csv("./mnist_x_data.csv", index=False) #csv파일로 만들어버리자 

# x데이터의 각 column은(=각 pixel값은) 0과 1사이의 값으로 이미 scale이 되어있는 상태(채도 차이)
# 0과 가까울 수 록 흰색을 지칭, 1과 가까울 수 록 색상이 어두워진다. 

#숫자 데이터 -> y train data로 불러오기
train_y_data_df = pd.DataFrame(mnist.train.labels)
train_y_data_df.to_csv("./mnist_y_data.csv", index=False)


# Data Loading이어서 
mnist.train.images # 학습용 데이터 불러오기 (2차원 array)
mnist.train.images.shape #(55000, 784)

#이미지데이터 -> x train data로 불러오기 

train_x_data_df = pd.DataFrame(mnist.train.images) #데이터 프레임으로 만들어
train_x_data_df.to_csv("./mnist_x_data.csv", index=False) #csv파일로 만들어버리자 

# x데이터의 각 column은(=각 pixel값은) 0과 1사이의 값으로 이미 scale이 되어있는 상태(채도 차이)
# 0과 가까울 수 록 흰색을 지칭, 1과 가까울 수 록 색상이 어두워진다. 

#숫자 데이터 -> y train data로 불러오기
train_y_data_df = pd.DataFrame(mnist.train.labels)
train_y_data_df.to_csv("./mnist_y_data.csv", index=False)


# Data Loading이어서 
mnist.train.images # 학습용 데이터 불러오기 (2차원 array)
mnist.train.images.shape #(55000, 784)

#이미지데이터 -> x train data로 불러오기 

train_x_data_df = pd.DataFrame(mnist.train.images) #데이터 프레임으로 만들어
train_x_data_df.to_csv("./mnist_x_data.csv", index=False) #csv파일로 만들어버리자 

# x데이터의 각 column은(=각 pixel값은) 0과 1사이의 값으로 이미 scale이 되어있는 상태(채도 차이)
# 0과 가까울 수 록 흰색을 지칭, 1과 가까울 수 록 색상이 어두워진다. 

#숫자 데이터 -> y train data로 불러오기
train_y_data_df = pd.DataFrame(mnist.train.labels)
train_y_data_df.to_csv("./mnist_y_data.csv", index=False)


# 학습이 종료되었으니 정확도 측정 (학습과 분리)      
# Accuracy(정확도)
predict = tf.argmax(H,1) #출력되는 셋 중 가장 큰 값의 index번호를 리턴 
correct = tf.equal(predict, tf.argmax(Y, 1)) # predict와 correct가 맞으면 좋은 모델(위치를 가지고 비교)
accuracy = tf.reduce_mean(tf.cast(correct, dtype=tf.float32))
print("정확도는 : {}".format(sess.run(accuracy, 
                                   feed_dict={X:mnist.test.images, 
                                              Y:mnist.test.labels})))


# Prediction 
# 랜덤으로 하나의 데이터를 추출해서 그놈을 이용해서 prediction을 한 후 결과를 비교해 보아요! 
r = np.random.randint(0,mnist.test.num_examples) #0부터 55000사이의 난수를튕김 
# 난수가 의미하는 행의 label값을 먼저 구해보아요! 

mnist.test.labels[r] # array([0., 0., 0., 0., 0., 1., 0., 0., 0., 0.])
                     # 요런 형태 
print("Label : {}" .format(sess.run(tf.argmax(mnist.test.labels[r:r+1], axis=1)))) 
#1차원의형태라서 axis=0밖에 못씀
#[r:r+1] = r번째 행이지만 차원형태를 위해 지칭
#print("Label : {}" .format(sess.run(tf.argmax(mnist.test.labels[r], axis=0)))) ->이거랑 동일 
print("Predict : {}" .format(sess.run(tf.argmax(H,1), 
                      feed_dict={X:mnist.test.images[r:r+1]})))
                        #2차원 placeholder와 차원을 맞춰주자 
                        #10번 중 한번골로 다른 결과가 다르게 나올 수도 있다.

#어떻게 생겼는지 보자     
#pyplot에 있는 imshow 메서드를 통해 이미지를 출력 
plt.imshow(mnist.test.images[r:r+1].reshape(28,28), cmap="Greys") #그냥 그리면 알아서 컬러풀하게 해주니 따로 원본처럼 설정 


## kaggle데이터를 이용해서 문제를 풀어보자 
## tf내장 데이터셋이 아닌 실제 공모 데이터로 측정 

## retry
## 모듈을 이용한 전처리
#필요모듈 불러오기 
import tensorflow as tf
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
from tensorflow.examples.tutorials.mnist import input_data
from sklearn.preprocessing import MinMaxScaler

# 원본데이터 불러오기 
mnist = pd.read_csv("./data/digit-recognizer/train.csv")

#데이터 이원화, 분류 
train_num = int(mnist.shape[0] * 0.8)
test_num = mnist.shape[0] - train_num

train_data = mnist[:train_num]
test_data = mnist[train_num:]

#정의역, 공역 데이터 생성

#train x,y 데이터 
train_x_data = mnist.drop("label", axis = 1, inplace = False)[:train_num].values
test_x_data = mnist.drop("label",axis = 1, inplace = False)[train_num:].values

#train x,y 데이터
train_y_data = mnist['label'][:train_num].values
test_y_data = mnist['label'][train_num:].values 

#학습을 위한 정규화(x데이터)
scaler = MinMaxScaler()
train_x_data = scaler.fit_transform(train_x_data)
test_x_data=scaler.fit_transform(test_x_data)

#더미변수화(One-hot Encoding, y데이터 )
train_y_data = pd.get_dummies(train_y_data)
test_y_data = pd.get_dummies(test_y_data)

display(train_x_data)
display(type(train_x_data)) #numpy.ndarray
display(train_y_data)
display(type(train_y_data)) #pandas.core.frame.DataFrame

# tensorflow로 machine learning
# placeholder
X = tf.placeholder(shape=[None,784], dtype=tf.float32)
Y = tf.placeholder(shape=[None,10], dtype=tf.float32)

# Weight & bias
W = tf.Variable(tf.random_normal([784,10]), name="weight")
b = tf.Variable(tf.random_normal([10]), name="bias")

# Hypothesis
logit = tf.matmul(X,W) + b
H = tf.nn.softmax(logit)

# Cost
cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits_v2(logits=logit,
                                                                labels=Y))

# train
train = tf.train.GradientDescentOptimizer(learning_rate=0.1).minimize(cost)

# session, 초기화
sess = tf.Session()
sess.run(tf.global_variables_initializer())

# 학습 (batch 이용)
train_epoch = 300
batch_size = 100

for step in range(train_epoch):
    num_of_iter = int(train_num/batch_size) #전체행 / batch_size
    cost_val = 0
    for i in range(num_of_iter):
        batch_x = train_x_data[i*batch_size : (i+1)*batch_size]
        batch_y = train_y_data[i*batch_size : (i+1)*batch_size]
        _,cost_val = sess.run([train,cost],
                             feed_dict = {X:batch_x,
                                          Y:batch_y})
    if step % 30 == 0:
        print("Cost값은: {}".format(cost_val))
		
# 정확도 측정 (test데이터 기반으로)
predict = tf.argmax(H, 1)
correct = tf.equal(predict, tf.argmax(Y,1))
accuracy = tf.reduce_mean(tf.cast(correct, dtype = tf.float32))
print("정확도는 : {}".format(sess.run(accuracy,
                                     feed_dict = {X: test_x_data,
                                                  Y: test_y_data})))
												  
# Prediction
# 랜덤으로 하나의 데이터를 추출해서 그놈을 이용해서 prediction을 한 후 결과를 비교해 보자

r = np.random.randint(0,test_num) # mnist.test.num_examples = 10000

print("Label : {}".format(sess.run(tf.argmax(test_y_data[r:r+1], axis=1))))

print("Predict :{}".format(sess.run(tf.argmax(H,1), 
         feed_dict={X:test_x_data[r:r+1]})))    # 2차원

plt.imshow(test_x_data[r:r+1].reshape(28,28), cmap="Greys")
# 1,784 => 28,28로 바꿀거야

## 진짜 test 파일로 돌려보자!!

test_data = pd.read_csv("./data/digit-recognizer/test.csv")

# MinMax scaler가 min, max값 가지고 있다.
prediction_data = scaler.transform(test_data)

#sess.run(H,feed_dict={X:prediction_data})
result = sess.run(tf.argmax(H,1), feed_dict={X:prediction_data})
result

#결과 출력 내용 -> df -> .csv로 만들자
my_df = pd.DataFrame()
my_df["ImageId"] = range(1,test_data.shape[0]+1)
my_df["Label"] = result
my_df #(28000, 2)

my_df.to_csv("mnist_submission.csv",index=False)