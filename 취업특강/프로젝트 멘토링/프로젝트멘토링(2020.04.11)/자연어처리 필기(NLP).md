#### 프로젝트 참고 링크

**NLP 처리에 유용한 참고자료**

>http://aiopen.etri.re.kr/ : 공공 인공지능 오픈 API , Data 서비스 포털 
>
>https://cafe.naver.com/itpe4you : 정보관리, 컴퓨터응용기술사 전문교육기관 
>
>http://aihub.or.kr/ : AI기술 및 제품개발에 필수적인 AI데이터, 알고리즘, 컴퓨팅 자원 등 통합 인프라제공 
>
>https://github.com/kakao/khaiii : 카카오 형태소 분석기 
>
>https://korquad.github.io/ : The Korean Question Answering Dataset 
>
>https://konlpy-ko.readthedocs.io/ko/v0.4.3/ :KoNLPy: 파이썬 한국어 NLP



#### nlp 용어정리

> - 임베딩 
>
>   사람이 쓰는 자연어를 기계가 이해할 수 있는 숫자의 나열인 Vector 로 바꾼 결과 혹은 그 일련이 과정 전체를 의미
>
> - 형태소(morpheme)
>
>   일정한 의미가 있는 가장 작은 말의 단위, 즉, 더 분석하면 뜻이 없어지는 말의 단위 
>
> > '어휘형태소', '문법형태소(는, 가, 아서, 어서, 에 , 았, 습니다)'로 나뉜다
>
> - 형태소 분석
>
> > 형태소보다 단위가 더 큰 언어의 단위인 어절, 혹은 문장을 최소 의미 단위인 형태소로 분절하는 과정 
> >
> > 오픈 소스 형태소 분석기: 은전한닢. 꼬꼬마, 띄어쓰기 
>
> - 품사태깅(part-of-speech tagging) = 태깅
>
>   문맥을 고려하여 하느이 어절에 대하여 가장 적합한 것으로 판단되는 형태소 분석 결과만을 출력하는 품사를 붙이는 과정 
>
> - 말뭉치(corpus, 코퍼스)
>
>   자연언어 연구를 위해 특정한 목적을 가지고 수집한 표본 
>
>   일정한 규모 이상의 크기를 갖추고 내용적으로 다양성과 균형성이 확보된 자료의 집합체
>
>   연어의 다양한 양상을 대표할 수 있는 여러 가지 다양한 텍스트가 균형 있게 구성되어 있고, 
>
>   도 일정한 규모 이상으로 모여야 '말뭉치' 라는 용어 사용 가능 
>
> - 토큰 
>
>   의미를 가지는 문자열 
>
>   토큰은 형태소(뜻을 가진 최소 단위)나 그보다 상위 개념인 단어(자립하여 쓸 수 있는 최소 단위)까지 포함 
>
>   단어, 형태소, 서브워드라고 부를 수 있다 
>
>   토큰 분리 기준은 그때그때 다르다(tokenize)
>
> - 어휘집합(vocabulary)
>
>   말뭉치에 있는 모든 문서를 문장으로 나누고 여기에 토큰나이즈를 실시한 후 중복을 제거한 토큰들의 집합 



**NLP 역사 **

> 규칙 → 확률/통계 → 딥러닝 의 흐름 
>
> Symbolic approch → Statistical approach → DeepLearning
>
> ```
> 사람이 질문하는 내용을 규칙으로 패턴을 매핑해 놓은 것 
> -> 문서 중 의미있는 키워드만 추출해 내는 기법(TF-IDF)
> -> 확률 / 통게 접근법을 딥러닝 기술에 적용 
> ```



**자연어 처리과정**

> 원문 → 어휘분석 → 구문분석 → 관계도출 

​	 

**자연어 처리응용**

> RNN으로 보다 용이해진 번역 과정 (machine translation)
>
> Cho 를 비롯한 몬트리올 대학 일원이 RNN을 이용한 Encoder-Decoder 아키텍쳐를 제시함 



---

#### 임베딩

> 범주형 자료를 연속형 벡터 형태로 변환시키는 것을 지칭
>
> 주로 인공신경망 학습을통해 범주형 자료를 벡터 형태로 만든다. 
>
> 인공신경망학습 과정을 통해 각 벡터에 가장 알맞는 값을 할당하게 된다. 이 과정에서 각 범주category 는 각각의 특징을 가장 잘 표현할 수 있는 형태의 백터값을 가지게 된다. 
>
> t-SNE와 같은 방법을 활용하면 이 벡터들을 시각화해서 표현할 수 있으며, 매우 직관적으로 표현된다. 



**임베딩의 역할**

> - 단어/문장 간 관련도 계산 
>
>   단어를 백터로 임베딩하는 순간 벡터들사이의 유사도를 계산하는 일이 가능해짐 
>
> - 의미적/문법적 정보 함축
>
>   임베딩은 벡터인 만큼 사칙연산 가능
>
>   단어 벡터 간 덧셈 뺄셈을 통해 단어들 사이의 의미적, 문법적 관계 도출 
>
> - 전이 학습(transfer learning)
>
>   임베딩을 다른 딥러닝 모델의 입력값으로 쓰는 기법
>
>   전이 학습 모델을 통해 문서분류라는 테스크를 빠르게 수행 



**임베딩 기법의 역사**

> AS-IS → TO-BE
>
> 통계기반 → NN 기반
>
> 단어수준 → 문장 수준
>
> Rule → End to End모델 → Pre-train/ Fine tuning 



**임베딩을 위한 과정**

>  말뭉치 → 전처리(preprocessing) → 형태소 분석 → 임베딩 구축 → 임베딩 파인튜닝 
>
> ```
> 한국어 위키백과 등 원천 데이터, KoreQUAD, 네이버 영화 리뷰 말뭉치 
> ```
>
> ​																			↓
>
> ```
> XML문서상에서 <text>태그안의 텍스트 추출, 특수문자, 공백, 이메일주소, 웹페이지 주소 등 제거
> 
> 원문 그대로 처리, 질문가 답변은 공백으로 연결해 한라인으로 처리 
> 
> 리뷰들을 extract 
> ```
>
> ​																			↓
>
> ```
> 지도학습기반 형태소 분석기 : KoNLPY, Khaiii
> 비지도학습기반 형태소 분석기 : soynlp, 구글 Sentencepiece
> ```
>
> ​																			↓
>
> ```
> 단어수준 임베딩 모델 
> 문장수준 임베딩 모델 
> ```
>
> ​																			↓
>
> ```
> 긍정, 부정 등 문서의 polarity 를 예측하는 문서 분류 과제 예시 
> ```



**단어수준임베딩 모델의 분류**

> 예측기반 모델(뉴럴 네트워크 기반)
>
> NPLM,  Word2Vec,  FastText

> 행렬분해 기반(matrix factorization)
>
> 잠재의미분석(LSA),  Glove,  Swivel, BERT

> https://github.com/yeontaek/BERT-Korean-Model :한국어 전용 BERT 모델