#### 출제 빈도 

14회차 시험까지 총 10회 이상 기출된 문제에 대해서 중점적으로 정리해보자 

17년도부터 출제 대상에서 제외된 파트는 제외한다. 

<u>밑줄</u>표시는 매년 출제된 문제이다. 

**1장: 데이터의 이해**

> 1.1.1 <u>데이터와 정보</u> (14회)
>
> > - 데이터 : 개별 데이터 자체로 의미가 중요하지 않은 객관적인 사실
> > - 정보 : 데이터의 *가공, 처리*와 데이터 간 *연관관계* 속에서 의미가 도출 
> > - 지식 : 정보를 구조화하여 유의미한 정보를 분류하고 *개인적 경험*을 결합하여 내재화한 것 
> > - 지혜 : 지식의 축적과 *아이디어*가 결합된 창의적 산물 
>
> 1.1.3 데이터 베이스 활용 (10회)
>
> > - 기업 내부 데이터 베이스
>
> ```
> OLTP, OLAP
> CRM, SCM, ERP, RTE, BI, EAI, KMS
> 제조부문, 금융부문, 유통부문 에서의 데이터 활용 
> ```
>
> > - 사회기반구조로서의 데이터 베이스 
>
> ```
> EDI, VAN, CALS
> 물류부문(CVO, PORT-MIS)
> 지리/교통부문(GIS, RS, GPS, ITS, LBS, SIM)
> 의료부문(PACS, U헬스)
> ```
>
> 
>
> 1.2.1 빅데이터의 이해 (11회)
>
> > - 빅데이터 정의
>
> ```
> McKinsey: 일반적인 데이터베이스 소프트웨어로 저장, 관리, 분석할 수 있는 범위를 초과하는 
> 		 규모의 데이터이다. 
> IDC:      다양한 종류의 대규모 데이터로부터 저렴한 비용으로 가치를 추출하고 데이터의 
> 		 초고속 수집/발굴/분석을 지원하도록 고안된 차세대 기술 및 아키텍처이다. 
> 가트너 그룹(Doug Laney)의 3V - Volumn, Variety, Velocity 
> ```
>
> > - 빅데이터 정의의 범주 및 효과 
>
> ``` 
> 1. 데이터 변화(규묘, 형태, 속도)
> 2. 기술변화(새로운 데이터 처리, 저장 분석 기술, 클라우드 컴퓨팅 활용)
> 3. 인재, 조직 변화(data scientist, 데이터 중심 조직)
> ```
>
> > - 빅데이터에 거는 기대의 비유적 표현 
>
> ```
> 산업혁명의 석탄과 철(생산성 ↑,사회,경제,문화,생활 전반에 혁명적 변화)
> 21세기의 원유(생산성↑, 새로운 범주의 산업 생성)
> 렌즈(현미경~생물학 → 데이터~산업발전)(예: Google Ngram Viewer)	
> 플랫폼 (='공동 활용의 목적으로 구축된 유무형의 구조물')
> ```
>
> > - 빅데이터가 만들어내는 본질적인 변화 
>
> ```
> 사전처리 → 사후처리 
> 표본처리 → 전수조사 
> 질 → 양
> 인과관계 → 상관관계 
> ```
>
> 1.2.2 빅데이터의 가치와 영향 (11회)
>
> > - 빅데이터 시대에서는 특정 데이터의 가치를 측정하는 것이 쉽지 않다 
>
> ```
> 1. 데이터 활용 방식 : 재사용, 재조합(mashup), 다목적용 개발
> 	→특정 데이터를 언제 어디서 누가 활용할지 ???
> 2. 새로운 가치 창출(=기존에 없던 가치)
> 3. 분석 기술 발전(현재 無가치 → 새로운 분석 기법 등장으로 거대한 가치)
> ```
>
> > - 빅데이터의 영향
>
> ```
> - 맥킨지가 언급한 빅데이터가 가치를 만들어내는 다섯가지 방식
> ① 투명성 제고로 연구개발 및 관리 효율성 제고
> ② 시뮬레이션을 통한 수요 포착 및 주요 변수 탐색으로 경쟁력 강화
> ③ 고객 세분화 및 맞춤 서비스 제공 
> ④ 알고리즘을 활용한 의사결정 보조 혹은 대체
> ⑤ 비즈니스 모델과 제품, 서비스의 혁신 
> 
> - 빅데이터의 가치 창출 방식이 시장에 있는 플레이어(기업,정부,소비자)에 미치는 영향
>  「① 기업: 혁신, 경쟁력 제고, 생산성 향상	
>    ② 정부: 환경 탐색, 상황 분석, 미래 대응  	→  생활 전반의 스마트화 	 
>    ③ 개인: 목적에 따라 활용 				」
> ```
>
> > - 빅데이터의 활용 기본 테크닉 7가지
>
> ```
> 연관규칙학습, 유형분석, 유전 알고리즘, 기계 학습, 회귀분석, 감정 분석, 소셜네트워크 분석 
> ```
>
> 1.3.1 빅데이터 분석과 전략 인사이트 (11회)
>
> > - 빅데이터의 회의론 원인 
>
> ```
> ① 부정적 학습효과- 과거의 고객관계관리(CRM): 공포 마케팅, 투자대비 효과 미흡 
> ② 부적절한 성공사례- 빅데이터가 필요없는 분석사례, 기존 CRM분석 성과 
> ```
>
> > - 싸이월드 퇴보 원인 
>
> ```
> ① OLAP과 같은 분석 인프라로 존재했으나 중요한 의사결정에 데이터 분석 활용 못함
> ② 웹로그 분석을 통한 일차원적 분석만 집중
> ③ 소셜 네트워킹 활동 특성과 관련된 분석을 위한 프레임워크나 평가지표도 없었음
> ④ 트랜드 변화가 사업모델에 미치는 영향에 대한 전략적 통찰(insight)을 가지지 못함 
> ```
>
> > - 전략적 통찰이 없는 분석의 함정 
>
> ```
> - 단순히 일차원적인 분석의 반복은 해당부서의 업무 영역에서는 효과적이지만 기업의 환경
>   변화와 고객 변화에 전략적으로 대처하기 힘듬 
>   
> - 전략적 통찰력의 창출에 초점을 맞춰 분석을 활용하면 사업의 중요한 기회를 발견할 수 있음 
> 
> - 최고가 되기 위해서는 일차원적인 분석을 통해 분석 경험을 늘리고 작은 성공을 통해 분석의 
>   활용 범위를 넓혀 사업 성과를 견인할 수 있는 전략적 인사이트를 주는 가치 기반 분석 단계로 
>   발전해야 함 
> ```
>
> ++ <u>기타</u> (19회)
>
> > - 기타 / 최신 빅데이터 상식 
>
> ```
> 1) DBMS 란 무엇인가? 
> - DBMS란 데이터베이스를 관리하여 응용 프로그램들이 데이터베이스를 공유하며 사용할 수 있는 
>   환경을 제공하는 소프트웨어
> - 데이터베이스를 구축하는 틀을 제공, 효율적인 데이터 검색, 저장 지능 제공 
> - 대표적인 데이터페이스 관리 시스템은 오라클, 인포믹스, 액세스 등이 존재 
> 
> 2) SQL이란 무엇인가?
> - Structured Query Language, 데이터베이스에 접근할 수 있는 데이터베이스 하부 언어 
> - 단순할 질의 기능뿐만 아니라 완전한 데이터 정의, 조작 기능을 갖춤 
> - 테이블을 단위로 연산을 수행하며 영어 문장과 비슷한 구문으로 초보자들도 비교적 쉽게 
>   사용할 수 있다. 
>   ※ 기본적인 SQL 문장은 해석할 수 있어야 함!
> ```
>
> > - Data에 관련한 기술 
>
> ```
> - 개인정보 비식별 기술 
> : 데이터 마스킹, 가명처리, 총계처리, 데이터 값 삭제 
> 
> - 무결성과 레이크 
> ① 데이터 무결성(integrity): 정확성, 일관성, 유효성, 신뢰성 보장을 위한 제한 
> 						 개체 무결성, 참조 무결성, 범위 무결성 존재 
> 						 
> ② 데이터 레이크: 수 많은 정보 속에서 의미 있는 내용을 찾기 위해 방식에 상관없이 데이터를 
>                 저장하는 시스템. 
>                 Apache Hadoop, Teradata Integrated BigData Platform 1700 같은 플랫폼
>                 으로 구성된 솔루션 제공 
>                 
> - 빅데이터 분석 기술 
> : Hadoop, Apache Spark, Smart Factory, Machine Learing & Deep Learning 
> 
> 
> - 기타
> ① 데이터양의 단위 
> : 바이트(B) → 킬로바이트(KB) → 메가바이트(MB) → 기가바이트(GB) → 테라바이트(TB)
>   → 페타바이트(PB) → 엑사바이트(EB) → 제타바이트(ZB) → 요타바이트(YB)
>   
> ② B2B 와 B2C
> B2B : 기업과 기업 사이의 거래를 기반으로 한 비즈니스 모델을 의미
> 	  기업이 필요로 하는 장비, 재로, 공사 입찰 등이 존재
> B2C : B2C란 기업과 고객 사이의 거래를 기반으로 한 비즈니스 모델을 의미
>       이동통신사, 여행회사, 신용카드회사, 옥션, 지마켓 등이 있음 
> 
> ③ 데이터의 유형 
> : 정형데이터, 반 정형 데이터, 비정형 데이터 
> ※ XML : Extensive Markup Language, 다목적 마크업 언어(태그를 이용한 언어)
>          인터넷에 연결된 시스템끼리 데이터를 쉽게 주고받을 수 있게 하여 HTML의 한계를 
>          극복할 목적으로 만들어졌다. 
>          기반언어로 XHTML, SVG등이 존재 
> ```
>
> 

**2장: 데이터 분석 기획**

> 2.2.2 <u>분석 거버넌스 체계 수립</u> (27회)

```
1. 마스터 플랜 수립 
- 비즈니스 관점에서 도출된 다양한 분석과제들을 기업에 적용시키기 위해서는 적용 우선순위를 
  평가해야 한다. 
- 빅데이터의 특징인 4V를 고려한 우선순위 평가기준을 적용할 수 있어야 한다. 
- 현재 기업이 당면해 있는 데이터 분석 적용 수준을 고려하여 시급성과 난이도 평가기준에 따라
  분석 적용 우선순위를 조정할 수 있다. 
- 기업 전사관점에서의 분석 적용에 대한 단계적 로드맵과 추진 일정계획을 수립하는 것이 중요하다.

2. 분석 거버넌스 체계
- 빅데이터 시대에 진입하면서 기업의 분석 수준 및 목적에 맞게 데이터를 분석하여 적용하는 것이 
  궁극적으로 기업의 경쟁력이 될 수 있기 때문에 기업 데이터의 체계적인 관리가 필수적이다. 
- 조직 내에 분석을 효율적으로 안정적으로 적용하기 위해서는 기업에 적합한 형태의 데이터 분석 
  전문조직을 구하고, 분석 전문 인력을 양성하는 것이 매우 중요하다. 
- 분석 전문 인력을 양성하기 위해 조직 구성원을 대상으로 분석 교육을 실시한다. 
- 마지막으로 분석을 조직의 문화로 정착시키기 위한 변화관리 노력이 필요하다. 
```



**3장: 데이터 분석**

> 3.2.2 R기초 (10회)
>
> > - 변수 다루기
>
> ```
> - R에서는 변수명만 선언하고 값을 할당하면 자료형태를 스스로 인식하고 선언
> - 화면에 프린트하고자 할 때, print()를 사용해도 되지만 변수 값만 표현해도 내용을 출력 
> - 변수의 값을 할당할 때는 대입연산자(<-, <<-, = ->, ->>)를 사용할 수 있음
>   (단, <- 추천)
> - 메모리에 불필요한 변수가 있는지 확인하기 위해서는 ls()를 활용하고 
>   삭제는 rm()을 활용 
> ```
>
> > - 기본적인 통계량 계산 
>
> ```
> ① 평균 : mean()					② 중간값 : median()
> ③ 표준편차 : sol()				   ④ 분산 : var()
> ⑤ 공분산 : cov()				    ⑥ 상관계수 : cor()
> ```
>
> > - 함수의 생성 및 활용 
>
> ```
> 1) R은 함수형 언어이기 때문에 프로그래머가 직접 활용 가능한 함수를 생성하여 활용 
> 2) 함수는 function(매개변수1, 매개변수2, ...) 선언하고 표현식이 2줄 이상인 경우에는 
>    {}로 묶어서 함수의 범위를 설정
> 3) 표현식은 변수 할당, 조건문(if문)과 반복문(for문, while문, repeat문) 그리고 
>    전달값(return)으로 구성 
>    
>    ※ 연산자 우선순위 숙지하고 있을 것 
> ```
>
> 3.3.1 데이터 변경 및 요약 (11회)
>
> > - 데이터 마트란?
> >
> > : 데이터 웨어하우스와 사용자 사이의 중간층에 우치한 것으로, 하나의 주제 또는 하나의 부서 중심의 데이터 웨어하우스라고 할 수 있다. 
> >
> > 데이터 마트를 어떻게 구축하느냐에 따라 분석 효과는 크게 차이난다. 
>
> > - 요약 변수 
>
> ```
> 정의 : 수집된 정보를 분석에 맞게 종합하는 변수, 데이터 마트에서 가장 기본적인 변수로            총 구매 금액, 횟수, 구매여부 등이 있으며 많은 모델이 공통으로 사용될 수 있어 
>        재활용성이 높다. 
> 기간별 구매 금액, 횟수, 여부/ 
> 위클리쇼퍼/ 
> 상품별 구매 금액, 횟수, 여부/
> 상품별구매순서/
> 유통 채널별 구매 금액/ 
> 단어 빈도/ 
> 초기 행동변수/ 
> 트랜드 변수/
> 결측값과 이상값 처리/
> 연속행 변수의 구간화 etc...
> ```
>
> > - 파생변수
>
> ```
> 정의 : 사용자가 특정 조건을 만족하거나 특정 함수에 의해 값을 만들어 의미를 부여하는 
>           변수로서 매우 주관적일 수 있으므로 논리적 타당성을 갖출 필요가 있다. 
> 근무시간 구매지수/
> 주 구매 매장 변수/
> 주 활동 지역 변수/
> 주 구매 상품 변수/
> 구매상품 다양성 변수/
> 선호하는 가격대 변수/
> 시즌 선호 고객 변수/
> 라이프 스테이지 변수/
> 라이프 스타일 변수/
> 행사민감 변수/
> 휴면가망 변수/
> 최대가치 변수/
> 최적 통화시간 변수/ etc....
> ```
>
> > - 관련 패키지들 
>
> ```
> 1) reshape 패키지
> - melt(): 데이터를 DB구조로 녹이는 함수 
> - cast(): 새로운 구조로 데이터를 만드는 함수 
> 
> 2) sqldf 패키지
> - R에서 sql명령어를 사용가능하게 해주는 패키지로 SAS의 proc sql과 같은 기능이다. 
> - head([df]): sqld("select * from [df] limit 6")
> - subset([df], [col] %in% c("BF", "HF") : sqldf("select * from [df] where [col] in('BF', 'HF')"))
> - merge([df1], [df2]): sqldf("select * from [df1], [df2]")
> 
> 3) plyr 패키지
> - apply 함수를 기반으로 데이터와 출력변수를 동시에 배열로 치환하여 처리하는 패키지로 
>   split-apply-combine 방식으로 데이터를 분리하고 처리한 다음, 다시 결합하는 등 필수적인 
>   데이터 처리기능을 제공한다. 
>   
> 4) data.table 패키지 
> - R에서 가장 많이 사용하는 데이터 핸들링 패키지 중 하나로 대용량 데이터의 탐색, 연산,
>   병합에 유용하다. 
> - 기존 data.frame 방식보다 월등히 빠른 속도이다. 
>   - 특정 column을 key값으로 색인을 지정한 후 데이터를 처리 
>   - 빠른 grouping과 ordering, 짧은 문장 지원 측면에서 데이터프레임보다 유용 
> ```
>
> 
>
> 3.3.3 기초 분석 및 데이터 관리 (11회) 
>
> > - 변수의 구간화 
>
> ```
> 1) 신용평가모형 또는 고객 세분화 등의 시스템으로 모형을 적용하기 위해서는 각 변수들을 
>    구간화하여 점수를 적용하는 방식이 활용
>    
> 2) binning : 연속형 변수를 범주형 변수로 변환하기 위해 50개 이하의 구간에 동일한 수의 
>    데이터를 할당하여 의미를 파악하면서 구간을 축소하는 방법 
>    
> 3) 의사결정 나무: 모형을 통해 연속형 변수를 범주형 변수로 변환하는 방법 
> ```
>
> > - 결측값 처리
>
> ```
> 1) 변수에 데이터가 비어 있는 경우 → NA, ., 999999, Unknown, Not Answer등으로 표현 
> 
> 2) 단순 대치법(single Imputation)
> 	① completes analysis : 결측값의 레코드를 삭제
> 	② 평균대치법 : 관측 및 실험을 통해 얻어진 데이터의 평균으로 대치 
> 	  - 비조건부 평균 대치법 : 관측 데이터의 평균으로 대치 
> 	  - 조건부 평균 대치법 : 회귀분석을 통해 데이터를 대치 
> 	③ 단순확률 대치법: 평균대치법에서 추정량 표준 오차의 과소 추정문제를 보완한 방법 
> 					 Hot-deck방법, nearest Neighbor 방법이 있다. 
> 3) 다중 대치법(multiple Imputation)
>    : 단순 대치법을 m번 실시하여 m개의 가상적 자료를 만들어 대치하는 방법 
> ```
>
> > - 이상값 처리 
>
> ```
> - bad data : 잘못 입력된 값이나 분석 목적에 부합되지 않는 값인 경우로 삭제
> - 이상값 : 의도하지 않는 현상으로 입력된 값이나, 의도된 극단값인 경우 활용 
> 
> 1) 이상값의 인식 
>    - 평균으로부터 3 표준편차 떨어진 값
>    - 기하평균보다 2.5 표준편차 이상 떨어진 값 
>    - 1사분위와 3사분위 값에서 범위보다 2.5배이상 떨어진 값 
>    
> 2) 이상값의 처리 
>    - 절단(trimming) : 이상값이 포함된 레코드를 삭제 
>    - 조정(winsorizing) : 이상값을 상한 또는 하한 값으로 조정 
> ```
>
> 
>
> 3.4.1 <u>통계 분석의 이해</u> (45회)
>
> > - 통계
>
> ```
> 1) 통계: 특정집단을 대상으로 수행한 조사나 실험을 통해 나온 결과에 대한 요약된 형태의 표현
> 2) 통계자료의 획득 방법: 총조사(census)와 표본조사(sampling)
> 3) 샘플링 기법: 단순랜덤추출(cluster sampling), 계통추출법(systematic sampling), 
>                 집략추출법(cluster sampling),  층화추출법(stratified random sampling)
> 4) 자료의 형태: 명목척도, 순서척도, 구간척도, 비율척도		   
> ```
>
> > - 통계분석
>
> ```
> 1) 통계적 추론(statistical inference): 추정, 가설검정, 예측
> 2) 기술통계(descriptive statistic): 평균, 표준편차, 중위수, 최빈값, 그래프의 표현 
> ```
>
> > - 확률 및 확률분포 
>
> ```
> 1) 확률변수(random variable): 특정값이 나타날 가능성이 확률적으로 주어지는 변수 
> 2) 이산형 확률분포(discrete distribution)
>    :베르누이 분포, 이항분포, 기하분포, 다항분포, 포아송 분포 
> 3) 연속형 확률분포(continuous distribution)
>    :균일분포, 정규분포, 지수분포, t분포, F분포, x^2분포 
> ```
>
> > - 추정 및 가설검정
>
> ```
> 1) 추정: 표본으로부터 미지의 모수를 추측하는 것 
> 2) 점추정(point estimation)
>    - '모수가 특정한 값일 것' 이라고 추정하는 것, 평균, 표준편차, 중앙값 등을 추정 
>    - 점추정 조건: 불편성, 효율성, 일치성, 충족성 
>    
> 3) 구간 추정(interval estimation)
>    - 점추정을 보완하기 위해 모수가 특정 구간에 있을 것이라고 추정하는 것 
>    - 모분산을 알거나 대표본의 경우 → '표준정규분포'를 활용
>    - 모분산을 모르거나 소표본의 경우 → 't분포'를 활용 
>    
> 4) 가설 검정
>    - 귀무가설(null hypothesis) vs 대립가설(alternative hypothesis)
>    - 1종 오류(type 1 error) : 귀무가설이 옳은데 귀무가설을 기각하는 오류
>    - 2종 오류(type 2 error) : 귀무가설이 옳지 않은데 귀무가설을 채택하는 오류 
>    - 1종 오류의 크기를 0.1, 0.05, 0.01 로 고정시키고 2종 오류가 최소가 되도록 기각열 설정 
> ```
>
> 
>
> 3.4.2 기초 통계 분석(+추론통계) (28회)
>
> 3.4.3 회귀분석 (34회)
>
> 3.4.4 시계열 분석 (14회)
>
> 3.4.6 <u>주성분분석</u> (14회)
>
> 3.5.1 <u>데이터마이닝 개요</u> (17회) 
>
> 3.5.2 분류 분석 (18회)
>
> 3.5.3 예측 분석  (12회)
>
> 3.5.4 <u>군집 분석</u>  (29회)
>
> 3.5.5 <u>연관 분석</u>  (20회 )