#### 출제 빈도 

14회차 시험까지 총 10회 이상 기출된 문제에 대해서 중점적으로 정리해보자 

17년도부터 출제 대상에서 제외된 파트는 제외한다. 

<u>밑줄</u>표시는 매년 출제된 문제이다. 

**1장: 데이터의 이해**

> 1.1.1 <u>데이터와 정보</u> (14회)
>
> > - 데이터 : 개별 데이터 자체로 의미가 중요하지 않은 객관적인 사실
> > - 정보 : 데이터의 *가공, 처리*와 데이터 간 *연관관계* 속에서 의미가 도출 
> > - 지식 : 정보를 구조화하여 유의미한 정보를 분류하고 *개인적 경험*을 결합하여 내재화한 것 
> > - 지혜 : 지식의 축적과 *아이디어*가 결합된 창의적 산물 
>
> 1.1.3 데이터 베이스 활용 (10회)
>
> > - 기업 내부 데이터 베이스
>
> ```
> OLTP, OLAP
> CRM, SCM, ERP, RTE, BI, EAI, KMS
> 제조부문, 금융부문, 유통부문 에서의 데이터 활용 
> ```
>
> > - 사회기반구조로서의 데이터 베이스 
>
> ```
> EDI, VAN, CALS
> 물류부문(CVO, PORT-MIS)
> 지리/교통부문(GIS, RS, GPS, ITS, LBS, SIM)
> 의료부문(PACS, U헬스)
> ```
>
> 
>
> 1.2.1 빅데이터의 이해 (11회)
>
> > - 빅데이터 정의
>
> ```
> McKinsey: 일반적인 데이터베이스 소프트웨어로 저장, 관리, 분석할 수 있는 범위를 초과하는 
> 		 규모의 데이터이다. 
> IDC:      다양한 종류의 대규모 데이터로부터 저렴한 비용으로 가치를 추출하고 데이터의 
> 		 초고속 수집/발굴/분석을 지원하도록 고안된 차세대 기술 및 아키텍처이다. 
> 가트너 그룹(Doug Laney)의 3V - Volumn, Variety, Velocity 
> ```
>
> > - 빅데이터 정의의 범주 및 효과 
>
> ``` 
> 1. 데이터 변화(규묘, 형태, 속도)
> 2. 기술변화(새로운 데이터 처리, 저장 분석 기술, 클라우드 컴퓨팅 활용)
> 3. 인재, 조직 변화(data scientist, 데이터 중심 조직)
> ```
>
> > - 빅데이터에 거는 기대의 비유적 표현 
>
> ```
> 산업혁명의 석탄과 철(생산성 ↑,사회,경제,문화,생활 전반에 혁명적 변화)
> 21세기의 원유(생산성↑, 새로운 범주의 산업 생성)
> 렌즈(현미경~생물학 → 데이터~산업발전)(예: Google Ngram Viewer)	
> 플랫폼 (='공동 활용의 목적으로 구축된 유무형의 구조물')
> ```
>
> > - 빅데이터가 만들어내는 본질적인 변화 
>
> ```
> 사전처리 → 사후처리 
> 표본처리 → 전수조사 
> 질 → 양
> 인과관계 → 상관관계 
> ```
>
> 1.2.2 빅데이터의 가치와 영향 (11회)
>
> > - 빅데이터 시대에서는 특정 데이터의 가치를 측정하는 것이 쉽지 않다 
>
> ```
> 1. 데이터 활용 방식 : 재사용, 재조합(mashup), 다목적용 개발
> 	→특정 데이터를 언제 어디서 누가 활용할지 ???
> 2. 새로운 가치 창출(=기존에 없던 가치)
> 3. 분석 기술 발전(현재 無가치 → 새로운 분석 기법 등장으로 거대한 가치)
> ```
>
> > - 빅데이터의 영향
>
> ```
> - 맥킨지가 언급한 빅데이터가 가치를 만들어내는 다섯가지 방식
> ① 투명성 제고로 연구개발 및 관리 효율성 제고
> ② 시뮬레이션을 통한 수요 포착 및 주요 변수 탐색으로 경쟁력 강화
> ③ 고객 세분화 및 맞춤 서비스 제공 
> ④ 알고리즘을 활용한 의사결정 보조 혹은 대체
> ⑤ 비즈니스 모델과 제품, 서비스의 혁신 
> 
> - 빅데이터의 가치 창출 방식이 시장에 있는 플레이어(기업,정부,소비자)에 미치는 영향
>  「① 기업: 혁신, 경쟁력 제고, 생산성 향상	
>    ② 정부: 환경 탐색, 상황 분석, 미래 대응  	→  생활 전반의 스마트화 	 
>    ③ 개인: 목적에 따라 활용 				」
> ```
>
> > - 빅데이터의 활용 기본 테크닉 7가지
>
> ```
> 연관규칙학습, 유형분석, 유전 알고리즘, 기계 학습, 회귀분석, 감정 분석, 소셜네트워크 분석 
> ```
>
> 1.3.1 빅데이터 분석과 전략 인사이트 (11회)
>
> > - 빅데이터의 회의론 원인 
>
> ```
> ① 부정적 학습효과- 과거의 고객관계관리(CRM): 공포 마케팅, 투자대비 효과 미흡 
> ② 부적절한 성공사례- 빅데이터가 필요없는 분석사례, 기존 CRM분석 성과 
> ```
>
> > - 싸이월드 퇴보 원인 
>
> ```
> ① OLAP과 같은 분석 인프라로 존재했으나 중요한 의사결정에 데이터 분석 활용 못함
> ② 웹로그 분석을 통한 일차원적 분석만 집중
> ③ 소셜 네트워킹 활동 특성과 관련된 분석을 위한 프레임워크나 평가지표도 없었음
> ④ 트랜드 변화가 사업모델에 미치는 영향에 대한 전략적 통찰(insight)을 가지지 못함 
> ```
>
> > - 전략적 통찰이 없는 분석의 함정 
>
> ```
> - 단순히 일차원적인 분석의 반복은 해당부서의 업무 영역에서는 효과적이지만 기업의 환경
>   변화와 고객 변화에 전략적으로 대처하기 힘듬 
>   
> - 전략적 통찰력의 창출에 초점을 맞춰 분석을 활용하면 사업의 중요한 기회를 발견할 수 있음 
> 
> - 최고가 되기 위해서는 일차원적인 분석을 통해 분석 경험을 늘리고 작은 성공을 통해 분석의 
>   활용 범위를 넓혀 사업 성과를 견인할 수 있는 전략적 인사이트를 주는 가치 기반 분석 단계로 
>   발전해야 함 
> ```
>
> ++ <u>기타</u> (19회)
>
> > - 기타 / 최신 빅데이터 상식 
>
> ```
> 1) DBMS 란 무엇인가? 
> - DBMS란 데이터베이스를 관리하여 응용 프로그램들이 데이터베이스를 공유하며 사용할 수 있는 
>   환경을 제공하는 소프트웨어
> - 데이터베이스를 구축하는 틀을 제공, 효율적인 데이터 검색, 저장 지능 제공 
> - 대표적인 데이터페이스 관리 시스템은 오라클, 인포믹스, 액세스 등이 존재 
> 
> 2) SQL이란 무엇인가?
> - Structured Query Language, 데이터베이스에 접근할 수 있는 데이터베이스 하부 언어 
> - 단순할 질의 기능뿐만 아니라 완전한 데이터 정의, 조작 기능을 갖춤 
> - 테이블을 단위로 연산을 수행하며 영어 문장과 비슷한 구문으로 초보자들도 비교적 쉽게 
>   사용할 수 있다. 
>   ※ 기본적인 SQL 문장은 해석할 수 있어야 함!
> ```
>
> > - Data에 관련한 기술 
>
> ```
> - 개인정보 비식별 기술 
> : 데이터 마스킹, 가명처리, 총계처리, 데이터 값 삭제 
> 
> - 무결성과 레이크 
> ① 데이터 무결성(integrity): 정확성, 일관성, 유효성, 신뢰성 보장을 위한 제한 
> 						 개체 무결성, 참조 무결성, 범위 무결성 존재 
> 						 
> ② 데이터 레이크: 수 많은 정보 속에서 의미 있는 내용을 찾기 위해 방식에 상관없이 데이터를 
>                 저장하는 시스템. 
>                 Apache Hadoop, Teradata Integrated BigData Platform 1700 같은 플랫폼
>                 으로 구성된 솔루션 제공 
>                 
> - 빅데이터 분석 기술 
> : Hadoop, Apache Spark, Smart Factory, Machine Learing & Deep Learning 
> 
> 
> - 기타
> ① 데이터양의 단위 
> : 바이트(B) → 킬로바이트(KB) → 메가바이트(MB) → 기가바이트(GB) → 테라바이트(TB)
>   → 페타바이트(PB) → 엑사바이트(EB) → 제타바이트(ZB) → 요타바이트(YB)
>   
> ② B2B 와 B2C
> B2B : 기업과 기업 사이의 거래를 기반으로 한 비즈니스 모델을 의미
> 	  기업이 필요로 하는 장비, 재로, 공사 입찰 등이 존재
> B2C : B2C란 기업과 고객 사이의 거래를 기반으로 한 비즈니스 모델을 의미
>       이동통신사, 여행회사, 신용카드회사, 옥션, 지마켓 등이 있음 
> 
> ③ 데이터의 유형 
> : 정형데이터, 반 정형 데이터, 비정형 데이터 
> ※ XML : Extensive Markup Language, 다목적 마크업 언어(태그를 이용한 언어)
>          인터넷에 연결된 시스템끼리 데이터를 쉽게 주고받을 수 있게 하여 HTML의 한계를 
>          극복할 목적으로 만들어졌다. 
>          기반언어로 XHTML, SVG등이 존재 
> ```
>
> 

**2장: 데이터 분석 기획**

> 2.2.2 <u>분석 거버넌스 체계 수립</u> (27회)

```
1. 마스터 플랜 수립 
- 비즈니스 관점에서 도출된 다양한 분석과제들을 기업에 적용시키기 위해서는 적용 우선순위를 
  평가해야 한다. 
- 빅데이터의 특징인 4V를 고려한 우선순위 평가기준을 적용할 수 있어야 한다. 
- 현재 기업이 당면해 있는 데이터 분석 적용 수준을 고려하여 시급성과 난이도 평가기준에 따라
  분석 적용 우선순위를 조정할 수 있다. 
- 기업 전사관점에서의 분석 적용에 대한 단계적 로드맵과 추진 일정계획을 수립하는 것이 중요하다.

2. 분석 거버넌스 체계
- 빅데이터 시대에 진입하면서 기업의 분석 수준 및 목적에 맞게 데이터를 분석하여 적용하는 것이 
  궁극적으로 기업의 경쟁력이 될 수 있기 때문에 기업 데이터의 체계적인 관리가 필수적이다. 
- 조직 내에 분석을 효율적으로 안정적으로 적용하기 위해서는 기업에 적합한 형태의 데이터 분석 
  전문조직을 구하고, 분석 전문 인력을 양성하는 것이 매우 중요하다. 
- 분석 전문 인력을 양성하기 위해 조직 구성원을 대상으로 분석 교육을 실시한다. 
- 마지막으로 분석을 조직의 문화로 정착시키기 위한 변화관리 노력이 필요하다. 
```



**3장: 데이터 분석**

> 3.2.2 R기초 (10회)
>
> > - 변수 다루기
>
> ```
> - R에서는 변수명만 선언하고 값을 할당하면 자료형태를 스스로 인식하고 선언
> - 화면에 프린트하고자 할 때, print()를 사용해도 되지만 변수 값만 표현해도 내용을 출력 
> - 변수의 값을 할당할 때는 대입연산자(<-, <<-, = ->, ->>)를 사용할 수 있음
>   (단, <- 추천)
> - 메모리에 불필요한 변수가 있는지 확인하기 위해서는 ls()를 활용하고 
>   삭제는 rm()을 활용 
> ```
>
> > - 기본적인 통계량 계산 
>
> ```
> ① 평균 : mean()					② 중간값 : median()
> ③ 표준편차 : sol()				   ④ 분산 : var()
> ⑤ 공분산 : cov()				    ⑥ 상관계수 : cor()
> ```
>
> > - 함수의 생성 및 활용 
>
> ```
> 1) R은 함수형 언어이기 때문에 프로그래머가 직접 활용 가능한 함수를 생성하여 활용 
> 2) 함수는 function(매개변수1, 매개변수2, ...) 선언하고 표현식이 2줄 이상인 경우에는 
>    {}로 묶어서 함수의 범위를 설정
> 3) 표현식은 변수 할당, 조건문(if문)과 반복문(for문, while문, repeat문) 그리고 
>    전달값(return)으로 구성 
>    
>    ※ 연산자 우선순위 숙지하고 있을 것 
> ```
>
> 3.3.1 데이터 변경 및 요약 (11회)
>
> > - 데이터 마트란?
> >
> > : 데이터 웨어하우스와 사용자 사이의 중간층에 우치한 것으로, 하나의 주제 또는 하나의 부서 중심의 데이터 웨어하우스라고 할 수 있다. 
> >
> > 데이터 마트를 어떻게 구축하느냐에 따라 분석 효과는 크게 차이난다. 
>
> > - 요약 변수 
>
> ```
> 정의 : 수집된 정보를 분석에 맞게 종합하는 변수, 데이터 마트에서 가장 기본적인 변수로            총 구매 금액, 횟수, 구매여부 등이 있으며 많은 모델이 공통으로 사용될 수 있어 
>        재활용성이 높다. 
> 기간별 구매 금액, 횟수, 여부/ 
> 위클리쇼퍼/ 
> 상품별 구매 금액, 횟수, 여부/
> 상품별구매순서/
> 유통 채널별 구매 금액/ 
> 단어 빈도/ 
> 초기 행동변수/ 
> 트랜드 변수/
> 결측값과 이상값 처리/
> 연속행 변수의 구간화 etc...
> ```
>
> > - 파생변수
>
> ```
> 정의 : 사용자가 특정 조건을 만족하거나 특정 함수에 의해 값을 만들어 의미를 부여하는 
>           변수로서 매우 주관적일 수 있으므로 논리적 타당성을 갖출 필요가 있다. 
> 근무시간 구매지수/
> 주 구매 매장 변수/
> 주 활동 지역 변수/
> 주 구매 상품 변수/
> 구매상품 다양성 변수/
> 선호하는 가격대 변수/
> 시즌 선호 고객 변수/
> 라이프 스테이지 변수/
> 라이프 스타일 변수/
> 행사민감 변수/
> 휴면가망 변수/
> 최대가치 변수/
> 최적 통화시간 변수/ etc....
> ```
>
> > - 관련 패키지들 
>
> ```
> 1) reshape 패키지
> - melt(): 데이터를 DB구조로 녹이는 함수 
> - cast(): 새로운 구조로 데이터를 만드는 함수 
> 
> 2) sqldf 패키지
> - R에서 sql명령어를 사용가능하게 해주는 패키지로 SAS의 proc sql과 같은 기능이다. 
> - head([df]): sqld("select * from [df] limit 6")
> - subset([df], [col] %in% c("BF", "HF") : sqldf("select * from [df] where [col] in('BF', 'HF')"))
> - merge([df1], [df2]): sqldf("select * from [df1], [df2]")
> 
> 3) plyr 패키지
> - apply 함수를 기반으로 데이터와 출력변수를 동시에 배열로 치환하여 처리하는 패키지로 
>   split-apply-combine 방식으로 데이터를 분리하고 처리한 다음, 다시 결합하는 등 필수적인 
>   데이터 처리기능을 제공한다. 
>   
> 4) data.table 패키지 
> - R에서 가장 많이 사용하는 데이터 핸들링 패키지 중 하나로 대용량 데이터의 탐색, 연산,
>   병합에 유용하다. 
> - 기존 data.frame 방식보다 월등히 빠른 속도이다. 
>   - 특정 column을 key값으로 색인을 지정한 후 데이터를 처리 
>   - 빠른 grouping과 ordering, 짧은 문장 지원 측면에서 데이터프레임보다 유용 
> ```
>
> 
>
> 3.3.3 기초 분석 및 데이터 관리 (11회) 
>
> > - 변수의 구간화 
>
> ```
> 1) 신용평가모형 또는 고객 세분화 등의 시스템으로 모형을 적용하기 위해서는 각 변수들을 
>    구간화하여 점수를 적용하는 방식이 활용
>    
> 2) binning : 연속형 변수를 범주형 변수로 변환하기 위해 50개 이하의 구간에 동일한 수의 
>    데이터를 할당하여 의미를 파악하면서 구간을 축소하는 방법 
>    
> 3) 의사결정 나무: 모형을 통해 연속형 변수를 범주형 변수로 변환하는 방법 
> ```
>
> > - 결측값 처리
>
> ```
> 1) 변수에 데이터가 비어 있는 경우 → NA, ., 999999, Unknown, Not Answer등으로 표현 
> 
> 2) 단순 대치법(single Imputation)
> 	① completes analysis : 결측값의 레코드를 삭제
> 	② 평균대치법 : 관측 및 실험을 통해 얻어진 데이터의 평균으로 대치 
> 	  - 비조건부 평균 대치법 : 관측 데이터의 평균으로 대치 
> 	  - 조건부 평균 대치법 : 회귀분석을 통해 데이터를 대치 
> 	③ 단순확률 대치법: 평균대치법에서 추정량 표준 오차의 과소 추정문제를 보완한 방법 
> 					 Hot-deck방법, nearest Neighbor 방법이 있다. 
> 3) 다중 대치법(multiple Imputation)
>    : 단순 대치법을 m번 실시하여 m개의 가상적 자료를 만들어 대치하는 방법 
> ```
>
> > - 이상값 처리 
>
> ```
> - bad data : 잘못 입력된 값이나 분석 목적에 부합되지 않는 값인 경우로 삭제
> - 이상값 : 의도하지 않는 현상으로 입력된 값이나, 의도된 극단값인 경우 활용 
> 
> 1) 이상값의 인식 
>    - 평균으로부터 3 표준편차 떨어진 값
>    - 기하평균보다 2.5 표준편차 이상 떨어진 값 
>    - 1사분위와 3사분위 값에서 범위보다 2.5배이상 떨어진 값 
>    
> 2) 이상값의 처리 
>    - 절단(trimming) : 이상값이 포함된 레코드를 삭제 
>    - 조정(winsorizing) : 이상값을 상한 또는 하한 값으로 조정 
> ```
>
> 
>
> 3.4.1 <u>통계 분석의 이해</u> (45회)
>
> > - 통계
>
> ```
> 1) 통계: 특정집단을 대상으로 수행한 조사나 실험을 통해 나온 결과에 대한 요약된 형태의 표현
> 2) 통계자료의 획득 방법: 총조사(census)와 표본조사(sampling)
> 3) 샘플링 기법: 단순랜덤추출(cluster sampling), 계통추출법(systematic sampling), 
>                 집략추출법(cluster sampling),  층화추출법(stratified random sampling)
> 4) 자료의 형태: 명목척도, 순서척도, 구간척도, 비율척도		   
> ```
>
> > - 통계분석
>
> ```
> 1) 통계적 추론(statistical inference): 추정, 가설검정, 예측
> 2) 기술통계(descriptive statistic): 평균, 표준편차, 중위수, 최빈값, 그래프의 표현 
> ```
>
> > - 확률 및 확률분포 
>
> ```
> 1) 확률변수(random variable): 특정값이 나타날 가능성이 확률적으로 주어지는 변수 
> 2) 이산형 확률분포(discrete distribution)
>    :베르누이 분포, 이항분포, 기하분포, 다항분포, 포아송 분포 
> 3) 연속형 확률분포(continuous distribution)
>    :균일분포, 정규분포, 지수분포, t분포, F분포, x^2분포 
> ```
>
> > - 추정 및 가설검정
>
> ```
> 1) 추정: 표본으로부터 미지의 모수를 추측하는 것 
> 2) 점추정(point estimation)
>    - '모수가 특정한 값일 것' 이라고 추정하는 것, 평균, 표준편차, 중앙값 등을 추정 
>    - 점추정 조건: 불편성, 효율성, 일치성, 충족성 
>    
> 3) 구간 추정(interval estimation)
>    - 점추정을 보완하기 위해 모수가 특정 구간에 있을 것이라고 추정하는 것 
>    - 모분산을 알거나 대표본의 경우 → '표준정규분포'를 활용
>    - 모분산을 모르거나 소표본의 경우 → 't분포'를 활용 
>    
> 4) 가설 검정
>    - 귀무가설(null hypothesis) vs 대립가설(alternative hypothesis)
>    - 1종 오류(type 1 error) : 귀무가설이 옳은데 귀무가설을 기각하는 오류
>    - 2종 오류(type 2 error) : 귀무가설이 옳지 않은데 귀무가설을 채택하는 오류 
>    - 1종 오류의 크기를 0.1, 0.05, 0.01 로 고정시키고 2종 오류가 최소가 되도록 기각열 설정 
> ```
>
> > - 비모수 검정 
>
> ```
> 1) 모집단의 분포에 대한 아무 제약을 가하지 않고 실행하는 검정 
> 2) 분포의 형태가 동일하다 또는 동일하지 않다 라는 식으로 가설 설정
> 3) 순위나 두 관측값 차이의 부호를 이용해 검정 
> 예) 부호검정(sign test), 윌콕슨의 순위합검정(rank sum test), 윌콕슨의 부호순위합검정(Wilcoxon signed rank test), 만-위트니의 U검정, 런 검정(run test), 스피어만의 순위상관계수 
> ```
>
> 3.4.2 기초 통계 분석(+추론통계) (28회)>
>
> > - 기술통계(Descriptive Statistics)
>
> ```
> - 자료의 특성을 표, 그림, 통계량 등을 사용하여 쉽게 파악할 수 있도록 정리/요약 하는 것 
> - 자료를 요약하는 기초적 통계를 의미한다. 
> - 데이터 분석에 앞서 데이터의 대략적인 통계적 수치를 계산해봄으로써 데이터에 대한 대략적인
>   이해와 앞으로 분석에 대한 통찰력을 얻기에 유리하다. 
> ```
>
> > - 통계량에 의한 자료 분석 
>
> ```
> 1) 중심위치: 평균, 중앙값, 최빈값
> 2) 산포의 척도: 분산, 표준편차, 범위, 사분위수범위, 변동계수, 표준오차
> 3) 분포의 형태: 왜도, 첨도 
> ```
>
> > - 그래프를 통한 자료 분석 
>
> ```
> 1) 범주형 자료 : 막대그래프와 파이차트 등 
> 2) 연속형 자료 : 히스토그램, 줄기-잎 그림, 상자그림 등 
> 3) 시계열 자료 : 꺾은선 그래프 
> ```
>
> > - 인과관계의 이해
>
> ```
> 1) 종속변수 vs 독립변수 
> 2) 산점도(Scatter plot)로 확인할 수 있는 것 
>    - 두 변수 사이의 선형관계가 성립하는가
>    - 두 변수 사이의 함수관계가 성립하는가 
>    - 이상값의 존재 여부와 몇 개 집단으로 구분되는지를 확인
> 3) 공분산(covariance)
>    - 두 확률변수 간의 방향성을 확인 
> ```
>
> > - 상관분석(correlation analysis)
>
> ```
> 1) 두 변수간의 상관 정도를 상관계수를 통해 확인할 수 있음 
> 2) 상관계수는 -1에서 1사이의 값으로 양수는 양의상관, 음수는 음의 상관을 표현
> 3) 상관계수가 0이면 데이터 간의 상관이 없다
> 4) 피어스 상관계수: 등간척도 이상으로 측정된 두 변수들의 상관관계 측정 
> 5) 스피어만 순서상관계수: 순서 또는 서열 척도인 두 변수들 간의 상관관계를 측정 
> 6) 예) R프로그램 사용시 
>    cor(x,y,method = 'spearman')
>    rcorr(as.matrix(data명), type='spearman')
> ```
>
> 3.4.3 회귀분석 (34회)
>
> > - 회귀분석(regression Analysis)
>
> ```
> - 하나 또는 그 이상의 독립변수들이 종속변수에 미치는 영향을 추정할 수 있는 통계 기법
> - 단순선형회귀분석: 독립변수가 하나인 경우
> - 다중선형회귀분석: 독립변수가 둘 이상인 경우 
> ```
>
> > - 회귀분석의 검정 
>
> ```
> 1) 회귀식(모형)에 대한 검증: F검정
> 2) 회귀계수들에 대한 검증: t검정 
> 3) 모형의 설명력: 결정계수 R^2 = 회귀제곱합/전체제곱합 = SSR/SST
>    - 단순회귀분석의 결정계수는 상관계수 r의 제곱과 같음
> 4) 선형회귀분석의 가정(데이터가 전제로 하는 가정)
>    - 선형성: 입력변수와 출력변수의 관계가 선형관계에 있음 
>       → 선형 입력변수와 출력변수의 산점도로 확인  
>       
>    - 독립성: 잔차와 독립변인의 값이 관련이 없어야 함 
>    - 등분산성: 독립변인의 모든 값에 대한 오차들의 분산이 일정
>    - 비상관성: 관측치들의 잔차들끼리 상관이 없어야 함 
>    - 정상성: 잔차항이 정규분포를 이뤄야함 
>    	  → 잔차와 출력변수의 산점도로 확인 
> ```
>
> > - 다중 선형회귀식
>
> ```
> 1) 다중공선성(multicollinearity)
>    : 다중 회귀분석에서 설명변수들 사이에 선형관계가 존재하면 회귀계수의 정확한 추정이 곤란
> 2) 다중공선성 검사방법
>    - 분산팽창요인(VIF): 10보다 크면 심각한 문제
>    - 상태지수 : 10이상이면 문제가 있다고 보고, 30보다 크면 심각
>    - 선형관계가 강한 변수 제거 
> ```
>
> > - 변수 선택법(variable selection)
>
> ```
> 1) 모든 가능한 조합: 모든 가능한 독립변수들이 조합에 대한 회귀모형을 분석해 가장 적합한 
>                     모형 선택
> 2) 전진선택법(foward selection)
>    : 절편만 있는 상수모형으로부터 시작해 중요하다고 생각되는 설명변수부터 차례로 모형에 
>      추가
>      → 이해 쉬움, 많은 변수에서 활용 가능, 변수 값의 작은 변동에 결과가 달라져 안정성이 
>        부족 
> 3) 후진소거법(backward selection)
>    : 독립변수 후보 모두를 포함한 모형에서 가장 적은 영향을 주는 변수부터 하나씩 제거 
>      → 전체 변수들의 정보를 이용 가능, 변수가 많은 경우 활용이 어려움, 안정성 부족 
> 4) 단계별 방법(stepwise method)
>    : 전진선택법에 의해 변수를 추가하면서 새롭게 추가된 변수에 기인해 기존 변수가 그 중요도
>      가 약화되면 해당변수를 제거하는 등 단계별로 추가 또는 삭제되는 변수를 검토해 더 이상 
>      없을 때 중단 
> ```
>
> 3.4.4 시계열 분석 (14회)
>
> > - 시계열 자료(time series)
>
> ```
> - 시간의 흐름에 따라 관찰된 값들 -> 시계열 데이터의 분석 목적 
> 1) 미래의 값을 예측한다
> 2) 시계열 데이터의 특성 파악(경향, 주기, 계절성, 불규칙성 등)
> ```
>
> > - 정상성(3가지를 모두 만족)
>
> ```
> 1) 평균이 일정하다(모든 시점에서 일정한 평균을 가진다)
> 2) 분산도 일정하다
> 3) 공분산도 특정 시점에서 t,s에 의존하지 않고 일정하다 
> ```
>
> > - 정상시계열의 특징
>
> ```
> 1) 어떤 시점에서 평균과 분산, 그리고 특정한 시차의 길이를 갖는 자기공분산을 측정동일한 
>    값이다
> 2) 항상 평균값으로 회귀하려는 경향이 있고 변동은 평균값 주변에서 일정한 폭을 유지한다
> 3) 비정상상시계열은 특정 기간의 시계열 자료로 부터 얻은 정보를 다른 시기로 일반화 할 수 
>    없다.
> ```
>
> > - 시계열 모형 
> >
> > 1) 자기회귀모형(AR모형, autoregressive model)
> >
> > 2) 이동평균모형(MA모형, moving average model)
> >
> > 3) 자기회귀누적이동평균 모형(ARIMA(p,d,q))
> >
> > 4) 분해 시계열 
>
> 3.4.6 <u>주성분분석</u> (14회)
>
> > - 주성분분석(PCA: Principal Component Analysis)
>
> ```
> - 상관관계에 있는 변수들을 결합해 상관관계가 없는 변수로 분산을 극대화하는 변수로, 
>   선형결합을 해 변수를 축약하는데 사용한다. 
>   
> 1) 요인분석 vs 주성분분석
> ① 요인분석은 몇 개의 요인(잠재된 변수)들을 추출하기 위해서 여러가지 방법이 사용될 수 있고, 
>   그 중 가장 많이 사용되는 방법이 주성분분석이다. 
> ② 공통점: 모두 데이터를 축소에 활용 - 많은 변수들을 몇 개의 변수로 축소한다. 
> ③ 차이점: 생성된 변수의 수와 이름, 생성된 변수들 간의 관계, 목표변수와의 관계 
> 
> 2) 주성분분석의 활용
> ① 여러 변수들 간의 상관성, 연관성을 이용해서 주성분차원으로 변수를 축소한다. 
> ② 회귀분석이나 의사결정나무 등 모형 개발 시 다중공선성이 존재할 경우, 상관도가 높은 
>   변수를 축소한다. 
> ③ 연관성이 높은 변수를 축소하여 군집분석을 군집화 결과, 연산속도를 개선한다. 
> ④ 기계에서 나오는 다양한 센서데이터를 주성분으로 차원을 축소한 후 시계열로 분포나 추세를 
>   분석하면 고장 징후를 사전에 파악할 수 있다. 
> ```
>
> 3.5.1 <u>데이터마이닝 개요</u> (17회) 
>
> > - 데이터마이닝은 대용량 데이터에서 의미 있는 패턴을 파악하거나 예측하여 의사결정에 활용하는 방법이다. 
> >
> > - 통계분석과 데이터 마이닝의 차이점 
> >
> >   통계분석은 가설이나 가정에 따른 분석이나 검증을 하지만 
> >
> >   데이터 마이닝은 다양한 수리 알고리즘을 이용해 데이터베이스의 데이터로부터 의미있는 정보를 찾아내는 방법을 통칭한다. 
> >
> > - 데이터 마이닝의 종류 
> >
> > ```
> > - 정보를 찾는 방법론에 따른 분류
> > ① 인공지능 (AI)
> > ② 의사결정 나무 (Decision Tree)
> > ③ K-평균군집화 (K-means Clustering)
> > ④ 연관분석(Association Rule)
> > ⑤ 회귀분석(Regression)
> > ⑥ 로짓분석(Logit Analysis)
> > ⑦ 최근접이웃(Nearest Neighborhood)
> > 
> > - 분석대상이나 활용 목적, 표현방법에 따라 분류
> > ① 시각화분석(visualization analysis)
> > ② 분류(classification)
> > ③ 군집화(clustring)
> > ④ 포케스팅(Forecasting)
> > 
> > ```
> >
> > - 데이터 마이닝 사용 분야 
> >
> > ```
> > - 병원에서 환자 데이터를 이용해서 해당 환자에게 발생 가능성이 높은 병을 예측
> > - 기존 환자가 응급실에 왔을 때 어떤 조치를 먼저 해야 하는지를 결정
> > - 고객 데이터를 이용해 해당 고객의 우량/불량을 예측해 대출적격 여부 판단 
> > - 세관 검사에서 입국자의 이력과 데이터를 이용해 관세물품 반입 여부를 예측 
> > ```
> >
> > - 데이터 마이닝 학습방법 
> >
> > ```
> > 1) 지도학습: 목적변수가 존재하는 분석, 의사결정나무, 인공신경망, 판별분석, 
> > 		    로지스틱 회기분석, 사례기반추론 등
> > 2) 비지도 학습: 목적변수가 없이 설명을 위한 분석, 연관성 분석, 연속규칙, 군집분석 등 
> > ```
> >
> > - 데이터마이닝 추진단계
> >
> > ```
> > 1) 목적설정(Business Understanding)
> > 2) 데이터 준비(Data Preparation)
> > 3) 데이터 가공(``)
> > 4) 기법 적용(Modeling)
> > 5) 검증(Evaluation)
> > 
> > *CRISP-DM, SEMMA프로세스(SAS의 데이터마이닝 프로세스) 순서 확인 
> > 
> > ```
> >
> > 
> >
> > - 최근의 데이터 마이닝 환경
> >
> > ```
> > - 데이터 마이닝 도구가 다양하고 체계화되어 환경에 적합한 제품을 선택하여 활용 가능함.
> > - 알고리즘에 대한 깊은 이해가 없이도 분석에 큰 어려움이 없다 
> > - 분석 결과의 품질은 분석가의 경험과 역량에 따라 차이가 나기 때문에 복잡성이나 중요도
> >   가 높으면 풍부한 경험을 가진 전문가에게 의뢰할 필요가 있다. 
> > ```
> >
> > - 대한민국의 데이터 마이닝 환경 
> >
> > ```
> > - 한국에 적용된 시기는 1990년대 중반이다 
> > - 2000년대에 비즈니스 관점에서 데이터마이닝의 CRM의 중요한 요소로 부각되었다. 
> > - 대중화를 위해 많은 시도가 있었으나, 통계학 전문가와 대기업 위주로 진행되었다. 
> > ```
> >
> > - 비즈니스 관점의 데이터마이닝의 가장 큰 어려움 
> >
> > ```
> > - 경영진에 대한 설득 : 
> > 데이터와 분석과 관련된 전문 내용들이다 보니 경영진과 소통하기 쉽지 않다. 
> > - 데이터 준비 : 
> > 데이터 기반으로 데이터 추출과 가공 등의 일이 많은 부담 
> > - 이해 부족으로 비즈니스 관점에서 정의하고 활용 방안을 도출하는데 여러 시행착오가 
> >   발생한다. 
> > ```
>
> 3.5.2 분류 분석 (18회)
>
> >- 
> >
> >
>
> 3.5.3 예측 분석  (12회)
>
> 3.5.4 <u>군집 분석</u>  (29회)
>
> 3.5.5 <u>연관 분석</u>  (20회 )