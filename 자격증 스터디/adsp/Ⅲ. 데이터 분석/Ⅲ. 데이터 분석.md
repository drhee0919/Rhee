### Ⅲ. 데이터 분석  



**chapter1 : 데이터 분석 개요 **

> - 데이터 분석 기법의 이해

**chapter2 : R 프로그래밍 기초**

> - R 소개
> - R 기초
> - 입력과 출력
> - 데이터 구조와 데이터 프레임 
> - 데이터 변형 

**chapter3 : 데이터 마트**

> - 데이터 변경 및 요약 
> - 데이터 가공 
> - 기초 분석 및 데이터 관리

**chapter4 : 통계 분석**

> - 통계 분석의 이해 
> - 기초 통계분석
> - 회귀분석
> - 시계열 분석
> - 다차원척도법
> - 주성분 분석

**chapter5 : 정형 데이터 마이닝**

> - 데이터 마이닝의 개요 
> - 분류분석
> - 군집분석
> - 연관분석

**chapter6 : 비정형 데이터 마이닝**

> - 텍스트 마이닝
> - 사회연결망 분석



#### chapter1 데이터 분석 개요

**제 1절 : 데이터 분석 기법의 이해**

-  데이터 처리 

> ![데이터처리](.\데이터처리.jpg)
>
> - 데이터 분석은 통계에 기반을 두고 있지만, 통계 지식과 복잡한 가정이 상대적으로 적은 실용적 분야. 
> - 대기업은 데이터웨어하우스(DW)와 데이터마트(DM)를 통해 분석 데이터를 가져와서 사용. 
> - 신규 시스템이나 DW에 포함되지 못한 자료의 경우, 기존 운영시스템(Legacy)에서 직접 가져오거나 ODS(Operational Data Store)에서 운영 시스템과 유사한 정제된 데이터를 가져와서 DW에서 가져온 내용과 결합하여 활용한다. 
> - 운영시스템에 직접 접근해 데이터를 활용하는 것은 매우 위험한 일이므로 ODS에서 가져온다. 
> - 자세히 보면 운영시스템에서 가져온 데이터를 임시 저장하는 스테이징 영역(Staging Area)에서 가져올 수도 있다. 
> - 스테이징 영역에서 가져온 데이터는 정제되어 있지 않기 때문에 데이터의 전처리를 해서 DW나 DM과 결합하여 활용한다. 
> - 최종 데이터 구조로 가공한다. 
>
> > * 시뮬레이션 모델링 : 모델링에 적합한 단계별 처리시간에 대한 분포를 파악할 수 있는 내용과 유형, 그에 따른 속성으로 만든다. 
> > * 최적화 : 제약값에 대한 내용과 목적함수와 제약 조건에 들어갈 계수 값을 프로세스별로 산출한다. 
> > * 데이터마이닝 분류 : 분류값과 입력변수들을 연관시켜 인구통계, 요약변수, 파생변수 등을 산출한다. 
>
> - 비정형 데이터나 소셜 데이터는 정형화된 패턴으로 처리해야 한다. 
>
> > * 비정형 데이터 : DBMS에 저장됐다가 텍스트 마이닝을 거쳐 데이터 마트와 통합한다. 
> > * 관계형 데이터 : DBMS에 저장되어 사회 신경망 분석을 거쳐 분석 결과 통계값이 마트와 통합되어 활용된다. 

- 시각화 

> - 시각화는 가장 낮은 수준의 분석이지만 잘 사용하면 복잡한 분석보다도 효율적이다. 
> - 대용량 데이터를 다루는 빅데이터 분야에서 시각화는 필수이다. 
> - '탐색적 분석(EDA)' 를 할 때 시각화는 필수이다. 

- 공간분석

> - 공간분석(Spatial Analysis)는 공간적 차원과 관련된 속성들을 시각화하는 분석이다. 
> - 지도 위에 관련 속성들을 생성하고, 크기, 모양, 선 굵기 등으로 구분하여 인사이트를 얻는다. <br>→ ex/ 대선&총선 지도

- 탐색적 자료분석(EDA)

> - 탐색적 분석이란 다양하나 차원과 값을 조합해가며 특이한 점이나 의미 있는 사실을 도출하고 분석의 최종 목적을 달성해가는 과정이다. 
> - 데이터의 특징과 내재하는 <u>구조적 관계</u>를 알아내기 위한 기법들의 통칭이다. 
> - 프린스톤 대학의 튜키교슈가 1977년 저서를 발표함으로 EDA가 등장했다. 
> - EDA의 4가지 주제 : 저항성의 강조, 잔차 계산, 자료변수의 재표현, 그래프를 통한 현시성. 
> - 탐색적 분석의 효율성 확대 방법
>
> > * 의미가 있을 것으로 판단되는 변수집단과 아닌 집단을 1차 분류한다. 
> > * 전체 변수가 300개일 경우, 의미가 있는 1차 집단 50개, 2차 집단이 100개, 의미가 없는 집단이 150 개 정도로 구분한다. 

- 통계분석

> - 통계란 어떤 현상을 종합적으로 한눈에 알아보기 쉽게 일정한 체계에 따라 숫자와 표, 그림의 형태로 나타내는 것이다. 
> - 기술통계(descriptive statistics)
>
> > 모집단으로부터 표본을 추출하고 표본이 가지고 있는 정보를 쉽게 파악할 수 있도록 데이터를 정리하거나 요약하기 위해 하나의 숫자로 또는 그래프의 형태로 표현하는 절차이다. 
>
> - 추측통계(inferntial statistics)
>
> > 모집단으로부터 추출된 표본의 표본통계량으로 부터 모집단의 특성인 모수에 관해 통계적을 추론하는 절차이다. 

- 데이터마이닝
- 시뮬레이션
- 최적화

 

#### chapter2 : R 프로그래밍 기초

**제 1절 : R소개**

- 데이터 분석 도구의 현황 

  |                           | SAS            | SPSS           | 오픈소스 R             |
  | ------------------------- | -------------- | -------------- | ---------------------- |
  | 프로그램 비용             | 유료, 고가     | 유료, 고가     | 오픈소스               |
  | 설치용량                  | 대용량         | 대용량         | 모듈화로 간단          |
  | 다양한 모듈 지원 및 비용  | 별도구매       | 별도구매       | 오픈소스               |
  | 최근 알고리즘 및 기술반영 | 느림           | 다소느림       | 매우빠름               |
  | 학습자료 입수의 편의성    | 유료 도서 위주 | 유료 도서 위주 | 공개 논문 및 자료 많음 |
  | 질의를 위한 공개 커뮤니티 | NA             | NA             | 매우 활발              |

  > - R의 특징
  >
  > > * 오픈 소스 프로그램 
  > > * 그래픽 및 성능 
  > > * 시스템 데이터 저장 방식 
  > > * 모든 운영체제에서 사용(*윈도우, 맥, 리눅스)
  > > * 표준 플랫폼
  > >
  > > > * S통계 언어를 기반 구현 
  > > > * R/S 플랫폼은 사실상 통계 전문가들의 표준 플랫폼으로 작용 
  > >
  > > - 객체지향언어이며 함수형 언어 
  > >
  > > > * 통계 기능 뿐만 아니라 일반 프로그래밍 언어처럼 자동화거나 새로운 함수를 생성하여 사용 가능하다. 
  > > > * SAS, SPSS에서 회귀분석시 화면에 결과가 산더미로 나오지만, R은 추정계수, 각각의 표준오차, 잔차 등 결과값인 '객체(Object)' 만을 반환함. 필요한 부분은 프로그래밍으로 골라 추출하여 활용 가능한다. 
  > > > * 함수형 언어의 특징 : <br>① 더욱 깔끔하고 단축된 코드<br>② 매우 빠른 코드 수행 속도<br>③ 단순한 코드로 디버깅 노력 감소<br>④ 병렬 프로그래밍으로의 전환이 더욱 용이
  >
  > - R 스튜디오
  >
  > > * 오픈소스이고 다양한 운영체계를 지원한다. 
  > > * R스튜디오는 메모리에 변수가 어떻게 되어 있는지와 타입이 무엇인지를 볼 수 있고, 스크립트 관리와 도큐먼테이션이 편리하다. 
  > > * 코딩을 해야하는 부담이 있으나 스크립트용 프로그래밍으로 어렵지 않고 쉽게 자동화가 가능한다. 
  > > * 래틀(Rattle)
  > >
  > > > * GUI가 패키지와 긴밀하게 결합돼 있어 정해진 기능만 사용 가능 
  > > > * 업그레이드가 제대로 안 되면 통합성에 문제 발생 가능. 



#### chapter3 : 데이터 마트

**제 1절 : R reshape를 이용한 데이터 마트 개발**

- 데이터마트란? 

> * 데이터 웨어하우스(DW)와 사용자 중간층에 위치한 것으로, 하나의 주제 또는 하나의 부서 중심의 데이터 웨어하우스라고 할 수 있다. 
> * 데이터 마트 내 대부분의 데이터는 데이터 웨어하우스로부터 복제되지만, 자체적으로 수집될 수도 있으며, 관계형 데이터 베이스나 다차원 데이터를 이용하여 구축한다. 
> * 데이터마트 구축은 CRM과 같은 관련업무 중에서 핵심과정이다.(고객 데이터 마트 구축)
> * 데이터 마트를 어떻게 구축하느냐에 따라 분석 효과는 크게 차이난다.(분석가의 역량 이상)
> * 데이터 마트를 활용할 수 있는 분석기법은 Classification, Clustering, Association Anaylsis등이 있다. 

- 요약변수

> * 수집된 정보를 분석에 맞게 종합한 변수 
>
> * 데이터 마트에서 가장 기본적인 변수로 데이터 분석을 위해 만들어진 변수. 
>
> * 재활용성이 높다. 
>
> * 간단한 구조로 자동화 프로그램으로 구축 가능한다. 
>
> * 단점 : 얼마 이상이면 기준값의 의미 해석이 애매할 수 있다.<br>→ 연속형 변수를 그룹핑해 사용 
>
> * 요약변수 예시 
>
>   기간별 구매 금액, 구매 횟수, 구매 여부, 구매 회수 여부 
>
>   단어 빈도 : 출현 빈도 데이터 화 
>
>   초기 행동변수 : 가령 고객 가입/ 거래 초기 1개월간 거래 패턴 → 1년 후 행동 평가 지표 활용 
>
>   트랜드 변수 : 추이값을 나타내는 변수
>
>   결측값의 이상값 처리 
>
>   연속형 변수의 구간화 

- 파생변수

> 사용자가 특정 조건을 만족하거나 특정 함수에 의해 값을 만들어 의미를 부여하는 변수 
>
> * 매우 주관적이므로 논리적 타당성을 갖추어 개발필요
>
> * 세분화, 고객행동 예측, 캠페인 반응 예측에 잘 활용 
>
> * 상황에 따라 특정 상황항에만 유의하지 않게 대표성을 나타나게 할 필요가 있다. 
>
> * 파생변수 예시 
>
>   근무시간 구매지수 : 근무시간대에 거래가 발생하는 비율을 산출하여 활용
>
>   주 구매 매장 변수 : 고객의 주거래 매장을 예측하여 적절한 분석에 활용
>
>   주 활동 지역 변수 : 고객의 정보나 거래내용을 통해 주 활동지역을 예측하여 분석에 활용
>
>   주 구매상품 변수 : 상품을 추천하는데 활용
>
>   구매상품 다양성 변수<br>  : 고객이 다양한 상품이나 같은 브랜드 등을 구매하는 성향을 파악하여 분석에 필요한 변수로 변환 
>
>   선호하는 가격대 변수 : 각자의취향, 소득, 서비스에 따른 투자 상품군별 변수 획득 
>
>   시즌 선호고객 변수 : 각자 의미있게 생각하는 날 소비가 많이 이루어지는 패턴을 파악하여 분석 활용
>
>   라이프 스테이지 변수<br> : 고객이 속한 라이프 스테이지를 예측하여 행동을 이해하고 그들의 니즈와 가치를 파악하는데 활용
>
>   라이프 스타일 변수 : 고객의 라이프스타일을 보고 상품구매 유도 등에 활용
>
>   행사민감 변수 : 특정 상품의 행사시 구매로 이어지는 민감도 및 행동패턴 파악에 활용
>
>   휴면가망 변수<br> : 고객이 항상 구매하는 것은 아니므로 고객의 취향, 관심사 변화, 타사 제품 선호 등에 사전 대응하기 위해 활용 
>
>   최대가치 변수 : 고객의 가치를 판단하여 어느 정도를 판매할 수 있는지 예측하는데 활용 
>
>   최적 통화 시간 : 콜센터 등에 걸려온 시간으로 고객의 직업 등을 고려한 통화시간을 예측하여 시도. 
>
>    

- reshape의 활용 

> - R의 reshape 패키지는 melt()와 cast()라는 2개의 핵심 함수가 존재 
> - 변수를 조합해 변수명을 만들고, 변수들을 시간, 상품 등의 차원에 결합해 다양한 요약변수와 파생변수를 쉽게 생성하여 데이터마트를 구성할 수 있게 한다. 
>
> > * melt() : 쉬운 casting을 위한 적당한 형태로 만들어주는 함수<br>**melt(data, id=...)**
> > * cast() : 데이터를 원하는 형태로 계산 또는 변형 시켜주는 함수<br>**cast(data, formula=... ~ variable, fun) **※fun은 함수(function)적용시 사용<br>cast(data=Cars93_sample_melt, Type ~ variable, fun = mean)<br>: type이란 variable을 기준으로 연비(mpg)평균 반환. 

- sqldf를 이용한 데이터 분석 

> sqldf는 R에서 sql의 명령어를 사용 가능하게 해주는 패키지이다. 
>
> - SAS에서의 proc sql 와 같이 R에서 활용 가능하다. 
>
> > * sql : select * from [data frame]
> >
> >   → R : **sqldf("select * from [data frame]")**
> >
> > * sql : select * from [data frame] numrows 10
> >
> >   → R : sqldf("select * from [data frame] limit 10")
> >
> > * sql : select * from [data frame] where [col] = 'char%'
> >
> >   → R : sqldf("select * from [data frame] where [col] like 'char%'")
> >
> >   이 외에도 기존 sql구문 및 R구문들이 sqldf에서 어떻게 환산될 수 있는지 확인 

- plyr을 이용한 데이터 분석 

> plyr은 apply 함수에 기반해 데이터와 출력변수를 동시에 배열로 치환하여 처리하는 패키지
>
> - split-apply-combine : plyr의 핵심기능. 데이터를 분리하고, 처리하고, 다시 결합하는 필수적 데이터 처리기능을 제공한다. 
> - 주로 ddply(dataframe ↔ dataframe), dlply(dataframe ↔ list), ldply(list↔dataframe)

```R
# 임의의 test.data를 가정하여 sd와 mean의 비율인 변동계수 CV(Coefficient of Variation) 산출
# test.data는 year, value로 이루어진 df이다. 
# >test.data
#  year value 
#1 2011    31
#2 2011    84
#      …
install.packages("plyr")
library(plyr)

dd.test <- ddply(test.data, "year, function(x)"){
    m.value <- mean(x$value)
    sd.value <- sd(x$value)
    cv <- round(sd.value/m.value, 4)
    data.frame(cv.value=cv)
})

# 이후 출력시 
# >dd.test
#  year cv.value
#1 2011   0.4396
#2 2012   0.3716
#	    …
```

- 데이터 테이블 

> data.table은 큰 데이터를 탐색, 연산, 병합하는데 유용하게 사용되는 패키지이다. 
>
> - data.table패키지는 R에서 가장 많이 사용하는 데이터 핸들링 패키지 중 하나이다. 
> - 기존 data.frame 방식보다 월등히 빠른 속도이다. 
>   * 특정 column을 key 값으로 색인을 지정한 후 데이터를 처리한다. 
>   * 빠른 그루핑과 ordering, 짧은 문장 지원 측면에서 데이터프레임보다 유용하다. 

```R
install.packages("data.table")
library(data.table)

DF <- data.frame(x = runif(2.6e+07), y = rep(LETTERS, each = 10000))
df <- data.frame(x = runif(2.6e+07), y = rep(letters, each = 10000))
system.time(x <- DF[DF$y=="C",])
# 사용자 시스템 elapsed
#  1.88   0.40   2.30
DT <- as.data.table(DF)
setket(DT,y) #색인 지정하기 
system.time(x<-DT[J("C"),])
# 사용자 시스템 elapsed
#  0.03   0.00   0.03

# 데이터테이블은 경우에 따라 데이터프레임보다 최소 20배 가량 빠르다.
```





**제 2 절 : 데이터 가공 **

- Data Exploration

>  데이터 분석을 위한 구성된 데이터들의 변수들의 상태를 파악한다. 
>
> - R의 head(), summary()등의 함수를 통해서 확인한다. 
> - 수치형 변수 : 최대값, 최소값, 평균, 1사분위수, 2사분위수(중앙값), 3사분위수 
> - 명목형 변수 : 명목 값, 데이터 개수 

- 변수 중요도 

> 변수 선택법과 유사한 개념으로 모형을 생성하여 사용된 변해서 수의 중요도를 살피는 과정 
>
> * R의 klaR 패키지 
>
> > 특정 변수가 주어졌을 때 클래스가 어떻게 분류되는지에 대해 에러율을 돌려주고, 그래픽으로 결과를 보여주는 기능 
> >
> > → naive bayes곡선 플롯을 도시하는데 사용한다. 
> >
> > - greedy.wilks : 세분화를 위한 stepwise foward 변수선택을 위한 패키지  
> > - Wilk's Lambda = 집단 내 분산 / 총 분산 

- 변수의 구간화 

> 신용평가모형, 고객 세분화 등 시스템으로 모형을 적용하기 위해서는 각 변수들을 구간화해서 구간별로 점수를 적용할 수 있어야 한다. 
>
> * binning : 연속형 변수를 범주형 변수로 변형하는 방식. 각각 동일한 개수의 레코드를 50개 이하의 구간에 데이터를 할당해서 구간들을 병합하면서 구간을 줄여나가는 방식의 구간화 방법 
> * 의사결정 나무(Decision Tree)<br>: 세분화 또는 예측에 활용되는 의사결정나무 모형을 활용하여 활용되는 입력변수들을 구간화한다. 



**제 3 절 : 기초 분석 및 데이터 관리 **

- 데이터EDA(탐색적 자료 분석)

> * 데이터 분석에 앞서 전체적으로 데이터의 특징으르 파악하고 데이터를 다양한 각도로 접근한다. 
> * summary()를 이용하여 데이터의 기초통계량부터 확인한다. 

- 결측값 처리 

> - 결측값 처리에 오랜 시간을 활용하는 것은 비효율적이며, 결측값 자체가 의미있는 경우도 있다. 
>
> > ex/ 특정 거래가 자체가 존재하지 않는 경우, 인구통계학적 데이터에서 outlier 식별 
>
> - 결측값 처리가 전체 작업 속도에 많은 영향을 준다. 
>
> > * 이상값 검색
> > * 이상값을 꼭 제거해야 하는 것은 아니기 때문에 분석의 목적이나 종류에 따라 적절한 판단이 필요하다. 
> > * na.rm을 통해 NA를 제거한 연산이 가능하다. 
> >
> > > ex/ 결측값을 제외한 자료의 평균구하기 : mean(x, na,rm=T)
>
> - 결측값 처리 방법(Imputation) 
>
> > * 단순 대치법(Single Imputation)
> >
> > > * completes analysis : 결측값이 존재하는 레코드를 삭제한다. 
> > >
> > > * 평균 대치법(Mean Imputation)
> > >
> > >   > * 비조건부 평균 대치법 : 관측데이터의 평균으로 일괄 대체
> > >   >
> > >   > * 조건부 평균 대치법(regression imputation) : 회귀분석을 활용한 대치법 
> > >
> > > * 단순확률 대치법(Single Stochastic Imputation)
> > >
> > >   > 평균대치법에서 추정량 표준 오차의 과소 추정문제를 보완하고자 고안된 방법 
> > >   >
> > >   > Hot-deck, nearest-neighbor 등 존재. 
> >
> > * 다중 대치법(Multiple Imputation)
> >
> > > * 단순대치법을 한번하지 않고 m번의 대치를 통해 m개의 가상적 완전 자료를 만드는 방법 
> > > * 1 : 대치(imputation step) → 2 : 분석(analysis step) → 3 : 결합(combination step)
> > > * amelia : time series cross sectional data set에서 bootstrapping based algorithm을 활용한 다중 대치법 
>
> - R에서 결측값 처리 
>
> > * 랜덤포레스트 : 패키지 내 **rfImpute()** 함수 활용<br>(랜덤포레스트 모델은 결측값이 존재할 경우, 바로 에러를 발생한다.)
> > * **complet.cases()** : 데이터내 레코드에 결측값이 있으면 FALSE, 없으면 TRUE반환(완벽여부)
> > * **is.na()** : 결측값이 NA인지 여부를 반환
> > * **centralImputation()** : DMwR패키지에 포함, NA값에 가운데 값(central value)으로 대치<br>(숫자 → 중위수, factor(범주) → 최빈값)
> > * **knnImputation()** : DMwR패키지에 포함, NA값을 k최근 이웃 분류 알고리즘을 이용하요 대치, k개 주변 이웃끼리의 거리를 고려하여 가중 평균한 값을 사용 
> > * **amelia()** : Amelia 패키지에 포함, time-series-cross-sectional data set(여러 국가에서 매년 측정된 자료) 에서 확인 



- 이상값(Outlier) 찾기와 처리 

> - 이상값(Outlier)
>
> > * 의도하지 않게 잘못 입력된 경우 
> > * 의도하지 않게 입력되었으나 분석 목적에 부합되지 않아 제거해야 하는 경우 
> > * 의도하지 않은 현상이지만 분석에 포함되어야 하는 경우 
> > * 의도된 이상값(fraud, 불량) 인 경우 
>
> - 이상값의 인식 방법 
>
> > * ESD(Extreme Studentized Deviation) : 평균으로부터 3 표준편차 떨어진 값 
> > * 기하평균보다 2.5표준편차 이상 떨어진 값<br>(기하평균 - 2.5 * 표준편차 < data < 기하평균 + 2.5 * 표준편차)
> > * 사분위수 이용하여 제거하기(이상값 정의)<br>ex/1사분위와 3사분위의 값에서 범위보다 2.5배 이상 떨어진 값<br>( Q1-2.5(Q3-Q1) < data < Q3+2.5(Q3-Q1)을 벗어나는 데이터 )
> > * Trimming(극단값 절단)<br>: 이상값이 포함된 레코드를 삭제
> >
> > > * geo_mean : 기하평균을 이용한 제거 
> > > * 하단, 상단 %를 이용한 제거 
> > > * 10% 절단 : 상 하위 5%에 해당하는 데이터 제거 
> >
> > * Winsorizing(극단값 조정)<br>: 이상값을 상한 또는 하한값으로 조정<br>→ 상한값과 하한값을 벗어나는 값들을 하한 상한 값으로 바꾸어 활용하는 방법<br>(데이터 손실↓, 설명력↑) 

#### chapter4 : 통계분석

#### chapter5 : 정형 데이터 마이닝

#### chapter6 : 비정형 데이터 마이닝 



