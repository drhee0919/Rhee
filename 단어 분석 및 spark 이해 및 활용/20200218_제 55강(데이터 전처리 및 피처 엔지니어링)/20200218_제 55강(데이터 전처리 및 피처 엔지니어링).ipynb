{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder.appName(\"Test\").config(\"spark.executor.instances\", \"1\").config(\"spark.executor.cores\", \"1\").config(\"spark.executor.memory\", \"2g\").config(\"spark.driver.memory\", \"2g\").getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+----+------+------------------+\n",
      "|color| lab|value1|            value2|\n",
      "+-----+----+------+------------------+\n",
      "|green|good|     1|14.386294994851129|\n",
      "|green| bad|    16|14.386294994851129|\n",
      "| blue| bad|     8|14.386294994851129|\n",
      "| blue| bad|     8|14.386294994851129|\n",
      "| blue| bad|    12|14.386294994851129|\n",
      "|green| bad|    16|14.386294994851129|\n",
      "|green|good|    12|14.386294994851129|\n",
      "|  red|good|    35|14.386294994851129|\n",
      "|  red|good|    35|14.386294994851129|\n",
      "|  red| bad|     2|14.386294994851129|\n",
      "|  red| bad|    16|14.386294994851129|\n",
      "|  red| bad|    16|14.386294994851129|\n",
      "| blue| bad|     8|14.386294994851129|\n",
      "|green|good|     1|14.386294994851129|\n",
      "|green|good|    12|14.386294994851129|\n",
      "| blue| bad|     8|14.386294994851129|\n",
      "|  red|good|    35|14.386294994851129|\n",
      "| blue| bad|    12|14.386294994851129|\n",
      "|  red| bad|    16|14.386294994851129|\n",
      "|green|good|    12|14.386294994851129|\n",
      "+-----+----+------+------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = spark.read.json(\"/home/lab13/data/simple-ml\")\n",
    "df.orderBy(\"value2\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+----+------+------------------+--------------------+-----+\n",
      "|color| lab|value1|            value2|            features|label|\n",
      "+-----+----+------+------------------+--------------------+-----+\n",
      "|green|good|     1|14.386294994851129|(10,[1,2,3,5,8],[...|  1.0|\n",
      "| blue| bad|     8|14.386294994851129|(10,[2,3,6,9],[8....|  0.0|\n",
      "| blue| bad|    12|14.386294994851129|(10,[2,3,6,9],[12...|  0.0|\n",
      "|green|good|    15| 38.97187133755819|(10,[1,2,3,5,8],[...|  1.0|\n",
      "|green|good|    12|14.386294994851129|(10,[1,2,3,5,8],[...|  1.0|\n",
      "|green| bad|    16|14.386294994851129|(10,[1,2,3,5,8],[...|  0.0|\n",
      "|  red|good|    35|14.386294994851129|(10,[0,2,3,4,7],[...|  1.0|\n",
      "|  red| bad|     1| 38.97187133755819|(10,[0,2,3,4,7],[...|  0.0|\n",
      "|  red| bad|     2|14.386294994851129|(10,[0,2,3,4,7],[...|  0.0|\n",
      "|  red| bad|    16|14.386294994851129|(10,[0,2,3,4,7],[...|  0.0|\n",
      "|  red|good|    45| 38.97187133755819|(10,[0,2,3,4,7],[...|  1.0|\n",
      "|green|good|     1|14.386294994851129|(10,[1,2,3,5,8],[...|  1.0|\n",
      "| blue| bad|     8|14.386294994851129|(10,[2,3,6,9],[8....|  0.0|\n",
      "| blue| bad|    12|14.386294994851129|(10,[2,3,6,9],[12...|  0.0|\n",
      "|green|good|    15| 38.97187133755819|(10,[1,2,3,5,8],[...|  1.0|\n",
      "|green|good|    12|14.386294994851129|(10,[1,2,3,5,8],[...|  1.0|\n",
      "|green| bad|    16|14.386294994851129|(10,[1,2,3,5,8],[...|  0.0|\n",
      "|  red|good|    35|14.386294994851129|(10,[0,2,3,4,7],[...|  1.0|\n",
      "|  red| bad|     1| 38.97187133755819|(10,[0,2,3,4,7],[...|  0.0|\n",
      "|  red| bad|     2|14.386294994851129|(10,[0,2,3,4,7],[...|  0.0|\n",
      "+-----+----+------+------------------+--------------------+-----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.feature import RFormula\n",
    "\n",
    "supervised = RFormula(formula=\"lab ~ . +color:value1 + color:value2\")\n",
    "#선언된 formaula에 따라 데이터를 변환할 객체(모델) 생성\n",
    "fittedRF = supervised.fit(df)   \n",
    " # 데이터 변환 객체를 이용하여 데이터 변환\n",
    "preparedRF = fittedRF.transform(df) \n",
    "preparedRF.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+----+------+------------------+----------------------------------------------------------------------+-----+\n",
      "|color|lab |value1|value2            |features                                                              |label|\n",
      "+-----+----+------+------------------+----------------------------------------------------------------------+-----+\n",
      "|green|good|1     |14.386294994851129|(10,[1,2,3,5,8],[1.0,1.0,14.386294994851129,1.0,14.386294994851129])  |1.0  |\n",
      "|blue |bad |8     |14.386294994851129|(10,[2,3,6,9],[8.0,14.386294994851129,8.0,14.386294994851129])        |0.0  |\n",
      "|blue |bad |12    |14.386294994851129|(10,[2,3,6,9],[12.0,14.386294994851129,12.0,14.386294994851129])      |0.0  |\n",
      "|green|good|15    |38.97187133755819 |(10,[1,2,3,5,8],[1.0,15.0,38.97187133755819,15.0,38.97187133755819])  |1.0  |\n",
      "|green|good|12    |14.386294994851129|(10,[1,2,3,5,8],[1.0,12.0,14.386294994851129,12.0,14.386294994851129])|1.0  |\n",
      "|green|bad |16    |14.386294994851129|(10,[1,2,3,5,8],[1.0,16.0,14.386294994851129,16.0,14.386294994851129])|0.0  |\n",
      "|red  |good|35    |14.386294994851129|(10,[0,2,3,4,7],[1.0,35.0,14.386294994851129,35.0,14.386294994851129])|1.0  |\n",
      "|red  |bad |1     |38.97187133755819 |(10,[0,2,3,4,7],[1.0,1.0,38.97187133755819,1.0,38.97187133755819])    |0.0  |\n",
      "|red  |bad |2     |14.386294994851129|(10,[0,2,3,4,7],[1.0,2.0,14.386294994851129,2.0,14.386294994851129])  |0.0  |\n",
      "|red  |bad |16    |14.386294994851129|(10,[0,2,3,4,7],[1.0,16.0,14.386294994851129,16.0,14.386294994851129])|0.0  |\n",
      "|red  |good|45    |38.97187133755819 |(10,[0,2,3,4,7],[1.0,45.0,38.97187133755819,45.0,38.97187133755819])  |1.0  |\n",
      "|green|good|1     |14.386294994851129|(10,[1,2,3,5,8],[1.0,1.0,14.386294994851129,1.0,14.386294994851129])  |1.0  |\n",
      "|blue |bad |8     |14.386294994851129|(10,[2,3,6,9],[8.0,14.386294994851129,8.0,14.386294994851129])        |0.0  |\n",
      "|blue |bad |12    |14.386294994851129|(10,[2,3,6,9],[12.0,14.386294994851129,12.0,14.386294994851129])      |0.0  |\n",
      "|green|good|15    |38.97187133755819 |(10,[1,2,3,5,8],[1.0,15.0,38.97187133755819,15.0,38.97187133755819])  |1.0  |\n",
      "|green|good|12    |14.386294994851129|(10,[1,2,3,5,8],[1.0,12.0,14.386294994851129,12.0,14.386294994851129])|1.0  |\n",
      "|green|bad |16    |14.386294994851129|(10,[1,2,3,5,8],[1.0,16.0,14.386294994851129,16.0,14.386294994851129])|0.0  |\n",
      "|red  |good|35    |14.386294994851129|(10,[0,2,3,4,7],[1.0,35.0,14.386294994851129,35.0,14.386294994851129])|1.0  |\n",
      "|red  |bad |1     |38.97187133755819 |(10,[0,2,3,4,7],[1.0,1.0,38.97187133755819,1.0,38.97187133755819])    |0.0  |\n",
      "|red  |bad |2     |14.386294994851129|(10,[0,2,3,4,7],[1.0,2.0,14.386294994851129,2.0,14.386294994851129])  |0.0  |\n",
      "+-----+----+------+------------------+----------------------------------------------------------------------+-----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "preparedRF.show(20, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 피처 엔지니어링이 수행된 데이터를 임의분할\n",
    "# 간단한 테스트셋 생성\n",
    "# 학습, 테스트 데이터 분리 (7:3)\n",
    "train, test = preparedRF.randomSplit([0.7, 0.3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "aggregationDepth: suggested depth for treeAggregate (>= 2). (default: 2)\n",
      "elasticNetParam: the ElasticNet mixing parameter, in range [0, 1]. For alpha = 0, the penalty is an L2 penalty. For alpha = 1, it is an L1 penalty. (default: 0.0)\n",
      "family: The name of family which is a description of the label distribution to be used in the model. Supported options: auto, binomial, multinomial (default: auto)\n",
      "featuresCol: features column name. (default: features, current: features)\n",
      "fitIntercept: whether to fit an intercept term. (default: True)\n",
      "labelCol: label column name. (default: label, current: label)\n",
      "lowerBoundsOnCoefficients: The lower bounds on coefficients if fitting under bound constrained optimization. The bound matrix must be compatible with the shape (1, number of features) for binomial regression, or (number of classes, number of features) for multinomial regression. (undefined)\n",
      "lowerBoundsOnIntercepts: The lower bounds on intercepts if fitting under bound constrained optimization. The bounds vector size must beequal with 1 for binomial regression, or the number oflasses for multinomial regression. (undefined)\n",
      "maxIter: max number of iterations (>= 0). (default: 100)\n",
      "predictionCol: prediction column name. (default: prediction)\n",
      "probabilityCol: Column name for predicted class conditional probabilities. Note: Not all models output well-calibrated probability estimates! These probabilities should be treated as confidences, not precise probabilities. (default: probability)\n",
      "rawPredictionCol: raw prediction (a.k.a. confidence) column name. (default: rawPrediction)\n",
      "regParam: regularization parameter (>= 0). (default: 0.0)\n",
      "standardization: whether to standardize the training features before fitting the model. (default: True)\n",
      "threshold: Threshold in binary classification prediction, in range [0, 1]. If threshold and thresholds are both set, they must match.e.g. if threshold is p, then thresholds must be equal to [1-p, p]. (default: 0.5)\n",
      "thresholds: Thresholds in multi-class classification to adjust the probability of predicting each class. Array must have length equal to the number of classes, with values > 0, excepting that at most one value may be 0. The class with largest value p/t is predicted, where p is the original probability of that class and t is the class's threshold. (undefined)\n",
      "tol: the convergence tolerance for iterative algorithms (>= 0). (default: 1e-06)\n",
      "upperBoundsOnCoefficients: The upper bounds on coefficients if fitting under bound constrained optimization. The bound matrix must be compatible with the shape (1, number of features) for binomial regression, or (number of classes, number of features) for multinomial regression. (undefined)\n",
      "upperBoundsOnIntercepts: The upper bounds on intercepts if fitting under bound constrained optimization. The bound vector size must be equal with 1 for binomial regression, or the number of classes for multinomial regression. (undefined)\n",
      "weightCol: weight column name. If this is not set or empty, we treat all instance weights as 1.0. (undefined)\n",
      "+-----+----------+\n",
      "|label|prediction|\n",
      "+-----+----------+\n",
      "|  0.0|       0.0|\n",
      "|  0.0|       0.0|\n",
      "|  0.0|       0.0|\n",
      "|  0.0|       0.0|\n",
      "|  0.0|       0.0|\n",
      "|  0.0|       0.0|\n",
      "|  0.0|       0.0|\n",
      "|  0.0|       0.0|\n",
      "|  0.0|       0.0|\n",
      "|  0.0|       0.0|\n",
      "|  0.0|       0.0|\n",
      "|  0.0|       0.0|\n",
      "|  0.0|       0.0|\n",
      "|  0.0|       0.0|\n",
      "|  0.0|       0.0|\n",
      "|  0.0|       0.0|\n",
      "|  0.0|       0.0|\n",
      "|  0.0|       0.0|\n",
      "|  0.0|       0.0|\n",
      "|  0.0|       0.0|\n",
      "+-----+----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 추정자 사용해보기 \n",
    "# 분류 알고리즘에 해당하는 로지스틱 회귀 알고리즘 객체 생성 \n",
    "# 레이블 칼럼 = label, 특징 컬럼 = feature\n",
    "from pyspark.ml.classification import LogisticRegression\n",
    "lr = LogisticRegression(labelCol=\"label\",featuresCol=\"features\")\n",
    "\n",
    "#로지스틱 회귀 알고리즘의 파라미터들 설명 확인\n",
    "print(lr.explainParams())\n",
    "\n",
    "#로지스틱 회귀 알고리즘 적용(학습)하여 모델 생성\n",
    "fittedLR = lr.fit(train)\n",
    "fittedLR.transform(train).select(\"label\", \"prediction\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#모델로부터 학습 데이터의 예측값과 실제값 출력 \n",
    "\n",
    "# 정확도 평가 후 하이퍼 파라미터 조정해서 학습 후 평가 -> 하이퍼 파라미터 조정해서 학습 후 평가를 반복 수행해야 하므로 ...\n",
    "#Pipeline으로 캡슐화 (적용할 모델과  모델의 하이퍼파라미터와 평가자를 조합한 ParmMap을 설정) 하여 수행 \n",
    "\n",
    "\n",
    "# 워크플로를 파이프라인 만들기 \n",
    "# 홀드 아웃 테스트셋 생성\n",
    "#pipeline으로 수행하기 위해 데이터를 임의 분할\n",
    "train, test = df.randomSplit([0.7, 0.3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 특성 변환을 위한 RFormaul객체 생성\n",
    "rForm = RFormula()\n",
    "lr = LogisticRegression().setLabelCol(\"label\").setFeaturesCol(\"features\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pipeline객체 생성 (수행 단계 설정 - RFormaula와 LogisticRegression객체)\n",
    "# RFormula 에 대한 다양한 값을 설정 \n",
    "from pyspark.ml import Pipeline\n",
    "stages = [rForm, lr]\n",
    "pipeline = Pipeline().setStages(stages)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 생성된 모델 학습 및 평가 \n",
    "#RFormaula와 하이퍼파라미터를 12가지조합으로 학습시킬 ParamMap객체 생성\n",
    "from pyspark.ml.tuning import ParamGridBuilder\n",
    "params = ParamGridBuilder().addGrid(rForm.formula, [\n",
    "    \"lab ~ . + color:value1\",\n",
    "    \"lab ~ . + color:value1 + color:value2\"]).addGrid(lr.elasticNetParam, [0.0, 0.5, 1.0]).addGrid(lr.regParam, [0.1, 2.0]).build()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#이진분류 평가기 객체 생성\n",
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator\n",
    "evaluator = BinaryClassificationEvaluator().setMetricName(\"areaUnderROC\").setRawPredictionCol(\"prediction\").setLabelCol(\"label\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ParamMap, 추정자Pipeline, 평가기 , 학습 데이터 를 설정\n",
    "from pyspark.ml.tuning import TrainValidationSplit\n",
    "tvs = TrainValidationSplit().setTrainRatio(0.75).setEstimatorParamMaps(params).setEstimator(pipeline).setEvaluator(evaluator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#학습 시킴(->모델 생성)\n",
    "tvsFitted = tvs.fit(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9166666666666667"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#평가하기  (테스트데이터에 모델 적용)\n",
    "evaluator.evaluate(tvsFitted.transform(test)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###################################################################################################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------------+\n",
      "| id|      features|\n",
      "+---+--------------+\n",
      "|  0|[1.0,0.1,-1.0]|\n",
      "|  1| [2.0,1.1,1.0]|\n",
      "|  0|[1.0,0.1,-1.0]|\n",
      "|  1| [2.0,1.1,1.0]|\n",
      "|  1|[3.0,10.1,3.0]|\n",
      "+---+--------------+\n",
      "\n",
      "+---+--------------+-----------------------------------+\n",
      "| id|      features|StandardScaler_55b2fb5026e7__output|\n",
      "+---+--------------+-----------------------------------+\n",
      "|  0|[1.0,0.1,-1.0]|               [1.19522860933439...|\n",
      "|  1| [2.0,1.1,1.0]|               [2.39045721866878...|\n",
      "|  0|[1.0,0.1,-1.0]|               [1.19522860933439...|\n",
      "|  1| [2.0,1.1,1.0]|               [2.39045721866878...|\n",
      "|  1|[3.0,10.1,3.0]|               [3.58568582800318...|\n",
      "+---+--------------+-----------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "####StandardScaler(평균 0, 분산 1이 되도록 데이터 변환) 실습 #####\n",
    "scaleDF = spark.read.parquet(\"/home/lab13/data/simple-ml-scaling/\")\n",
    "scaleDF.show()\n",
    "\n",
    "from pyspark.ml.feature import StandardScaler\n",
    "sScaler = StandardScaler().setInputCol(\"features\")\n",
    "sScaler.fit(scaleDF).transform(scaleDF).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+\n",
      "|  id|\n",
      "+----+\n",
      "| 0.0|\n",
      "| 1.0|\n",
      "| 2.0|\n",
      "| 3.0|\n",
      "| 4.0|\n",
      "| 5.0|\n",
      "| 6.0|\n",
      "| 7.0|\n",
      "| 8.0|\n",
      "| 9.0|\n",
      "|10.0|\n",
      "|11.0|\n",
      "|12.0|\n",
      "|13.0|\n",
      "|14.0|\n",
      "|15.0|\n",
      "|16.0|\n",
      "|17.0|\n",
      "|18.0|\n",
      "|19.0|\n",
      "+----+\n",
      "\n",
      "+----+-------------------------------+\n",
      "|  id|Bucketizer_3b7c8dc9030f__output|\n",
      "+----+-------------------------------+\n",
      "| 0.0|                            0.0|\n",
      "| 1.0|                            1.0|\n",
      "| 2.0|                            1.0|\n",
      "| 3.0|                            1.0|\n",
      "| 4.0|                            1.0|\n",
      "| 5.0|                            2.0|\n",
      "| 6.0|                            2.0|\n",
      "| 7.0|                            2.0|\n",
      "| 8.0|                            2.0|\n",
      "| 9.0|                            2.0|\n",
      "|10.0|                            3.0|\n",
      "|11.0|                            3.0|\n",
      "|12.0|                            3.0|\n",
      "|13.0|                            3.0|\n",
      "|14.0|                            3.0|\n",
      "|15.0|                            4.0|\n",
      "|16.0|                            4.0|\n",
      "|17.0|                            4.0|\n",
      "|18.0|                            4.0|\n",
      "|19.0|                            4.0|\n",
      "+----+-------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "########버켓팅으로 데이터 변환(경계값을 이용하여 버켓 생성) ###\n",
    "contDF = spark.range(20).selectExpr(\"cast(id as double)\")\n",
    "contDF.show()\n",
    "\n",
    "from pyspark.ml.feature import Bucketizer\n",
    "\n",
    "bucketBorders = [-1.0 , 0.5, 5.0, 10.0, 15.0, 20.0 ]\n",
    "bucketer = Bucketizer().setSplits(bucketBorders).setInputCol(\"id\")\n",
    "bucketer.transform(contDF).show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------------+-----------------------------------+\n",
      "| id|      features|StandardScaler_c1185a560ac6__output|\n",
      "+---+--------------+-----------------------------------+\n",
      "|  0|[1.0,0.1,-1.0]|               [1.19522860933439...|\n",
      "|  1| [2.0,1.1,1.0]|               [2.39045721866878...|\n",
      "|  0|[1.0,0.1,-1.0]|               [1.19522860933439...|\n",
      "|  1| [2.0,1.1,1.0]|               [2.39045721866878...|\n",
      "|  1|[3.0,10.1,3.0]|               [3.58568582800318...|\n",
      "+---+--------------+-----------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.feature import StandardScaler\n",
    "'''\n",
    "자료형 double 에서 int로 바꿔주기\n",
    "contDF = spark.range(20)\n",
    "contDF.show()\n",
    "'''\n",
    "sScaler = StandardScaler().setInputCol(\"features\")\n",
    "sScaler.fit(scaleDF).transform(scaleDF).show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------------+---------------------------------+\n",
      "| id|      features|MinMaxScaler_ad94506207aa__output|\n",
      "+---+--------------+---------------------------------+\n",
      "|  0|[1.0,0.1,-1.0]|                    [5.0,5.0,5.0]|\n",
      "|  1| [2.0,1.1,1.0]|                    [7.5,5.5,7.5]|\n",
      "|  0|[1.0,0.1,-1.0]|                    [5.0,5.0,5.0]|\n",
      "|  1| [2.0,1.1,1.0]|                    [7.5,5.5,7.5]|\n",
      "|  1|[3.0,10.1,3.0]|                 [10.0,10.0,10.0]|\n",
      "+---+--------------+---------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.feature import MinMaxScaler\n",
    "minMax = MinMaxScaler().setMin(5).setMax(10).setInputCol(\"features\")\n",
    "minMax.fit(scaleDF).transform(scaleDF).show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------------+---------------------------------+\n",
      "| id|      features|MaxAbsScaler_d6ada2d416e2__output|\n",
      "+---+--------------+---------------------------------+\n",
      "|  0|[1.0,0.1,-1.0]|             [0.33333333333333...|\n",
      "|  1| [2.0,1.1,1.0]|             [0.66666666666666...|\n",
      "|  0|[1.0,0.1,-1.0]|             [0.33333333333333...|\n",
      "|  1| [2.0,1.1,1.0]|             [0.66666666666666...|\n",
      "|  1|[3.0,10.1,3.0]|                    [1.0,1.0,1.0]|\n",
      "+---+--------------+---------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.feature import MaxAbsScaler\n",
    "maScaler = MaxAbsScaler(). setInputCol(\"features\")\n",
    "maScaler.fit(scaleDF).transform(scaleDF).show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------------+---------------------------------------+\n",
      "| id|      features|ElementwiseProduct_b36df450308a__output|\n",
      "+---+--------------+---------------------------------------+\n",
      "|  0|[1.0,0.1,-1.0]|                       [10.0,1.5,-20.0]|\n",
      "|  1| [2.0,1.1,1.0]|                       [20.0,16.5,20.0]|\n",
      "|  0|[1.0,0.1,-1.0]|                       [10.0,1.5,-20.0]|\n",
      "|  1| [2.0,1.1,1.0]|                       [20.0,16.5,20.0]|\n",
      "|  1|[3.0,10.1,3.0]|                      [30.0,151.5,60.0]|\n",
      "+---+--------------+---------------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.feature import ElementwiseProduct\n",
    "from pyspark.ml.linalg import Vectors\n",
    "scaleVec = Vectors.dense(10.0, 15.0, 20.0)\n",
    "scalingUp = ElementwiseProduct(). setScalingVec(scaleVec).setInputCol(\"features\")\n",
    "scalingUp.transform(scaleDF).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------------+-------------------------------+\n",
      "| id|      features|Normalizer_da76019113c1__output|\n",
      "+---+--------------+-------------------------------+\n",
      "|  0|[1.0,0.1,-1.0]|           [0.47619047619047...|\n",
      "|  1| [2.0,1.1,1.0]|           [0.48780487804878...|\n",
      "|  0|[1.0,0.1,-1.0]|           [0.47619047619047...|\n",
      "|  1| [2.0,1.1,1.0]|           [0.48780487804878...|\n",
      "|  1|[3.0,10.1,3.0]|           [0.18633540372670...|\n",
      "+---+--------------+-------------------------------+\n",
      "\n",
      "+---+--------------+-------------------------------+\n",
      "| id|      features|Normalizer_39f1c474d94c__output|\n",
      "+---+--------------+-------------------------------+\n",
      "|  0|[1.0,0.1,-1.0]|           [0.70534561585859...|\n",
      "|  1| [2.0,1.1,1.0]|           [0.80257235390512...|\n",
      "|  0|[1.0,0.1,-1.0]|           [0.70534561585859...|\n",
      "|  1| [2.0,1.1,1.0]|           [0.80257235390512...|\n",
      "|  1|[3.0,10.1,3.0]|           [0.27384986857909...|\n",
      "+---+--------------+-------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.feature import Normalizer\n",
    "manhattenDistance = Normalizer().setP(1).setInputCol(\"features\")\n",
    "manhattenDistance.transform(scaleDF).show()\n",
    "uclideDistance = Normalizer().setP(2).setInputCol(\"features\")\n",
    "uclideDistance.transform(scaleDF).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+----+------+------------------+--------+\n",
      "|color| lab|value1|            value2|valueInd|\n",
      "+-----+----+------+------------------+--------+\n",
      "|green|good|     1|14.386294994851129|     2.0|\n",
      "| blue| bad|     8|14.386294994851129|     4.0|\n",
      "| blue| bad|    12|14.386294994851129|     0.0|\n",
      "|green|good|    15| 38.97187133755819|     5.0|\n",
      "|green|good|    12|14.386294994851129|     0.0|\n",
      "|green| bad|    16|14.386294994851129|     1.0|\n",
      "|  red|good|    35|14.386294994851129|     6.0|\n",
      "|  red| bad|     1| 38.97187133755819|     2.0|\n",
      "|  red| bad|     2|14.386294994851129|     7.0|\n",
      "|  red| bad|    16|14.386294994851129|     1.0|\n",
      "|  red|good|    45| 38.97187133755819|     3.0|\n",
      "|green|good|     1|14.386294994851129|     2.0|\n",
      "| blue| bad|     8|14.386294994851129|     4.0|\n",
      "| blue| bad|    12|14.386294994851129|     0.0|\n",
      "|green|good|    15| 38.97187133755819|     5.0|\n",
      "|green|good|    12|14.386294994851129|     0.0|\n",
      "|green| bad|    16|14.386294994851129|     1.0|\n",
      "|  red|good|    35|14.386294994851129|     6.0|\n",
      "|  red| bad|     1| 38.97187133755819|     2.0|\n",
      "|  red| bad|     2|14.386294994851129|     7.0|\n",
      "+-----+----+------+------------------+--------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "###########범주형 데이터 인덱싱화(변환) ###############\n",
    "from pyspark.ml.feature import StringIndexer\n",
    "\n",
    "simpleDF = spark.read.json(\"/home/lab13/data/simple-ml\")\n",
    "\n",
    "valIndexer = StringIndexer().setInputCol(\"value1\").setOutputCol(\"valueInd\")\n",
    "valIndexer.fit(simpleDF).transform(simpleDF).show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "###########(25.7) 텍스트 데이터 변환자 ###############\n",
    "##텍스트 처리 (불용어 제거, ngram)#####################\n",
    "\n",
    "##데이터셋 부터 불러오기 \n",
    "sales = spark.read.format(\"csv\") .option(\"header\", \"true\")\\\n",
    "  .option(\"inferSchema\", \"true\").load(\"/home/lab13/data/retail_data/all/*.csv\").coalesce(5).where(\"Description IS NOT NULL\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------------------------+------------------------------------------+\n",
      "|Description                        |DescOut                                   |\n",
      "+-----------------------------------+------------------------------------------+\n",
      "|WHITE HANGING HEART T-LIGHT HOLDER |[white, hanging, heart, t-light, holder]  |\n",
      "|WHITE METAL LANTERN                |[white, metal, lantern]                   |\n",
      "|CREAM CUPID HEARTS COAT HANGER     |[cream, cupid, hearts, coat, hanger]      |\n",
      "|KNITTED UNION FLAG HOT WATER BOTTLE|[knitted, union, flag, hot, water, bottle]|\n",
      "|RED WOOLLY HOTTIE WHITE HEART.     |[red, woolly, hottie, white, heart.]      |\n",
      "|SET 7 BABUSHKA NESTING BOXES       |[set, 7, babushka, nesting, boxes]        |\n",
      "|GLASS STAR FROSTED T-LIGHT HOLDER  |[glass, star, frosted, t-light, holder]   |\n",
      "|HAND WARMER UNION JACK             |[hand, warmer, union, jack]               |\n",
      "|HAND WARMER RED POLKA DOT          |[hand, warmer, red, polka, dot]           |\n",
      "|ASSORTED COLOUR BIRD ORNAMENT      |[assorted, colour, bird, ornament]        |\n",
      "|POPPY'S PLAYHOUSE BEDROOM          |[poppy's, playhouse, bedroom]             |\n",
      "|POPPY'S PLAYHOUSE KITCHEN          |[poppy's, playhouse, kitchen]             |\n",
      "|FELTCRAFT PRINCESS CHARLOTTE DOLL  |[feltcraft, princess, charlotte, doll]    |\n",
      "|IVORY KNITTED MUG COSY             |[ivory, knitted, mug, cosy]               |\n",
      "|BOX OF 6 ASSORTED COLOUR TEASPOONS |[box, of, 6, assorted, colour, teaspoons] |\n",
      "|BOX OF VINTAGE JIGSAW BLOCKS       |[box, of, vintage, jigsaw, blocks]        |\n",
      "|BOX OF VINTAGE ALPHABET BLOCKS     |[box, of, vintage, alphabet, blocks]      |\n",
      "|HOME BUILDING BLOCK WORD           |[home, building, block, word]             |\n",
      "|LOVE BUILDING BLOCK WORD           |[love, building, block, word]             |\n",
      "|RECIPE BOX WITH METAL HEART        |[recipe, box, with, metal, heart]         |\n",
      "+-----------------------------------+------------------------------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 25.7.1 텍스트 토큰화하기 \n",
    "#tokenizer 클래스를 사용하는 방법 \n",
    "from pyspark.ml.feature import Tokenizer\n",
    "\n",
    "tkn = Tokenizer().setInputCol(\"Description\").setOutputCol(\"DescOut\")\n",
    "tokenized = tkn.transform(sales.select(\"Description\"))\n",
    "tokenized.show(20,False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------------------------+------------------------------------------+\n",
      "|Description                        |DescOut                                   |\n",
      "+-----------------------------------+------------------------------------------+\n",
      "|WHITE HANGING HEART T-LIGHT HOLDER |[white, hanging, heart, t-light, holder]  |\n",
      "|WHITE METAL LANTERN                |[white, metal, lantern]                   |\n",
      "|CREAM CUPID HEARTS COAT HANGER     |[cream, cupid, hearts, coat, hanger]      |\n",
      "|KNITTED UNION FLAG HOT WATER BOTTLE|[knitted, union, flag, hot, water, bottle]|\n",
      "|RED WOOLLY HOTTIE WHITE HEART.     |[red, woolly, hottie, white, heart.]      |\n",
      "|SET 7 BABUSHKA NESTING BOXES       |[set, 7, babushka, nesting, boxes]        |\n",
      "|GLASS STAR FROSTED T-LIGHT HOLDER  |[glass, star, frosted, t-light, holder]   |\n",
      "|HAND WARMER UNION JACK             |[hand, warmer, union, jack]               |\n",
      "|HAND WARMER RED POLKA DOT          |[hand, warmer, red, polka, dot]           |\n",
      "|ASSORTED COLOUR BIRD ORNAMENT      |[assorted, colour, bird, ornament]        |\n",
      "|POPPY'S PLAYHOUSE BEDROOM          |[poppy's, playhouse, bedroom]             |\n",
      "|POPPY'S PLAYHOUSE KITCHEN          |[poppy's, playhouse, kitchen]             |\n",
      "|FELTCRAFT PRINCESS CHARLOTTE DOLL  |[feltcraft, princess, charlotte, doll]    |\n",
      "|IVORY KNITTED MUG COSY             |[ivory, knitted, mug, cosy]               |\n",
      "|BOX OF 6 ASSORTED COLOUR TEASPOONS |[box, of, 6, assorted, colour, teaspoons] |\n",
      "|BOX OF VINTAGE JIGSAW BLOCKS       |[box, of, vintage, jigsaw, blocks]        |\n",
      "|BOX OF VINTAGE ALPHABET BLOCKS     |[box, of, vintage, alphabet, blocks]      |\n",
      "|HOME BUILDING BLOCK WORD           |[home, building, block, word]             |\n",
      "|LOVE BUILDING BLOCK WORD           |[love, building, block, word]             |\n",
      "|RECIPE BOX WITH METAL HEART        |[recipe, box, with, metal, heart]         |\n",
      "+-----------------------------------+------------------------------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 다른 tokenize 방법\n",
    "# RegexTokenizer 사용 \n",
    "from pyspark.ml.feature import RegexTokenizer \n",
    "\n",
    "rt = RegexTokenizer().setInputCol(\"Description\").setOutputCol(\"DescOut\").setPattern(\" \").setToLowercase(True)\n",
    "rt.transform(sales.select(\"Description\")).show(20, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+-------------------------------------+\n",
      "|         Description|             DescOut|StopWordsRemover_a8a921a2b58d__output|\n",
      "+--------------------+--------------------+-------------------------------------+\n",
      "|WHITE HANGING HEA...|[white, hanging, ...|                 [white, hanging, ...|\n",
      "| WHITE METAL LANTERN|[white, metal, la...|                 [white, metal, la...|\n",
      "|CREAM CUPID HEART...|[cream, cupid, he...|                 [cream, cupid, he...|\n",
      "|KNITTED UNION FLA...|[knitted, union, ...|                 [knitted, union, ...|\n",
      "|RED WOOLLY HOTTIE...|[red, woolly, hot...|                 [red, woolly, hot...|\n",
      "|SET 7 BABUSHKA NE...|[set, 7, babushka...|                 [set, 7, babushka...|\n",
      "|GLASS STAR FROSTE...|[glass, star, fro...|                 [glass, star, fro...|\n",
      "|HAND WARMER UNION...|[hand, warmer, un...|                 [hand, warmer, un...|\n",
      "|HAND WARMER RED P...|[hand, warmer, re...|                 [hand, warmer, re...|\n",
      "|ASSORTED COLOUR B...|[assorted, colour...|                 [assorted, colour...|\n",
      "|POPPY'S PLAYHOUSE...|[poppy's, playhou...|                 [poppy's, playhou...|\n",
      "|POPPY'S PLAYHOUSE...|[poppy's, playhou...|                 [poppy's, playhou...|\n",
      "|FELTCRAFT PRINCES...|[feltcraft, princ...|                 [feltcraft, princ...|\n",
      "|IVORY KNITTED MUG...|[ivory, knitted, ...|                 [ivory, knitted, ...|\n",
      "|BOX OF 6 ASSORTED...|[box, of, 6, asso...|                 [box, 6, assorted...|\n",
      "|BOX OF VINTAGE JI...|[box, of, vintage...|                 [box, vintage, ji...|\n",
      "|BOX OF VINTAGE AL...|[box, of, vintage...|                 [box, vintage, al...|\n",
      "|HOME BUILDING BLO...|[home, building, ...|                 [home, building, ...|\n",
      "|LOVE BUILDING BLO...|[love, building, ...|                 [love, building, ...|\n",
      "|RECIPE BOX WITH M...|[recipe, box, wit...|                 [recipe, box, met...|\n",
      "+--------------------+--------------------+-------------------------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 25.7.2 일반적인 단어 제거하기 \n",
    "from pyspark.ml.feature import StopWordsRemover\n",
    "englishStopWords = StopWordsRemover.loadDefaultStopWords(\"english\")\n",
    "stops = StopWordsRemover().setStopWords(englishStopWords)  .setInputCol(\"DescOut\")\n",
    "stops.transform(tokenized).show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------------------------------+------------------------------------------+\n",
      "|DescOut                                   |NGram_fe8cf7ab2363__output                |\n",
      "+------------------------------------------+------------------------------------------+\n",
      "|[white, hanging, heart, t-light, holder]  |[white, hanging, heart, t-light, holder]  |\n",
      "|[white, metal, lantern]                   |[white, metal, lantern]                   |\n",
      "|[cream, cupid, hearts, coat, hanger]      |[cream, cupid, hearts, coat, hanger]      |\n",
      "|[knitted, union, flag, hot, water, bottle]|[knitted, union, flag, hot, water, bottle]|\n",
      "|[red, woolly, hottie, white, heart.]      |[red, woolly, hottie, white, heart.]      |\n",
      "|[set, 7, babushka, nesting, boxes]        |[set, 7, babushka, nesting, boxes]        |\n",
      "|[glass, star, frosted, t-light, holder]   |[glass, star, frosted, t-light, holder]   |\n",
      "|[hand, warmer, union, jack]               |[hand, warmer, union, jack]               |\n",
      "|[hand, warmer, red, polka, dot]           |[hand, warmer, red, polka, dot]           |\n",
      "|[assorted, colour, bird, ornament]        |[assorted, colour, bird, ornament]        |\n",
      "+------------------------------------------+------------------------------------------+\n",
      "only showing top 10 rows\n",
      "\n",
      "+------------------------------------------+--------------------------------------------------------------+\n",
      "|DescOut                                   |NGram_d1a1b00c31f5__output                                    |\n",
      "+------------------------------------------+--------------------------------------------------------------+\n",
      "|[white, hanging, heart, t-light, holder]  |[white hanging, hanging heart, heart t-light, t-light holder] |\n",
      "|[white, metal, lantern]                   |[white metal, metal lantern]                                  |\n",
      "|[cream, cupid, hearts, coat, hanger]      |[cream cupid, cupid hearts, hearts coat, coat hanger]         |\n",
      "|[knitted, union, flag, hot, water, bottle]|[knitted union, union flag, flag hot, hot water, water bottle]|\n",
      "|[red, woolly, hottie, white, heart.]      |[red woolly, woolly hottie, hottie white, white heart.]       |\n",
      "|[set, 7, babushka, nesting, boxes]        |[set 7, 7 babushka, babushka nesting, nesting boxes]          |\n",
      "|[glass, star, frosted, t-light, holder]   |[glass star, star frosted, frosted t-light, t-light holder]   |\n",
      "|[hand, warmer, union, jack]               |[hand warmer, warmer union, union jack]                       |\n",
      "|[hand, warmer, red, polka, dot]           |[hand warmer, warmer red, red polka, polka dot]               |\n",
      "|[assorted, colour, bird, ornament]        |[assorted colour, colour bird, bird ornament]                 |\n",
      "+------------------------------------------+--------------------------------------------------------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 25.7.3 단어 조합 만들기 \n",
    "from pyspark.ml.feature import NGram\n",
    "unigram = NGram().setInputCol(\"DescOut\").setN(1)\n",
    "bigram = NGram().setInputCol(\"DescOut\").setN(2)\n",
    "unigram.transform(tokenized.select(\"DescOut\")).show(10, False)\n",
    "bigram.transform(tokenized.select(\"DescOut\")).show(10, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+---------+--------------------+--------+--------------+---------+----------+--------------+\n",
      "|InvoiceNo|StockCode|         Description|Quantity|   InvoiceDate|UnitPrice|CustomerID|       Country|\n",
      "+---------+---------+--------------------+--------+--------------+---------+----------+--------------+\n",
      "|   536365|   85123A|WHITE HANGING HEA...|       6|12/1/2010 8:26|     2.55|     17850|United Kingdom|\n",
      "|   536365|    71053| WHITE METAL LANTERN|       6|12/1/2010 8:26|     3.39|     17850|United Kingdom|\n",
      "|   536365|   84406B|CREAM CUPID HEART...|       8|12/1/2010 8:26|     2.75|     17850|United Kingdom|\n",
      "|   536365|   84029G|KNITTED UNION FLA...|       6|12/1/2010 8:26|     3.39|     17850|United Kingdom|\n",
      "|   536365|   84029E|RED WOOLLY HOTTIE...|       6|12/1/2010 8:26|     3.39|     17850|United Kingdom|\n",
      "|   536365|    22752|SET 7 BABUSHKA NE...|       2|12/1/2010 8:26|     7.65|     17850|United Kingdom|\n",
      "|   536365|    21730|GLASS STAR FROSTE...|       6|12/1/2010 8:26|     4.25|     17850|United Kingdom|\n",
      "|   536366|    22633|HAND WARMER UNION...|       6|12/1/2010 8:28|     1.85|     17850|United Kingdom|\n",
      "|   536366|    22632|HAND WARMER RED P...|       6|12/1/2010 8:28|     1.85|     17850|United Kingdom|\n",
      "|   536367|    84879|ASSORTED COLOUR B...|      32|12/1/2010 8:34|     1.69|     13047|United Kingdom|\n",
      "|   536367|    22745|POPPY'S PLAYHOUSE...|       6|12/1/2010 8:34|      2.1|     13047|United Kingdom|\n",
      "|   536367|    22748|POPPY'S PLAYHOUSE...|       6|12/1/2010 8:34|      2.1|     13047|United Kingdom|\n",
      "|   536367|    22749|FELTCRAFT PRINCES...|       8|12/1/2010 8:34|     3.75|     13047|United Kingdom|\n",
      "|   536367|    22310|IVORY KNITTED MUG...|       6|12/1/2010 8:34|     1.65|     13047|United Kingdom|\n",
      "|   536367|    84969|BOX OF 6 ASSORTED...|       6|12/1/2010 8:34|     4.25|     13047|United Kingdom|\n",
      "|   536367|    22623|BOX OF VINTAGE JI...|       3|12/1/2010 8:34|     4.95|     13047|United Kingdom|\n",
      "|   536367|    22622|BOX OF VINTAGE AL...|       2|12/1/2010 8:34|     9.95|     13047|United Kingdom|\n",
      "|   536367|    21754|HOME BUILDING BLO...|       3|12/1/2010 8:34|     5.95|     13047|United Kingdom|\n",
      "|   536367|    21755|LOVE BUILDING BLO...|       3|12/1/2010 8:34|     5.95|     13047|United Kingdom|\n",
      "|   536367|    21777|RECIPE BOX WITH M...|       4|12/1/2010 8:34|     7.95|     13047|United Kingdom|\n",
      "+---------+---------+--------------------+--------+--------------+---------+----------+--------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sales.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------------------------+------------------------------------------+---------------------------------------------+\n",
      "|Description                        |DescOut                                   |countVec                                     |\n",
      "+-----------------------------------+------------------------------------------+---------------------------------------------+\n",
      "|WHITE HANGING HEART T-LIGHT HOLDER |[white, hanging, heart, t-light, holder]  |(500,[4,15,19,21,23],[1.0,1.0,1.0,1.0,1.0])  |\n",
      "|WHITE METAL LANTERN                |[white, metal, lantern]                   |(500,[14,15,208],[1.0,1.0,1.0])              |\n",
      "|CREAM CUPID HEARTS COAT HANGER     |[cream, cupid, hearts, coat, hanger]      |(500,[57,98,222,280],[1.0,1.0,1.0,1.0])      |\n",
      "|KNITTED UNION FLAG HOT WATER BOTTLE|[knitted, union, flag, hot, water, bottle]|(500,[34,39,40,89,256],[1.0,1.0,1.0,1.0,1.0])|\n",
      "|RED WOOLLY HOTTIE WHITE HEART.     |[red, woolly, hottie, white, heart.]      |(500,[3,15],[1.0,1.0])                       |\n",
      "|SET 7 BABUSHKA NESTING BOXES       |[set, 7, babushka, nesting, boxes]        |(500,[0,160,163],[1.0,1.0,1.0])              |\n",
      "|GLASS STAR FROSTED T-LIGHT HOLDER  |[glass, star, frosted, t-light, holder]   |(500,[21,23,31,84],[1.0,1.0,1.0,1.0])        |\n",
      "|HAND WARMER UNION JACK             |[hand, warmer, union, jack]               |(500,[89,101,112,137],[1.0,1.0,1.0,1.0])     |\n",
      "|HAND WARMER RED POLKA DOT          |[hand, warmer, red, polka, dot]           |(500,[3,101,112],[1.0,1.0,1.0])              |\n",
      "|ASSORTED COLOUR BIRD ORNAMENT      |[assorted, colour, bird, ornament]        |(500,[62,79,87,308],[1.0,1.0,1.0,1.0])       |\n",
      "+-----------------------------------+------------------------------------------+---------------------------------------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 25.7.4 단어를 숫자로 변환하기 \n",
    "'''\n",
    "CountVectorizer는 토큰화된 데이터에서 작동하며, 다음 두 가지 작업을 수행합니다. \n",
    "1. 모델을 적합하는 프로세스 동안 모든 문서에서 단어 집합을 찾은 다음 문서별로 해당 단어의 \n",
    "   출현 빈도를 계산합니다. \n",
    "2. 그런 다음 변환 과정에서 DataFrame 컬럼의 각 로우에서 주어진 단어의 발생 빈도를 계산하고 \n",
    "   해당 로우에 포함된 용어를 벡터 형태로 출력합니다. \n",
    "'''\n",
    "from pyspark.ml.feature import CountVectorizer\n",
    "cv = CountVectorizer()\\\n",
    "    .setInputCol(\"DescOut\")\\\n",
    "    .setOutputCol(\"countVec\")\\\n",
    "    .setVocabSize(500)\\\n",
    "    .setMinTF(1)\\\n",
    "    .setMinDF(2)\n",
    "    \n",
    "fittedCV = cv.fit(tokenized)\n",
    "fittedCV.transform(tokenized).show(10, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------------------------+\n",
      "|DescOut                             |\n",
      "+------------------------------------+\n",
      "|[red, woolly, hottie, white, heart.]|\n",
      "|[hand, warmer, red, polka, dot]     |\n",
      "|[red, coat, rack, paris, fashion]   |\n",
      "|[alarm, clock, bakelike, red]       |\n",
      "|[set/2, red, retrospot, tea, towels]|\n",
      "|[red, toadstool, led, night, light] |\n",
      "|[hand, warmer, red, polka, dot]     |\n",
      "|[edwardian, parasol, red]           |\n",
      "|[red, woolly, hottie, white, heart.]|\n",
      "|[edwardian, parasol, red]           |\n",
      "+------------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 다른방법: Tf-IDF 사용하기 \n",
    "# 여러 문서에서 발생하는 용어보다 적은 문서에서 출현하는 용어가 더 많은 가중치가 부여\n",
    "# ex/ the : 가중치가 매우 낮음, streaming : 가중치가 상대적으로 높게 부여 \n",
    "\n",
    "#예시로 'red'라는 단어가 포함된 일부 문서확인 \n",
    "tfIdfIn = tokenized\\\n",
    "    .where(\"array_contains(DescOut, 'red')\")\\\n",
    "    .select(\"DescOut\")\\\n",
    "    .limit(10)\n",
    "tfIdfIn.show(10, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------------------------+--------------------------------------------------------+--------------------------------------------------------------------------------------------------------------------+\n",
      "|DescOut                             |TFOut                                                   |IDFOut                                                                                                              |\n",
      "+------------------------------------+--------------------------------------------------------+--------------------------------------------------------------------------------------------------------------------+\n",
      "|[red, woolly, hottie, white, heart.]|(10000,[4077,4291,6756,6872,7142],[1.0,1.0,1.0,1.0,1.0])|(10000,[4077,4291,6756,6872,7142],[1.2992829841302609,0.0,1.2992829841302609,1.2992829841302609,1.2992829841302609])|\n",
      "|[hand, warmer, red, polka, dot]     |(10000,[3280,3423,4220,4291,8977],[1.0,1.0,1.0,1.0,1.0])|(10000,[3280,3423,4220,4291,8977],[1.2992829841302609,1.2992829841302609,1.2992829841302609,0.0,1.2992829841302609])|\n",
      "|[red, coat, rack, paris, fashion]   |(10000,[477,598,4291,5106,9627],[1.0,1.0,1.0,1.0,1.0])  |(10000,[477,598,4291,5106,9627],[0.0,0.0,0.0,0.0,0.0])                                                              |\n",
      "|[alarm, clock, bakelike, red]       |(10000,[4291,4852,4995,9668],[1.0,1.0,1.0,1.0])         |(10000,[4291,4852,4995,9668],[0.0,0.0,0.0,0.0])                                                                     |\n",
      "|[set/2, red, retrospot, tea, towels]|(10000,[811,2591,4291,5716,8540],[1.0,1.0,1.0,1.0,1.0]) |(10000,[811,2591,4291,5716,8540],[0.0,0.0,0.0,0.0,0.0])                                                             |\n",
      "|[red, toadstool, led, night, light] |(10000,[76,3526,4291,7860,9159],[1.0,1.0,1.0,1.0,1.0])  |(10000,[76,3526,4291,7860,9159],[0.0,0.0,0.0,0.0,0.0])                                                              |\n",
      "|[hand, warmer, red, polka, dot]     |(10000,[3280,3423,4220,4291,8977],[1.0,1.0,1.0,1.0,1.0])|(10000,[3280,3423,4220,4291,8977],[1.2992829841302609,1.2992829841302609,1.2992829841302609,0.0,1.2992829841302609])|\n",
      "|[edwardian, parasol, red]           |(10000,[3266,3944,4291],[1.0,1.0,1.0])                  |(10000,[3266,3944,4291],[1.2992829841302609,1.2992829841302609,0.0])                                                |\n",
      "|[red, woolly, hottie, white, heart.]|(10000,[4077,4291,6756,6872,7142],[1.0,1.0,1.0,1.0,1.0])|(10000,[4077,4291,6756,6872,7142],[1.2992829841302609,0.0,1.2992829841302609,1.2992829841302609,1.2992829841302609])|\n",
      "|[edwardian, parasol, red]           |(10000,[3266,3944,4291],[1.0,1.0,1.0])                  |(10000,[3266,3944,4291],[1.2992829841302609,1.2992829841302609,0.0])                                                |\n",
      "+------------------------------------+--------------------------------------------------------+--------------------------------------------------------------------------------------------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 상기 출력된 단어들을 해싱(hashing) 하여 TF-IDF에 입력 \n",
    "# 해싱 : 임의의 길이를 가진 데이터를 고정된 길이의 데이터로 매핑 (검색 성능 개선)\n",
    "from pyspark.ml.feature import HashingTF, IDF\n",
    "\n",
    "tf = HashingTF()\\\n",
    "    .setInputCol(\"DescOut\")\\\n",
    "    .setOutputCol(\"TFOut\")\\\n",
    "    .setNumFeatures(10000)\n",
    "idf = IDF()\\\n",
    "    .setInputCol(\"TFOut\")\\\n",
    "    .setOutputCol(\"IDFOut\")\\\n",
    "    .setMinDocFreq(2)\n",
    "\n",
    "idf.fit(tf.transform(tfIdfIn)).transform(tf.transform(tfIdfIn)).show(10, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text: [Hi, I, heard, about, Spark] => \n",
      "Vector: [-0.08174108117818833,-0.021760249510407448,-0.020029566623270514]\n",
      "\n",
      "Text: [I, wish, Java, could, use, case, classes] => \n",
      "Vector: [-0.03161891416779586,-0.028586057147809436,-0.04831806330808571]\n",
      "\n",
      "Text: [Logistic, regression, models, are, neat] => \n",
      "Vector: [0.015932495146989824,0.00997152216732502,0.014660616219043732]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "####### 25.7.5 word2Vec #####\n",
    "# https://ratsgo.github.io/from%20frequency%20to%20semantics/2017/03/30/word2vec/\n",
    "from pyspark.ml.feature import Word2Vec\n",
    "\n",
    "#입력데이터: 각 로우는 문장 또는 문서의 단어주머니입니다. \n",
    "documentDF = spark.createDataFrame([\n",
    "    (\"Hi I heard about Spark\" .split(\" \"), ),\n",
    "    (\"I wish Java could use case classes\" .split(\" \"), ),\n",
    "    (\"Logistic regression models are neat\" .split(\" \"),)\n",
    "    \n",
    "], [\"text\"])\n",
    "\n",
    "#단어를 백터에 매핑합니다.\n",
    "word2Vec = Word2Vec(vectorSize=3, minCount=0, inputCol=\"text\", outputCol=\"result\")\n",
    "model = word2Vec.fit(documentDF)\n",
    "result = model.transform(documentDF)\n",
    "for row in result.collect():\n",
    "    text, vector = row \n",
    "    print(\"Text: [%s] => \\nVector: %s\\n\" % (\", \".join(text), str(vector)))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "F"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
