**머신러닝**

기계를 통해서 스스로 학습할 수 있게 만드는 알고리즘을 만드는 것 : 머신러닝

얻고자 하는 인사이트에는 : 예측, 분류

parametic algorithm(특성값에 대한 수식을 가정) → 정답라벨에 대한 기울기 개수, 정답 개수를 조정 

non-parametic algorithm → support vector machine, naive baysian classifier

머신러닝이 활용되는 분야에 대한 설명: 챗봇, 고객 성향 분석, 번역(텍스트 의미 추출)

머신러닝 학습의 갈래 

지도학습 : 입력에 대한 가설을 세우고 정답과의 오차를 계산(입력 출력간의 관계 )

비지도학습:  인과관계를 알 수 없음(가설 지정 x) 입력데이터를 오로지 머신에 맡겨 가설 패턴을 찾아냄 

```
머신러닝 :
기계 스스로 데이터를 이용해 문제를 해결할 수 있는 알고리즘을 만드는 것
입력 데이터로부터 원하는 결과값(출력값)을 얻어내는 것이 목적
예] 회귀 알고리즘 LogisticRegression -> fit() - >LogisticRegressionModel

Parametric 알고리즘 : 
고정된 개수의 파라미터, 계수를 사용하는 것
입력과 출력 사이의 관계를 특성 값에 관한 수학적 함수 또는 수식으로 가정
수식의 결과가 실제 결과값에 가깝도록 계수를 조정하는 방법을 사용
예] 선형회귀(Linear Regression) , 로지스틱(Logistic Regression)

Nonparametric 알고리즘  :
입력과 출력 사이의 가설을 세우지 않고 머신러닝의 수행 결과를 그대로 사용하는 방식
예] SVM(Support Vector Machine), 나이브 베이즈(Nativ Bayes)  

머신러닝 활용 : 
유사한 것들끼리 묶거나 분류하기
인터넷 쇼핑몰 등에서 사용자의 과거 행동 패턴을 바탕으로 상품 추천하기
시스템이나 사용자의 비정상적 상태를 검출하거나 예측하기
이벤트나 상품의 보이지 않는 관계 찾아내기
텍스트 정보로부터 긍정적/부정적 표현 여부를 판정하는 등 텍스트의 의미 정보 추정하기

지도 학습(Supervised Learning) : 
훈련 데이터에 레이블(정답)에 관한 정보가 포함되며 알고리즘은 입력과 출력에 대한 가설과 정답 정보를 이용해 오차를 계산하고 이를 통해 입력과 출력 사이의 관계를 유추하게 됩니다.

비지도 학습 (Unsupervised Learning) :
특성과 레이블 간의 인과 관계를 모르거나 특별히 지정하지 않고 컴퓨터의 처리에 맡기는 것
예] Clustering


Spark MLlib API 주요 기능 : 
- 머신러닝 알고리즘 : classification, Regression, Clustering, collaborative filtering 등
- 특성 추출, 변환, 선택
- 파이프라인(Pipeline) : 여러 종류의 머신러닝 알고리즘을 순차적으로 수행할 수 있도록 제공
- 저장 : 알고리즘, 모델, 파이프라인에 대한 저장 및 불러오기 기능 제공
- 유틸리티 : 선형대수, 통계, 데이터 처리 등의 유용한 함수를 제공


벡터 :  1차원 데이터를 다루는 데이터 타입
LocalVector - 기본적인 벡터 타입으로 밀집 벡터인 DenseVector 클래스와 희소 벡터인 SparseVector 클래스 
LabeledPoint - LocalVector에 레이블을 부여한 데이터 타입. 예) 지도학습 알고리즘을 이용할 때 독립변수와 종속변수를 함께 보존할 때 이용


Transformer - 데이터를 변환처리를 시행하는 컴포넌트
			 모델의 입력변수로 사용할 특징을 개발
			 (특징 수를 줄이거나, 특징을 추가하거나, 데이터 조작 등)
			 모델의 입력변수는 doublwe 타입이나 vector로 구성되어야 한다. 

Estimator - 데이터프레임에 알고리즘을 적용해 새로운 트랜스포머(머신러닝 모델)를 생성하는 역할
모든 Estimator 클래스는 Transformer를 생성하는 fit() 메서드를 가짐

Pipeline -  여러 알고리즘을 순차적으로 실행할 수 있는 워크플로우를 생성하는 평가자
```

----

#### 24.4 MLlib 사용하기

```python
# 시작하기 전 .json 확장자 형태의 샘플 데이터 불러오기
df = spark.read.json("/home/lab13/data/simple-ml")
df.orderBy("value2").show()
```

> **24.4.1 변환자를 사용해서 피처 엔지니어링 수행하기 **
>
> - MLlib을 사용할 때 스파크에서 제공하는 머신러닝 알고리즘의 모든 입력변수는 Double타입(label 용) 이나 Vector[Double] 타입(특징용)으로 구성되어야 한다.(*단, 일부 예외존재)
> - 위와 같이 데이터셋을 조건에 맞게 변환해주기 위해 RFormula 사용 
>
> >R formula 연산자 
> >
> >- ~ 
> >
> >  함수에서 타깃(target)과 항(term)을 분리
> >
> >- +
> >
> >  연결기호, '+0'은 절편 제거를 의미(우리가 맞추고자 하는 선의 y절편이 0이 됨을 의미)
> >
> >- -
> >
> >  삭제기호. '-1'은 절편 제거를의미(우리가 맞추고자 하는 선의 y절편이 0이 됨을 의미함. '+0'과 결과가 같다.)
> >
> >- :
> >
> >  상호작용(수치형 값이나 이진화된 범주 값에 대한 곱셈)
> >
> >- .
> >
> >  타깃/종속변수를 제외한 모든 컬럼 
>
> ``` python
> from pyspark.ml.feature import RFormula
> 
> supervised = RFormula(formula="lab ~ . +color:value1 + color:value2")
> #선언된 formaula에 따라 데이터를 변환할 객체(모델) 생성
> fittedRF = supervised.fit(df)   
>  # 데이터 변환 객체를 이용하여 데이터 변환
> preparedRF = fittedRF.transform(df) 
> preparedRF.show()
> ```
>
> 









#### 25.7 텍스트 데이터 변환자 

> 텍스트는 처리하기가 매우 까다로운 데이터 
>
> → 머신러닝 모델이 효과적으로 작동하기 위해서는 텍스트 데이터를 올바른 형식으로 맵핑 필요.
>
> 일반적으로 접하는 텍스트 데이터는 보통 자유형 텍스트와 문자열로 된 범주형 변수가 존재. 
>
> → 이번에는 자유형 텍스트 데이터를 다루는 방법에 대해서 다뤄볼 것 

**25.7.1 텍스트 토큰화하기**

> 토큰화는 자유형 텍스트를 '토큰' 또는 개별 단어 목록으로 변환하는 프로세스 
>
> → Tokenizer 클래스를 사용, 데이터셋에서 설명 필드를 토큰 리스트로 변환 

``` python
from pyspark.ml.feature import Tokenizer

tkn = Tokenizer().setInputCol("Description").setOutputCol("DescOut")
tokenized = tkn.transform(sales.select("Description"))
tokenized.show(20,False)
```

``` python
# 다른 tokenize 방법
# RegexTokenizer 사용 
from pyspark.ml.feature import RegexTokenizer 

rt = RegexTokenizer().setInputCol("Description").setOutputCol("DescOut").setPattern(" ").setToLowercase(True)
rt.transform(sales.select("Description")).show(20, False)

#또는 
#공백사용이 아닌 사전에 제시된 패턴에 매칭되는 값을 출력하게 끔 하는 방법 
rt = RegexTokenizer().setInputCol("Description").setOutputCol("DescOut").setPattern(" ").setGaps(False).setToLowercase(True)
rt.transform(sales.select("Description")).show(20, False)	
```



**25.7.2 일반적인 단어 제거하기 **

> 텍스트를 토큰화 한 다음에는 일반적으로 불용어나 분석과 관련 없는 무의미한 용어를 필터링
>
> 필요하다면 대소문자도 구분 가능(영어)

``` python
# 25.7.2 일반적인 단어 제거하기 
from pyspark.ml.feature import StopWordsRemover
englishStopWords = StopWordsRemover.loadDefaultStopWords("english")
stops = StopWordsRemover().setStopWords(englishStopWords)  .setInputCol("DescOut")
stops.transform(tokenized).show()
```



**25.7.3 단어 조합 만들기**

```python
# 25.7.3 단어 조합 만들기 
from pyspark.ml.feature import NGram
unigram = NGram().setInputCol("DescOut").setN(1)
bigram = NGram().setInputCol("DescOut").setN(2)
unigram.transform(tokenized.select("DescOut")).show(10, False)
bigram.transform(tokenized.select("DescOut")).show(10, False)
```

**25.7.4 단어를 숫자로 변환하기**

> 일단 단어 특징을 생성하면 모델에서 사용하기 위해 단어와 단어 조합 수를 세어야 한다. 
>
> 문서의 크기와 발생 횟수 정보를 정규화하고 콘텐츠 기반으로 문서를 분류할 수 있는 수치형 특징을 확보할 수 있다. 
>
> - 주어진 문서에서 단어 포함 여부를 바이너리 형식으로 세어서 포함하는 방법 
> - CountVectorizer 를 사용해서 단어를 실셈 
> - TF-IDF변환을 사용하여 모든 문서에서 주어진 단어의 발생 빈도에 따라 단어 가중치 재측정 

```python
'''
CountVectorizer는 토큰화된 데이터에서 작동하며, 다음 두 가지 작업을 수행합니다. 
1. 모델을 적합하는 프로세스 동안 모든 문서에서 단어 집합을 찾은 다음 문서별로 해당 단어의 
   출현 빈도를 계산합니다. 
2. 그런 다음 변환 과정에서 DataFrame 컬럼의 각 로우에서 주어진 단어의 발생 빈도를 계산하고 
   해당 로우에 포함된 용어를 벡터 형태로 출력합니다. 
'''
from pyspark.ml.feature import CountVectorizer
cv = CountVectorizer()\
    .setInputCol("DescOut")\
    .setOutputCol("countVec")\
    .setVocabSize(500)\
    .setMinTF(1)\
    .setMinDF(2)
    
fittedCV = cv.fit(tokenized)
fittedCV.transform(tokenized).show(10, False)
```

```python
# 다른방법
# Tf-IDF 사용하기 
# 여러 문서에서 발생하는 용어보다 적은 문서에서 출현하는 용어가 더 많은 가중치가 부여
# ex/ the : 가중치가 매우 낮음, streaming : 가중치가 상대적으로 높게 부여 
tfIdfln = tokenized\
    .where("array_contains(DescOut, 'red')")\
    .select("DescOut")
    .limit(10)
tfIdfIn.show(10, False)
```

```python
# 상기 출력된 단어들을 해싱(hashing) 하여 TF-IDF에 입력 
# 해싱 : 임의의 길이를 가진 데이터를 고정된 길이의 데이터로 매핑 (검색 성능 개선)
from pyspark.ml.feature import HashingTF, IDF

tf = HashingTF()\
    .setInputCol("DescOut")\
    .setOutputCol("TFOut")\
    .setNumFeatures(10000)
idf = IDF()\
    .setInputCol("TFOut")\
    .setOutputCol("IDFOut")\
    .setMinDocFreq(2)

idf.fit(tf.transform(tfIdfIn)).transform(tf.transform(tfIdfIn)).show(10, False)
```

**27.7.5 Word2Vec**

> word2vec은 단어 집합의 벡터 표현을 계산하기 위한 딥러닝 기반 도구
>
> 목표 : 비슷한 단어를 백터 공간에서 서로 가깝게 배치하여 단어를 일반화할 수 있도록 하는 것 
>
> (https://ratsgo.github.io/from%20frequency%20to%20semantics/2017/03/30/word2vec/)

```python
from pyspark.ml.feature import Word2Vec

#입력데이터: 각 로우는 문장 또는 문서의 단어주머니입니다. 
documentDF = spark.createDataFrame([
    ("Hi I heard about Spark" .split(" "), ),
    ("I wish Java could use case classes" .split(" "), ),
    ("Logistic regression models are neat" .split(" "),)
    
], ["text"])

#단어를 백터에 매핑합니다.
word2Vec = Word2Vec(vectorSize=3, minCount=0, inputCol="text", outputCol="result")
model = word2Vec.fit(documentDF)
result = model.transform(documentDF)
for row in result.collect():
    text, vector = row 
    print("Text: [%s] => \nVector: %s\n" % (", ".join(text), str(vector)))
    
```

``` python
#결과(값은 다르게 나올 수 있다.)
'''
Text: [Hi, I, heard, about, Spark] => 
Vector: [-0.08174108117818833,-0.021760249510407448,-0.020029566623270514]

Text: [I, wish, Java, could, use, case, classes] => 
Vector: [-0.03161891416779586,-0.028586057147809436,-0.04831806330808571]

Text: [Logistic, regression, models, are, neat] => 
Vector: [0.015932495146989824,0.00997152216732502,0.014660616219043732]
'''
```

