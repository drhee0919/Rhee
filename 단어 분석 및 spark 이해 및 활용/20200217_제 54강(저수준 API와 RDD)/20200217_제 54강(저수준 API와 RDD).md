시작하기 전에 

> - 빅데이터 에코시스템
>
> https://wikidocs.net/22651
>
> 복습하면서 주요 어휘 숙지할 것
>
> ( hdfs 클러스터 host node mapreduce framework hive library java streaming sql ...)



#### 10.5 카탈로그 

> - DB에 저장되는 정보들은 2가지가 있다.
>
>   1. User Data, Business Data, Application Data
>
>   2. User명/ Password, Table구조(이름, 타입, 컬럼명, 컬럼타입, 컬럼크기), 제약조건(PK,UK,FK, ... )
>
>      → 운용 필요 정보\(Meta 정보) : System Catalog, Data Dictionary
>
> - 카탈로그Catalog 란 스파트 SQL에서 가장 높은 추상화 단계로서 테이블에 저장된 데이터에 대한 메타데이터 뿐만 아니라 데이터 베이스, 테이블, 함수, 그리고 뷰에 대한 정보를 추상화합니다. 
>
>   → DDL 구문을 통해서 카탈로그를 관리하게 된다. 

#### 10.6 테이블 

> - 스파트 SQL에서 테이블은 명령을 실행할 때마다 데이터의 구조라는 점에서 DataFrame과 논리적으로 동일한 개념을 가진다. → 조인, 필터링, 집계 등 여러 데이터 변환 작업을 수행 가능하다. 
>
> **10.6.1 스파크 관리형 테이블**
>
> > * saveAsTable 메서드 
> >
> > > * 스파크는 데이터 뿐만 아니라 파일에 대한 메타데이터를 관리할 수 있다.
> > > * DataFrame의 saveAsTable메서드를 통해 스파크가 관련된 모든 정보를 추적할 수 있는 관리형 테이블 생성이 가능하다.
> > > * saveAsTable 메서드는 테이블을 읽고 데이터를 스파크 포맷으로 변환한 후 새로운 경로에 저장 
>
> **10.6.2 테이블 생성하기**
>
> >  spark-sql을 키고 다음 구문을 입력해보자 
> >
> > (https://docs.databricks.com/spark/latest/spark-sql/language-manual/show-tables.html)
>
> ``` sql
> -- json이나 csv파일로부터 DataFrame구조로 로드해서 데이터를 처리하는 방법
> 
> -- json이나 csv파일로부터 스키마 구조를 정의(create table구문~)를 메타스토어 
> -- hive.warehouse를 기본 저장소에 등록후 sql을 사용해서 데이터 처리 가능 
> 
> spark-sql> create table flights 
>              dest_country_name string,
>              origin_country_name string,
>              count long)
>              using json options (path '/home/lab13/data/2015-summary.json');
> spark-sql> describe table  flights;
> ```
>
> ``` sql
> spark-sql> cache table flights;
> spark-sql> show databases;  --데이터베이스 목록 확인(default)
> 						  -- default
> spark-sql> create database some_db;  --새로운 데이터베이스 생성
> spark-sql> show databases;   --데이터베이스 목록 확인
> 						   -- default
> 						   -- some_db
> spark-sql> select current_database(); --현재 사용중인 데이터베이스 이름 확인
> spark-sql> use some_db;  --다른 데이터베이스를 사용하기 위해 전환
> spark-sql> select current_database(); -- some_db
> 
> spark-sql> show tables;   --테이블 목록 확인
> spark-sql> select * from default.flights; --default 데이터베이스에 등록된 테이블 데이터 조회
> spark-sql> select current_database()
> spark-sql> use default; --default db로 사용전환 
> spark-sql> drop database if exists some_db;  --데이터베이스 삭제
> spark-sql> show databases; --default
> ```



#### part 3. 

#### chapter 12. RDD

> - 이전 2부에서는 스파크의 구조적 API를 알아보았다. 대부분의 상황에서는 구조적 API를 사용하는 것이 좋다. (spark 2.0 부터 DataFrame 사용 권장)
>
> - RDD는 spark 1.X의 핵심 API
>
>   (spark 2.X의 사용 권장 API는 구조적 API(DataFrame, SQL, Dataset) 이지만, 연산이 수행될 때 RDD로 변환(최적화된 물리적 실행 계획을 만드는데 사용됨)되어 실행됩니다.)
>
> > * RDD : 불변성, 클러스터(분산환경)에서 파티셔닝된 레코드들의 병렬 처리 지원
> >
> >   ​			RDD 레코드는 (수동으로 정의, 사용자 정의 포맷 형식) 객체 
> >
> >   ​			내부 구조를 스파크에서 파악할 수 없기 때문에 최적화를 수동으로 선언해 수행해야 하므			로 최적화에 시간이 많이 걸리며 모든 값을 다루거나 값 사이의 상호작용도 모두 수동으로 			정의해야 한다. 	
> >
> > * DataFrame : 스키마(Field로 구성된 ROW객체)
> >
> >   ​					 내부 구조를 스파크에서 파악할 수 있기 때문에 최적화를 빠르게 자동으로 수행
> >
> >   ​					 (수작업이 줄어듬)
>
> - 비즈니스등에서 기술적 문제를 고수준(high-level) API를 사용해 모두 처리할 수 있는 것은 아니다. 
>
>   → 이런 경우 스파크의 저수준 API를 사용해야 할 수도 있다. 
>
> - 스파크의 저수준 API는 RDD, SparkContext 그리고 어큐뮬레이터accumulator 와 브로드캐스트 변수 broadcast variable 같은 분산형 공유 변수distributed shared varaibles 등을 의미한다.



#### 12.1 저수준 API란

> - 스파크에는 두 종류의 저수준 API가 있다. 
>
> > 1. 분산 데이터 처리를 위한 RDD
> > 2. 브로드캐스트 변수와 어큐뮬레이터처럼 분산형 공유 변수를 배포하고 다루기 위한 API 

**12.1.1 저수준 API는 언제 사용할까**

> - 다음과 같은 상황에서 저수준 API를 사용한다. 
>
> > * 고수준 API에서 제공하지 않는 기능이 필요한 경우. 예를 들어 클러스터의 물리적 데이터의 배치를 아주 세밀하게 제어해야 하는 상황에서 저수준 API가 필요하다. 
> > * RDD를 사용해 개발된 기존 코드를 유지해야 하는 경우
> > * 사용자가 정의한 공유 변수를 다뤄야 하는 경우(공유변수 관련 설명은 14장에서)



#### 12.2 RDD 개요 

> - RDD는 불변성을 가지며 병렬로 처리할 수 있는 파티셔닝된 레코드의 모음 
>
>   (Collection : 객체를 저장하는 집합)
>
> * DataFrame의 각 레코드는 스키마를 알고 있는 필드로 구성된 구조화된 row인 반면, RDD의 레코드는 그저 프로그래머가 선택하는 자바, 스칼라, 파이썬의 객체 
>
> * DataFrame이나 Dataset코드는 RDD로 컴파일 됨. 
>
> * 스파크 UI에서 RDD단위로 job이 수행됨(18장)

**12.2.1 RDD의 유형**

> 사용자는 두가지 타입의 RDD를 만들 수 있다. 
>
> > 1. Generic RDD
> >
> > > ※**제네릭(generic)**이란? 
> > >
> > > 자바에서 제네릭(generic)이란 데이터의 타입(data type)을 일반화한다(generalize)는 것을 의미합니다. 제네릭은 클래스나 메소드에서 사용할 내부 데이터 타입을 컴파일 시에 미리 지정하는 방법입니다.
> >
> > 2. 키-값 RDD
> >
> > > (둘다 객체의 컬렉션을 포현하지만) 키 -값 RDD는 특수 연산뿐만 아니라 키를 이요한 사용자 지정 파티셔닝 개념을 가지고 있다. 



(이하 12장 13장 코드 실습)



#### chapter 14 분산형 공유 변수 

> 스파크의 저수준 API에는 RDD 인터페이스 외에도 두 번째 유형인 '분산형 공유 변수' 가 존재한다. 
>
> 분산형 공유 변수에는 **브로드캐스트** 변수와 **어큐물레이터**라는 두개의 타입이 존재한다. 
>
> - 어큐뮬레이터를 사용하면 모든 태스트의 데이터를 공유 결과에 추가할 수 있다. 
> - 브로드캐스트 변수를 사용하면 모든 워커 노드에 큰 값을 저장하므로 재전송 없이 많은 스파크 액션에서 재사용이 가능하다. 



(0, 2, 2

0, 1, 2)

(0, 0, 1

2, 2, 2)

(1, 2, 3

4, 5, 6)