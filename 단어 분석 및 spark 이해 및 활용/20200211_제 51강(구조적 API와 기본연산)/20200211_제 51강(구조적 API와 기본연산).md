#### 스파크의 구조적 API 

> 스파크의 구조적 api는 비정형 로그 파일부터 반정형 CSV파일, 매우 정형화된 Parquet(파케이) 파일까지 다양한 데이터를 처리할 수 있다.
>
> 구조적 API는 데이터의 흐름을 정의하는 기본 추상화 개념 
>
> > * 타입형(typed) / 비타입형(untyped) API의 개념과 차이점 
> > * 핵심 용어 
> > * 스파크가 구조적 API의 데이터 흐름을 해석하고 클러스터에서 실행하는 방식 
>
> - 구조적 API에는 다음과 같은 분산 컬렉션 API가 있다. (3가지)
>
> > * Dataset
> > * DataFrame
> >
> > > [Row(...), Row(...),..]
> > >
> > > DataFrame은 Row 타입의 레코드와 각 레코드에 수행할 연산 표현식을 나타내는 여러 컬럼으로 구성됩니다.
> >
> > * SQL 테이블 &뷰
>
> * 배치(batch) 와 스트리밍(streaming) 처리에서 구조적 API를 사용할 수 있다. 
>
>   → 구조적 API를 활용 하면 배치 작업을 스트리밍 작업으로 손쉽게 변환할 수 있다.(vice versa)



**스키마**

> 스키마는 DataFrame의 컬럼명과 데이터 타입을 정의한다. 
>
> 스키마는 데이터 소스에서 얻거나 직접 정의할 수 있다. 
>
> 스키마는 여러 데이터 타입으로 구성되므로 어떤 데이터 타입이 어느 위치에 있는지 정의하는 방법이 필요하다. → Dataframe과 Dataset을 정의하는 바탕이 된다. 
>
> 스키만를 직접 정의할 경우 StructType과 StructField객체로 정의한다. 

``` python
spark.read.format("json").load("/data/flight-data/json/2015-summary.json").schema


#p126실습
#스키마 정보 확인
from pyspark.sql.types import StructField, StructType, StringType, LongType

myManualSchema = StructType([
  StructField("DEST_COUNTRY_NAME", StringType(), True),
  StructField("ORIGIN_COUNTRY_NAME", StringType(), True),
  StructField("count", LongType(), False, metadata={"hello":"world"})
])
df = spark.read.format("json").schema(myManualSchema).load("/home/lab13/data/2015-summary.json")
df.printSchema()
df.schema

컬럼  접근 방법 :
1. df.col("count")  --- select()함수내에서 사용할 예정
2. df.column("count")  ----- select()함수내에서 사용할 예정
3. df.columns

from pyspark.sql.functions import col, column
df.columns 


df.first()  ---첫번째 row객체 반환

# Row객체 직접 생성
from pyspark.sql import Row
myRow = Row("Hello", None, 1, False)    
myRow[0]
myRow[2]

#ROW객체와 Schema를 직접 생성해서 DataFrame생성
from pyspark.sql import Row
from pyspark.sql.types import StructField, StructType, StringType, LongType
myManualSchema = StructType([
  StructField("some", StringType(), True),
  StructField("col", StringType(), True),
  StructField("names", LongType(), False)
])
myRow = Row("Hello", None, 1)
myDf = spark.createDataFrame([myRow], myManualSchema)
myDf.show()

'''출력결과
+-----+----+-----+
| some| col|names|
+-----+----+-----+
|Hello|null|    1|
+-----+----+-----+

'''
```

**select와 selectExpr**

> select와 selectExpr 메서드를 사용하면 데이터 테이블에  SQL을 실행하는 것처럼 DataFrame에서도 SQL을 사용할 수 있다. 

```SQL
SELECT * FROM dataFrameTable
SELECT * columnName FROM dataFrameTable
SELECT columnName * 10, otherColumn, someOtherCol as c FROM dataFrameTable
```

``` python
#DataFrame으로부터 selection 수행
# 파이썬
df.select("DEST_COUNTRY_NAME").show(2)
#-- SQL
#SELECT DEST_COUNTRY_NAME FROM dfTable LIMIT 2
'''
+-----------------+
|DEST_COUNTRY_NAME|
+-----------------+
|    United States|
|    United States|
+-----------------+

'''




# 파이썬
df.select("DEST_COUNTRY_NAME", "ORIGIN_COUNTRY_NAME").show(2)
#-- SQL
#SELECT DEST_COUNTRY_NAME, ORIGIN_CONTRY_NAME FROM dfTable LIMIT 2
'''
+-----------------+-------------------+
|DEST_COUNTRY_NAME|ORIGIN_COUNTRY_NAME|
+-----------------+-------------------+
|    United States|            Romania|
|    United States|            Croatia|
+-----------------+-------------------+

'''




from pyspark.sql.functions import expr, col, column
df.select(
    expr("DEST_COUNTRY_NAME"),
    col("DEST_COUNTRY_NAME"),
    column("DEST_COUNTRY_NAME")).show(2)
'''
+-----------------+-----------------+-----------------+
|DEST_COUNTRY_NAME|DEST_COUNTRY_NAME|DEST_COUNTRY_NAME|
+-----------------+-----------------+-----------------+
|    United States|    United States|    United States|
|    United States|    United States|    United States|
+-----------------+-----------------+-----------------+
'''

```

```python
#표현식으로 DataFrame에 컬럼추가 트랜스포메이션 연산후 액션
df.selectExpr(
  "*", # all original columns
  "(DEST_COUNTRY_NAME = ORIGIN_COUNTRY_NAME) as withinCountry").show(10)
'''
+-----------------+-------------------+-----+-------------+
|DEST_COUNTRY_NAME|ORIGIN_COUNTRY_NAME|count|withinCountry|
+-----------------+-------------------+-----+-------------+
|    United States|            Romania|   15|        false|
|    United States|            Croatia|    1|        false|
|    United States|            Ireland|  344|        false|
|            Egypt|      United States|   15|        false|
|    United States|              India|   62|        false|
|    United States|          Singapore|    1|        false|
|    United States|            Grenada|   62|        false|
|       Costa Rica|      United States|  588|        false|
|          Senegal|      United States|   40|        false|
|          Moldova|      United States|    1|        false|
+-----------------+-------------------+-----+-------------+

'''





#표현식으로 DataFrame의 데이터로부터 집계
df.selectExpr("avg(count)", "count(distinct(DEST_COUNTRY_NAME))").show(2)
'''
+-----------+---------------------------------+
| avg(count)|count(DISTINCT DEST_COUNTRY_NAME)|
+-----------+---------------------------------+
|1770.765625|                              132|
+-----------+---------------------------------+

'''



#상수 리터럴을 컬럼값으로 추가, 별칭 선언
from pyspark.sql.functions import lit
df.select(expr("*"), lit(1).alias("One")).show(2)
'''
+-----------------+-------------------+-----+---+
|DEST_COUNTRY_NAME|ORIGIN_COUNTRY_NAME|count|One|
+-----------------+-------------------+-----+---+
|    United States|            Romania|   15|  1|
|    United States|            Croatia|    1|  1|
+-----------------+-------------------+-----+---+

'''



#DataFrame에 컬럼추가
df.withColumn("numberOne", lit(1)).show(2)
'''
+-----------------+-------------------+-----+---------+
|DEST_COUNTRY_NAME|ORIGIN_COUNTRY_NAME|count|numberOne|
+-----------------+-------------------+-----+---------+
|    United States|            Romania|   15|        1|
|    United States|            Croatia|    1|        1|
+-----------------+-------------------+-----+---------+

'''
```

