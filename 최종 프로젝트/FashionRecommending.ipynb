{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "FashionRecommending.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "sJHfMs-x37Wh",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 127
        },
        "outputId": "9e31ce52-4609-4649-e7df-5ca55c2b2eeb"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EibViatNpSQB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "## ResNet Implementation\n",
        "\n",
        "'''\n",
        "The create_variables function takes in name (the name of the variable), \n",
        "shape (a list of dimensions), initializer (Xavier is the default option), \n",
        "and is_fc_layer (set to be False). It returns the created variable new_variables.\n",
        "'''\n",
        "\n",
        "def create_variables(name,\n",
        "                     shape,\n",
        "                     initializer=tf.contrib.layers.xavier_initializer(),     \n",
        "                     is_fc_layer=False):\n",
        "    if is_fc_layer is True:\n",
        "        regularizer =  tf.contrib.layers.l2_regularizer(scale=FLAGS.fc_weight_decay)\n",
        "    else:\n",
        "        regularizer = tf.contrib.layers.l2_regularizer(scale=FLAGS.weight_decay)\n",
        "    \n",
        "\n",
        "\tnew_variables = tf.get_variable(name,\n",
        "\t\t\t\t\tshape=shape,\n",
        "\t\t\t\t\tinitializer=initializer,\n",
        "\t\t\t\t\tregularizer=regularizer)\n",
        "\treturn new_variablesdef create_variables(name,\n",
        "                     shape,\n",
        "                     initializer=tf.contrib.layers.xavier_initializer(),     \n",
        "                     is_fc_layer=False):\n",
        "    if is_fc_layer is True:\n",
        "        regularizer =  tf.contrib.layers.l2_regularizer(scale=FLAGS.fc_weight_decay)\n",
        "    else:\n",
        "        regularizer = tf.contrib.layers.l2_regularizer(scale=FLAGS.weight_decay)\n",
        "    \n",
        "\n",
        "\tnew_variables = tf.get_variable(name,\n",
        "\t\t\t\t\tshape=shape,\n",
        "\t\t\t\t\tinitializer=initializer,\n",
        "\t\t\t\t\tregularizer=regularizer)\n",
        "\treturn new_variables"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qdcBVKvuprPN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "'''\n",
        "The output_layer function takes in input_layer (a 2D tensor) and num_labels (the number of output labels). \n",
        "It returns the output layers fc_h and fc_h2.\n",
        "'''\n",
        "def output_layer(input_layer, num_labels):\n",
        "    input_dim = input_layer.get_shape().as_list()[-1]\n",
        "    \n",
        "    fc_w = create_variables(name='fc_weights', shape=[input_dim, num_labels], \n",
        "    is_fc_layer=True, initializer=tf.uniform_unit_scaling_initializer(factor=1.0))\n",
        "    \n",
        "    fc_b = create_variables(name='fc_bias', shape=[num_labels], \n",
        "    initializer=tf.zeros_initializer)\n",
        "    \n",
        "    fc_w2 = create_variables(name='fc_weights2', shape=[input_dim, 4], \n",
        "    is_fc_layer=True, initializer=tf.uniform_unit_scaling_initializer(factor=1.0))\n",
        "    \n",
        "    fc_b2 = create_variables(name='fc_bias2', shape=[4], \n",
        "    initializer=tf.zeros_initializer)\n",
        "    \n",
        "    fc_h = tf.matmul(input_layer, fc_w) + fc_b\n",
        "    fc_h2 = tf.matmul(input_layer, fc_w2) + fc_b2\n",
        "    return fc_h, fc_h2"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NzE5-ARYp5ZL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "'''\n",
        "The conv_bn_relu_layer function applies convolution, batch normalization and ReLU \n",
        "to the input tensor sequentially. It takes in input_layer (a 4D tensor), \n",
        "filter_shape (a list that contains [filter_height, filter_width, filter_depth, filter_number]), \n",
        "and stride (the stride size for our convolution). It returns a 4D tensor output.\n",
        "'''\n",
        "\n",
        "def conv_bn_relu_layer(input_layer, filter_shape, stride, \n",
        "                        second_conv_residual=False, relu=True):\n",
        "    out_channel = filter_shape[-1]\n",
        "    if second_conv_residual is False:\n",
        "        filter = create_variables(name='conv', shape=filter_shape)\n",
        "    else: filter = create_variables(name='conv2', shape=filter_shape)\n",
        "\n",
        "    conv_layer = tf.nn.conv2d(input_layer, filter, \n",
        "                              strides=[1, stride, stride, 1], padding='SAME')\n",
        "    mean, variance = tf.nn.moments(conv_layer, axes=[0, 1, 2])\n",
        "\n",
        "    if second_conv_residual is False:\n",
        "        beta = tf.get_variable('beta', out_channel, tf.float32,\n",
        "                               initializer=tf.constant_initializer(0.0, tf.float32))\n",
        "        gamma = tf.get_variable('gamma', out_channel, tf.float32,\n",
        "                                initializer=tf.constant_initializer(1.0, tf.float32))\n",
        "    else:\n",
        "        beta = tf.get_variable('beta_second_conv', out_channel, tf.float32,\n",
        "                               initializer=tf.constant_initializer(0.0, tf.float32))\n",
        "        gamma = tf.get_variable('gamma_second_conv', out_channel, tf.float32,\n",
        "                                initializer=tf.constant_initializer(1.0, tf.float32))\n",
        "\n",
        "    bn_layer = tf.nn.batch_normalization(conv_layer, mean, variance, \n",
        "                                          beta, gamma, BN_EPSILON)\n",
        "    if relu:\n",
        "        output = tf.nn.relu(bn_layer)\n",
        "    else:\n",
        "        output = bn_layer\n",
        "    return output"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zQaGFMtGp79r",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "'''\n",
        "The bn_relu_conv_layer function applies batch normalization, \n",
        "ReLU and convolution to the input layer sequentially. \n",
        "The inputs and output are similar to that of conv_bn_relu_layer.\n",
        "'''\n",
        "\n",
        "def bn_relu_conv_layer(input_layer, filter_shape, stride, \n",
        "                        second_conv_residual=False):\n",
        "    in_channel = input_layer.get_shape().as_list()[-1]\n",
        "    mean, variance = tf.nn.moments(input_layer, axes=[0, 1, 2])\n",
        "\n",
        "    if second_conv_residual is False:\n",
        "        beta = tf.get_variable('beta', in_channel, tf.float32,\n",
        "                               initializer=tf.constant_initializer(0.0, tf.float32))\n",
        "        gamma = tf.get_variable('gamma', in_channel, tf.float32,\n",
        "                                initializer=tf.constant_initializer(1.0, tf.float32))\n",
        "    else:\n",
        "        beta = tf.get_variable('beta_second_conv', in_channel, tf.float32,\n",
        "                               initializer=tf.constant_initializer(0.0, tf.float32))\n",
        "        gamma = tf.get_variable('gamma_second_conv', in_channel, tf.float32,\n",
        "                                initializer=tf.constant_initializer(1.0, tf.float32))\n",
        "\n",
        "    bn_layer = tf.nn.batch_normalization(input_layer, mean, variance, \n",
        "                                          beta, gamma, BN_EPSILON)\n",
        "    relu_layer = tf.nn.relu(bn_layer)\n",
        "\n",
        "    if second_conv_residual is False:\n",
        "        filter = create_variables(name='conv', shape=filter_shape)\n",
        "    else: \n",
        "        filter = create_variables(name='conv2', shape=filter_shape)\n",
        "    \n",
        "    conv_layer = tf.nn.conv2d(relu_layer, filter, \n",
        "                              strides=[1, stride, stride, 1], padding='SAME')\n",
        "    return conv_layer"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zlFPKY_Mp-6z",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "'''\n",
        "The residual_block_new function defines a residual block in ResNet. \n",
        "It takes in input_layer (a 4D tensor), output_channel (the shape of our output tensor),\n",
        "and first_block (whether or not this is the first residual block of our network).\n",
        "It returns a 4D tensor output.\n",
        "'''\n",
        "\n",
        "def residual_block_new(input_layer, output_channel, first_block=False):\n",
        "    input_channel = input_layer.get_shape().as_list()[-1]\n",
        "\n",
        "    if input_channel * 2 == output_channel:\n",
        "        increase_dim = True\n",
        "        stride = 2\n",
        "    elif input_channel == output_channel:\n",
        "        increase_dim = False\n",
        "        stride = 1\n",
        "    else:\n",
        "        raise ValueError('Output and input channel does not match in residual block')\n",
        "\n",
        "    if first_block:\n",
        "        filter = create_variables(name='conv', \n",
        "                                  shape=[3, 3, input_channel, output_channel])\n",
        "        conv1 = tf.nn.conv2d(input_layer, filter=filter, \n",
        "                              strides=[1, 1, 1, 1], padding='SAME')\n",
        "    else:\n",
        "        conv1 = bn_relu_conv_layer(input_layer, \n",
        "                                    [3, 3, input_channel, output_channel], stride)\n",
        "    \n",
        "    conv2 = bn_relu_conv_layer(conv1, [3, 3, output_channel, output_channel], 1,\n",
        "                               second_conv_residual=True)\n",
        "    \n",
        "    if increase_dim is True:\n",
        "        pooled_input = tf.nn.avg_pool(input_layer, ksize=[1, 2, 2, 1],\n",
        "                                      strides=[1, 2, 2, 1], padding='SAME')\n",
        "        padded_input = tf.pad(pooled_input, \n",
        "        [[0, 0], [0, 0], [0, 0], [input_channel // 2, input_channel // 2]])\n",
        "    else:\n",
        "        padded_input = input_layer \n",
        "           \n",
        "    output = conv2 + padded_input\n",
        "    return output"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d9VId65WqBpT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "'''\n",
        "The main inference function defines ResNet. \n",
        "It takes in \n",
        "input_tensor_batch (a 4D tensor), \n",
        "n (the number of residual blocks), \n",
        "reuse (setting it to be True if we want to build a train graph, \n",
        "       False if we want to build a validation graph \n",
        "       and share weights with a train graph). \n",
        "It returns the last layer in the network.\n",
        "'''\n",
        "\n",
        "def inference(input_tensor_batch, n, reuse, keep_prob_placeholder):\n",
        "    layers = []\n",
        "    with tf.variable_scope('conv0', reuse=reuse):\n",
        "        conv0 = conv_bn_relu_layer(input_tensor_batch, [3, 3, 3, 16], 1)\n",
        "        layers.append(conv0)\n",
        "\n",
        "    for i in range(n):\n",
        "        with tf.variable_scope('conv1_%d' %i, reuse=reuse):\n",
        "            if i == 0:\n",
        "                conv1 = residual_block_new(layers[-1], 16, first_block=True)\n",
        "            else:\n",
        "                conv1 = residual_block_new(layers[-1], 16)\n",
        "            layers.append(conv1)\n",
        "\n",
        "    for i in range(n):\n",
        "        with tf.variable_scope('conv2_%d' %i, reuse=reuse):\n",
        "            conv2 = residual_block_new(layers[-1], 32)\n",
        "            layers.append(conv2)\n",
        "\n",
        "    for i in range(n):\n",
        "        with tf.variable_scope('conv3_%d' %i, reuse=reuse):\n",
        "            conv3 = residual_block_new(layers[-1], 64)\n",
        "            layers.append(conv3)\n",
        "        assert conv3.get_shape().as_list()[1:] == [16, 16, 64]\n",
        "\n",
        "    with tf.variable_scope('fc', reuse=reuse):\n",
        "        in_channel = layers[-1].get_shape().as_list()[-1]\n",
        "        mean, variance = tf.nn.moments(layers[-1], axes=[0, 1, 2])\n",
        "        beta = tf.get_variable('beta', in_channel, tf.float32,\n",
        "                               initializer=tf.constant_initializer(0.0, tf.float32))\n",
        "        gamma = tf.get_variable('gamma', in_channel, tf.float32,\n",
        "                                initializer=tf.constant_initializer(1.0, tf.float32))\n",
        "        bn_layer = tf.nn.batch_normalization(layers[-1], mean, variance, \n",
        "                                              beta, gamma, BN_EPSILON)\n",
        "        relu_layer = tf.nn.relu(bn_layer)\n",
        "        global_pool = tf.reduce_mean(relu_layer, [1, 2])\n",
        "        assert global_pool.get_shape().as_list()[-1:] == [64]\n",
        "        cls_output, bbx_output = output_layer(global_pool, NUM_LABELS)\n",
        "        layers.append(cls_output)\n",
        "        layers.append(bbx_output)\n",
        "        \n",
        "    return cls_output, bbx_output, global_pool"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x8QSR-TGzkSd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "## Image Pre-Processing\n",
        "# hyper-parameters that are arbitrarily set.\n",
        "shuffle = True\n",
        "localization = FLAGS.is_localization\n",
        "imageNet_mean_pixel = [103.939, 116.799, 123.68]\n",
        "global_std = 68.76\n",
        "IMG_ROWS = 64\n",
        "IMG_COLS = 64"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FBB9fTVkz3jV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# reads in the path to the images and resizes them to 64 x 64 dimension.\n",
        "def get_image(path, x1, y1, x2, y2):\n",
        "    img = cv2.imread(path)\n",
        "    if localization is True:\n",
        "        if img is None or img.shape[0] == 0 or img.shape[1] == 0:\n",
        "            img = np.zeros((1, IMG_ROWS, IMG_COLS, 0))\n",
        "        img = cv2.resize(img, (IMG_ROWS, IMG_COLS))\n",
        "        assert img.shape == (IMG_ROWS, IMG_COLS, 3)\n",
        "    else:\n",
        "        img = cv2.resize(img, (IMG_ROWS, IMG_COLS))\n",
        "    img = img.reshape(1, IMG_ROWS, IMG_COLS, 3)\n",
        "    return img"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ffpj1oRK0Ypo",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# loads the data NumPy arrays \n",
        "# and whitens them with global average pixel ([103.939, 116.799, 123.68]) \n",
        "# and the global standard deviation (68.76).\n",
        "# augment the data by randomly flipping the images horizontally.\n",
        "def load_data_numpy(df):\n",
        "    num_images = len(df)\n",
        "    image_path_array = df['image_path'].as_matrix()\n",
        "    label_array = df['category'].as_matrix()\n",
        "    x1 = df['x1_modified'].as_matrix().reshape(-1, 1)\n",
        "    y1 = df['y1_modified'].as_matrix().reshape(-1, 1)\n",
        "    x2 = df['x2_modified'].as_matrix().reshape(-1, 1)\n",
        "    y2 = df['y2_modified'].as_matrix().reshape(-1, 1)\n",
        "    bbox_array = np.concatenate((x1, y1, x2, y2), axis=1)\n",
        "\n",
        "    image_array = np.array([]).reshape(-1, IMG_ROWS, IMG_COLS, 3)\n",
        "    adjusted_std = 1.0/np.sqrt(IMG_COLS * IMG_ROWS * 3)\n",
        "\n",
        "    for i in range(num_images):\n",
        "        img = get_image(image_path_array[i], \n",
        "        x1=x1[i, 0], y1=y1[i, 0], x2=x2[i, 0], y2=y2[i, 0])\n",
        "        flip_indicator = np.random.randint(low=0, high=2)\n",
        "        if flip_indicator == 0:\n",
        "            img[0, ...] = cv2.flip(img[0, ...], 1)\n",
        "        image_array = np.concatenate((image_array, img))\n",
        "    image_array = (image_array - imageNet_mean_pixel) / global_std\n",
        "    # Convert to BGR image for ResNet\n",
        "    assert image_array.shape[1:] == (IMG_ROWS, IMG_COLS, 3)\n",
        "    return image_array, label_array, bbox_array"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UEj0EnC-1Iqc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "## Training- Helper Functions For Training\n",
        "\n",
        "#takes the path of a csv file and its column as inputs, and then returns a Pandas dataframe as output.\n",
        "def prepare_df(path, usecols, shuffle=shuffle):\n",
        "    df = pd.read_csv(path, usecols=usecols)\n",
        "    if shuffle is True:\n",
        "        order = np.random.permutation(len(df))\n",
        "        df = df.iloc[order, :]\n",
        "    return df\n",
        "\n",
        "    "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PXGI15sS1Wln",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# loss function, which takes in labels, logits, bounding boxes and their labels as inputs,\n",
        "# and returns a sum loss of cross entropy loss and mean squared error loss.\n",
        "def loss(self, logits, bbox, labels, bbox_labels):\n",
        "        labels = tf.cast(labels, tf.int64)\n",
        "        cross_entropy = tf.nn.sparse_softmax_cross_entropy_with_logits(logits=logits, labels=labels, name='cross_entropy_per_example')\n",
        "        mse_loss = tf.reduce_mean(tf.losses.mean_squared_error(bbox_labels, bbox), name='mean_square_loss')\n",
        "        cross_entropy_mean = tf.reduce_mean(cross_entropy, name='cross_entropy')\n",
        "        return cross_entropy_mean + mse_loss"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MH8kPg671dlH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# top_k_error function, which takes the predictions, \n",
        "# labels, and arbitrary k value as inputs, and returns the top k error value.\n",
        "def top_k_error(self, predictions, labels, k):\n",
        "        batch_size = predictions.get_shape().as_list()[0]\n",
        "        in_top1 = tf.to_float(tf.nn.in_top_k(predictions, labels, k=1))\n",
        "        num_correct = tf.reduce_sum(in_top1)\n",
        "        return (batch_size - num_correct) / float(batch_size)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rgi4TLJM1m5H",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# operations function to be applied to the training and validation dataset.\n",
        "def train_operation(self, global_step, total_loss, top1_error):\n",
        "        tf.summary.scalar('learning_rate', self.lr_placeholder)\n",
        "        tf.summary.scalar('train_loss', total_loss)\n",
        "        tf.summary.scalar('train_top1_error', top1_error)\n",
        "\n",
        "        ema = tf.train.ExponentialMovingAverage(0.95, global_step)\n",
        "        train_ema_op = ema.apply([total_loss, top1_error])\n",
        "        tf.summary.scalar('train_top1_error_avg', ema.average(top1_error))\n",
        "        tf.summary.scalar('train_loss_avg', ema.average(total_loss))\n",
        "\n",
        "        opt = tf.train.MomentumOptimizer(learning_rate=self.lr_placeholder, momentum=0.9)\n",
        "        train_op = opt.minimize(total_loss, global_step=global_step)\n",
        "        \n",
        "        return train_op, train_ema_op\n",
        "        \n",
        "def validation_op(self, validation_step, top1_error, loss):\n",
        "        ema = tf.train.ExponentialMovingAverage(0.0, validation_step)\n",
        "        ema2 = tf.train.ExponentialMovingAverage(0.95, validation_step)\n",
        "        val_op = tf.group(validation_step.assign_add(1), ema.apply([top1_error, loss]), ema2.apply([top1_error, loss]))\n",
        "        \n",
        "        top1_error_val = ema.average(top1_error)\n",
        "        top1_error_avg = ema2.average(top1_error)\n",
        "        loss_val = ema.average(loss)\n",
        "        loss_val_avg = ema2.average(loss)\n",
        "        \n",
        "        tf.summary.scalar('val_top1_error', top1_error_val)\n",
        "        tf.summary.scalar('val_top1_error_avg', top1_error_avg)\n",
        "        tf.summary.scalar('val_loss', loss_val)\n",
        "        tf.summary.scalar('val_loss_avg', loss_val_avg)\n",
        "        \n",
        "        return val_op"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NaRR329c1vFp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "## Training Main Function - actual training step \n",
        "train_df = prepare_df(FLAGS.train_path, \n",
        "                      usecols=['image_path', 'category', 'x1_modified', 'y1_modified', 'x2_modified', 'y2_modified'])\n",
        "\n",
        "vali_df = prepare_df(FLAGS.vali_path, \n",
        "                     usecols=['image_path', 'category', 'x1_modified', 'y1_modified', 'x2_modified', 'y2_modified'])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ELt2We-S2BFI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# define a couple of important hyper-parameters to be used during training with TensorFlow\n",
        "# Below are the number of training samples, \n",
        "# global step and validation step (which refer to the number of batches used during training).\n",
        "num_train = len(train_df)\n",
        "global_step = tf.Variable(0, trainable=False)\n",
        "validation_step = tf.Variable(0, trainable=False)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9Xojf-ib2JPo",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "'''\n",
        "logits and vali_logits are the output of ResNet before going through the softmax function. \n",
        "bbox and vali_bbox are the bounding boxes of the images. \n",
        "These variables to perform inference on the test data.\n",
        "'''\n",
        "logits, bbox, _ = inference(self.image_placeholder, \n",
        "                            n=FLAGS.num_residual_blocks, \n",
        "                            reuse=False,\n",
        "                            keep_prob_placeholder=self.dropout_prob_placeholder)\n",
        "\n",
        "vali_logits, vali_bbox, _ = inference(self.vali_image_placeholder, \n",
        "                                      n=FLAGS.num_residual_blocks, \n",
        "                                      reuse=True, \n",
        "                                      keep_prob_placeholder=self.dropout_prob_placeholder)\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UFvvhcBE2a8g",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# loss function combines the regularization loss and the multi-label classification loss.\n",
        "reg_losses = tf.get_collection(tf.GraphKeys.REGULARIZATION_LOSSES)\n",
        "loss = self.loss(logits, bbox, self.label_placeholder, self.bbox_placeholder)\n",
        "full_loss = tf.add_n([loss] + reg_losses)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jWfihlvF2h9y",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# variables for output predictions and top-1 error results.\n",
        "predictions = tf.nn.softmax(logits)\n",
        "top1_error = self.top_k_error(predictions, self.label_placeholder, 1)\n",
        "vali_loss = self.loss(vali_logits, vali_bbox, self.vali_label_placeholder, self.vali_bbox_placeholder)\n",
        "vali_predictions = tf.nn.softmax(vali_logits)\n",
        "vali_top1_error = self.top_k_error(vali_predictions, self.vali_label_placeholder, 1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Lfvk35TO2oaK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# variables for training and validation operations.\n",
        "train_op, train_ema_op = self.train_operation(global_step, full_loss, top1_error)\n",
        "val_op = self.validation_op(validation_step, vali_top1_error, vali_loss)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mWfjEpr82rwS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "'''\n",
        "The empty lists below are initialized to keep track of the training steps, \n",
        "training errors, and validation errors. min_error is an arbitrary variable \n",
        "to maintain the current minimum error value.\n",
        "'''\n",
        "step_list = []\n",
        "train_error_list = []\n",
        "vali_error_list = []\n",
        "min_error = 0.5"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Fk-nbdC82zNl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "'''\n",
        "All the code below lies inside a for loop that iterates through all the steps: \n",
        "for step in range(STEP_TO_TRAIN).  Let’s define a couple of variables:\n",
        "\n",
        "ºoffset is used to limit the batch size.\n",
        "ºtrain_batch_df is the NumPy data array that contains training data batch.\n",
        "ºThe function load_data_numpy is called on train_batch_df  to return the 3 NumPy arrays of training batch,\n",
        "  training batch labels, and training batch bounding boxes.\n",
        "ºThe function generate_validation_batch is called on validation data to return the 3 NumPy arrays of \n",
        "  validation batch, validation batch labels, and validation batch bounding boxes.\n",
        "'''\n",
        "\n",
        "offset = np.random.choice(num_train - TRAIN_BATCH_SIZE, 1)[0]\n",
        "train_batch_df = train_df.iloc[offset:offset+TRAIN_BATCH_SIZE, :]\n",
        "batch_data, batch_label, batch_bbox = load_data_numpy(train_batch_df)\n",
        "vali_image_batch, vali_labels_batch, vali_bbox_batch = generate_validation_batch(vali_df)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UtQF-K8n3Pbo",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# The code below calculates the top 1 error value and loss value for validation data.\n",
        "if step == 0:\n",
        "  if FULL_VALIDATION is True:\n",
        "    top1_error_value, vali_loss_value = self.full_validation(vali_df, sess=sess, \n",
        "    vali_loss=vali_loss, vali_top1_error=vali_top1_error, batch_data=batch_data, \n",
        "    batch_label=batch_label, batch_bbox=batch_bbox)\n",
        "    \n",
        "    vali_summ = tf.Summary()\n",
        "    vali_summ.value.add(tag='full_validation_error',\n",
        "                        simple_value=top1_error_value.astype(np.float))\n",
        "    vali_summ.value.add(tag='full_validation_loss',\n",
        "                        simple_value=vali_loss_value.astype(np.float))\n",
        "    summary_writer.add_summary(vali_summ, step)\n",
        "    summary_writer.flush()\n",
        "\n",
        "  else:\n",
        "    _, top1_error_value, vali_loss_value = sess.run(\n",
        "    [val_op, vali_top1_error, vali_loss], \n",
        "    {self.image_placeholder: batch_data, \n",
        "    self.label_placeholder: batch_label,\n",
        "    self.vali_image_placeholder: vali_image_batch, \n",
        "    self.vali_label_placeholder: vali_labels_batch, \n",
        "    self.lr_placeholder: FLAGS.learning_rate,\n",
        "    self.bbox_placeholder: batch_bbox,\n",
        "    self.vali_bbox_placeholder: vali_bbox_batch,\n",
        "    self.dropout_prob_placeholder: 1.0})\n",
        "    \n",
        "  print('Validation top1 error = %.4f' % top1_error_value)\n",
        "  print('Validation loss = ', vali_loss_value)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OcLMmBF33Ymd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# The code below calculates the top 1 error value for training data. \n",
        "# Strings of every iteration and corresponding loss values are also returned.\n",
        "if step % REPORT_FREQ == 0:\n",
        "  summary_str = sess.run(summary_op, \n",
        "                        {self.image_placeholder: batch_data,\n",
        "                         self.label_placeholder: batch_label,\n",
        "                         self.bbox_placeholder: batch_bbox,\n",
        "                         self.vali_image_placeholder: vali_image_batch,\n",
        "                         self.vali_label_placeholder: vali_labels_batch,\n",
        "                         self.vali_bbox_placeholder: vali_bbox_batch,\n",
        "                         self.lr_placeholder: FLAGS.learning_rate,\n",
        "                         self.dropout_prob_placeholder: 0.5})\n",
        "  summary_writer.add_summary(summary_str, step)\n",
        "\n",
        "  num_examples_per_step = TRAIN_BATCH_SIZE\n",
        "  examples_per_sec = num_examples_per_step / duration\n",
        "  sec_per_batch = float(duration)\n",
        "\n",
        "  format_str = ('%s: step %d, loss = %.4f (%.1f examples/sec; %.3f ' 'sec/batch)')\n",
        "  print (format_str % (datetime.now(), step, loss_value,\n",
        "          examples_per_sec, sec_per_batch))\n",
        "  print('Train top1 error = ', train_top1_error)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y1c0sBpX3gm-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# The code below updates the current lowest error via the min_error variable. \n",
        "# It also updates the step_list, train_error_list, and vali_error_list.\n",
        "\n",
        "if top1_error_value < min_error:\n",
        "    min_error = top1_error_value\n",
        "    checkpoint_path = os.path.join(TRAIN_DIR, 'min_model.ckpt')\n",
        "    saver.save(sess, checkpoint_path, global_step=step)\n",
        "    print('Current lowest error = ', min_error)\n",
        "\n",
        "step_list.append(step)\n",
        "train_error_list.append(train_top1_error)\n",
        "vali_error_list.append(top1_error_value)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lu77VgL93n5u",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "'''\n",
        "The learning rate was 0.1 at the beginning and decayed to 0.01 at 25000 steps. \n",
        "The model was trained for 30000 steps in total. \n",
        "When training is finished, the model is saved into model.ckpt \n",
        "and the error_df data frame is saved into a separate csv file.\n",
        "'''\n",
        "if step == DECAY_STEP0 or step == DECAY_STEP1:\n",
        "  FLAGS.learning_rate = FLAGS.learning_rate * 0.1\n",
        "\n",
        "if step % 10000 == 0 or (step + 1) == STEP_TO_TRAIN:\n",
        "  checkpoint_path = os.path.join(TRAIN_DIR, 'model.ckpt')\n",
        "  saver.save(sess, checkpoint_path, global_step=step)\n",
        "\n",
        "  error_df = pd.DataFrame(data={'step':step_list, 'train_error':\n",
        "      train_error_list, 'validation_error': vali_error_list})\n",
        "  error_df.to_csv(TRAIN_DIR + TRAIN_LOG_PATH, index=False)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zbYRkuJd31lI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "## Test Main Function\n",
        "'''All the images are then evaluated/tested using the well-trained model. \n",
        " The nearest neighbor search is based on the values of the feature layer.\n",
        " '''\n",
        " def test(self):\n",
        "  self.test_image_placeholder = tf.placeholder(dtype=tf.float32, \n",
        "                                shape=[25, IMG_ROWS, IMG_COLS, 3])\n",
        "  self.test_label_placeholder = tf.placeholder(dtype=tf.int32, shape=[25])\n",
        "  ##########################\n",
        "  # Build test graph\n",
        "  logits, global_pool = inference(self.test_image_placeholder, \n",
        "  n=FLAGS.num_residual_blocks, reuse=False,\n",
        "  keep_prob_placeholder=self.dropout_prob_placeholder)\n",
        "  \n",
        "  predictions = tf.nn.softmax(logits)\n",
        "  test_error = self.top_k_error(predictions, self.test_label_placeholder, 1)\n",
        "\n",
        "  saver = tf.train.Saver(tf.all_variables())\n",
        "  sess = tf.Session()\n",
        "  saver.restore(sess, FLAGS.test_ckpt_path)\n",
        "  print('Model restored!')\n",
        "  ##########################\n",
        "  test_df = prepare_df(FLAGS.test_path, \n",
        "  usecols=['image_path', 'category', 'x1', 'y1', 'x2', 'y2'], shuffle=False)\n",
        "  test_df = test_df.iloc[-25:, :]\n",
        "\n",
        "  prediction_np = np.array([]).reshape(-1, 6)\n",
        "  fc_np = np.array([]).reshape(-1, 64)\n",
        "  \n",
        "  for step in range(len(test_df) // TEST_BATCH_SIZE):\n",
        "      if step % 100 == 0:\n",
        "          print('Testing %i batches...' %step)\n",
        "          if step != 0:\n",
        "              print('Test_error = ', test_error_value)\n",
        "\n",
        "      df_batch = test_df.iloc[step*25 : (step+1)*25, :]\n",
        "      test_batch, test_label = load_data_numpy(df_batch)\n",
        "\n",
        "      prediction_batch_value, test_error_value = sess.run([predictions, test_error],\n",
        "      feed_dict={\n",
        "          self.test_image_placeholder:test_batch, \n",
        "          self.test_label_placeholder: test_label})\n",
        "      fc_batch_value = sess.run(global_pool, \n",
        "      feed_dict={\n",
        "          self.test_image_placeholder:test_batch, \n",
        "          self.test_label_placeholder: test_label})\n",
        "\n",
        "      prediction_np = np.concatenate((prediction_np, prediction_batch_value), axis=0)\n",
        "      fc_np = np.concatenate((fc_np, fc_batch_value))\n",
        "\n",
        "  print('Predictin array has shape ', fc_np.shape)\n",
        "  np.save(FLAGS.fc_path, fc_np[-5:,:])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HhM8DMtY4Mr2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "## Recommendation Results\n",
        "train = Train()\n",
        "train.train()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0Se2FcbT4Pyk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#To test the model\n",
        "train.test()"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}