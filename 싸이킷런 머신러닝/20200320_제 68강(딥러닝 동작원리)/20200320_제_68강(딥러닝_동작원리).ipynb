{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "20200320_제 68강(딥러닝 동작원리).ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "ict8Z57SiMq2",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 125
        },
        "outputId": "0ed241ac-d7d7-46f3-8b10-7e64dd767e6a"
      },
      "source": [
        "## deeplearning.pdf 교재로 사용(이어서 학습)\n",
        "'''\n",
        "최소제곱법\n",
        "최소 제곱법 공식으로 구한 성적 예측 코딩\n",
        "'''\n",
        "\n",
        "import numpy as np\n",
        "# x 값과 y 값\n",
        "x=[2, 4, 6, 8]\n",
        "y=[81, 93, 91, 97]\n",
        "\n",
        "# x와 y의 평균값\n",
        "mx = np.mean(x)\n",
        "my = np.mean(y)\n",
        "print(\"x의 평균값:\", mx)\n",
        "print(\"y의 평균값:\", my)\n",
        "\n",
        "# 기울기 공식의 분모\n",
        "divisor = sum([(mx - i)**2 for i in x])\n",
        "\n",
        "# 기울기 공식의 분자\n",
        "def top(x, mx, y, my):\n",
        "    d = 0\n",
        "    for i in range(len(x)):\n",
        "        d += (x[i] - mx) * (y[i] - my)\n",
        "    return d\n",
        "\n",
        "dividend = top(x, mx, y, my)\n",
        "\n",
        "print(\"분모:\", divisor)\n",
        "print(\"분자:\", dividend)\n",
        "\n",
        "# 기울기와 y 절편 구하기\n",
        "a = dividend / divisor\n",
        "b = my - (mx*a)\n",
        "\n",
        "# 출력으로 확인\n",
        "print(\"기울기 a =\", a)\n",
        "print(\"y 절편 b =\", b)"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "x의 평균값: 5.0\n",
            "y의 평균값: 90.5\n",
            "분모: 20.0\n",
            "분자: 46.0\n",
            "기울기 a = 2.3\n",
            "y 절편 b = 79.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5jIyvttvgL6G",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 107
        },
        "outputId": "1438e501-87c8-4522-adde-1011e4e86ffc"
      },
      "source": [
        "'''\n",
        "평균 제곱근 오차(root mean square error) 구현 코드\n",
        "'''\n",
        "import numpy as np\n",
        "\n",
        "# 기울기 a와 y 절편 b\n",
        "ab = [3, 76]\n",
        "\n",
        "# x, y의 데이터 값\n",
        "data = [[2, 81], [4, 93], [6, 91], [8, 97]]\n",
        "x = [i[0] for i in data]\n",
        "y = [i[1] for i in data]\n",
        "\n",
        "# y = ax + b에 a와 b 값을 대입하여 결과를 출력하는 함수\n",
        "def predict(x):\n",
        "    return ab[0]*x + ab[1]\n",
        "\n",
        "# RMSE 함수\n",
        "def rmse(p, a):\n",
        "    return np.sqrt(((p - a) ** 2).mean())\n",
        "\n",
        "# RMSE 함수를 각 y 값에 대입하여 최종 값을 구하는 함수\n",
        "def rmse_val(predict_result,y):\n",
        "    return rmse(np.array(predict_result), np.array(y))\n",
        "\n",
        "# 예측 값이 들어갈 빈 리스트\n",
        "predict_result = []\n",
        "\n",
        "# 모든 x 값을 한 번씩 대입하여\n",
        "for i in range(len(x)):\n",
        "    # predict_result 리스트를 완성한다.\n",
        "    predict_result.append(predict(x[i]))\n",
        "    print(\"공부한 시간 = %.f, 실제 점수 = %.f, 예측 점수 = %.f\" % (x[i], y[i], predict(x[i])))\n",
        "# 최종 RMSE 출력\n",
        "print(\"rmse 최종값: \" + str(rmse_val(predict_result,y)))\n"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "공부한 시간 = 2, 실제 점수 = 81, 예측 점수 = 82\n",
            "공부한 시간 = 4, 실제 점수 = 93, 예측 점수 = 88\n",
            "공부한 시간 = 6, 실제 점수 = 91, 예측 점수 = 94\n",
            "공부한 시간 = 8, 실제 점수 = 97, 예측 점수 = 100\n",
            "rmse 최종값: 3.3166247903554\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s1Eo_5p6hs-l",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 459
        },
        "outputId": "a180de5b-7060-40b9-fc1f-185eaecbec1b"
      },
      "source": [
        "'''\n",
        "오차를 줄여나가는 방법을 알아보자 \n",
        "오차 수정 – 경사하강법(gradient decent)\n",
        " 기울기 a를 무한대로 키우면 오차도 무한대로 커지고 a를 무한대로 작게 해도 역시 오차도 무한대로 \n",
        "  작아지는 상관관계는 이차 함수 그래프로 표현할 수 있습니다.\n",
        "\n",
        "(p.50)\n",
        " y = X2 그래프에서 X에 a1 , a2그리고 m을 대입하여 미분하면 각 점에서의 순간 기울기가 그려집니다.\n",
        " '순간 기울기가 0인 점이 최솟값 m이다.'\n",
        " 그래프가 이차 함수 포물선이므로 꼭짓점의 기울기는 x축과 평행한 선이 되며, 기울기가 0입니다.\n",
        " 경사 하강법은 반복적으로 기울기 a를 변화시켜서 m의 값을 찾아내는 방법 (‘미분 값이 0인 지점’을 찾는\n",
        "것)입니다.\n",
        "'''\n",
        "\n",
        "'''\n",
        "• 오차가 가장 작을 때는 x가 그래프의 가장 아래쪽의 볼록한 부분에\n",
        "이르렀을 때입니다.\n",
        "• 즉, 기울기 a가 m 위치에 있을 때입니다.\n",
        "• 컴퓨터를 이용해 m의 값을 구하려면 임의의 한 점(a1)을 찍고 이 점\n",
        "을 m에 가까운 쪽으로 점점 이동(a1 → a2 → a3)시키는 과정이 필요\n",
        "합니다.\n",
        "• ( a1의 값보다 a2의 값이 m에 더 가깝고 a2의 값보다 a3가 m에 더\n",
        "가깝다는 것을 컴퓨터가 알아야 하겠지요)\n",
        "• 미분 기울기를 이용하는 경사 하강법(gradient decent)은 이차 함수\n",
        "그래프에서 오차를 비교하여 가장 작은 방향으로 이동시키는 방법입\n",
        "니다.\n",
        "\n",
        "→ 기울기 a와 오차와의 관계: 적절한 기울기를 찾았을 때 오차가 최소화된다\n",
        "'''\n",
        "\n",
        "'''\n",
        "미분 \n",
        " Y = X2 이라는 그래프의 x축에 한 점 a가 있을 때 y 값은 Y = a2 입니다.\n",
        " a가 아주 미세하게 오른쪽이나 왼쪽으로 이동하면 종속 변수인 y 값도 그에 따라 아주 미세하게 변화합니\n",
        "다.\n",
        " a가 변화량이 0에 가까울 만큼 아주 미세하게 변화하면 y 값의 변화 역시 아주 미세해서 0에 가깝게 변화\n",
        "합니다. (순간변화율)\n",
        " 순간 변화율은 ‘어느 쪽’이라는 방향성을 지니고 있으므로 이 방향에 맞추어 직선(점에서의 ‘기울기’, 접선)\n",
        "을 그릴 수가 있습니다.\n",
        " 미분은 x 값이 아주 미세하게 움직일 때의 y 변화량을 구한 뒤, 이를 x의 변화량으로 나누는 과정입니다.\n",
        "'''\n",
        "\n",
        "'''\n",
        "도함수\n",
        "어떤 함수 안에 포함도나 값 각각이 0에 한없이 가까워지는 극한값(미분계수)을 구하는 함수 \n",
        "y = f(x)의 도함수 f'(x)는 lim{ f(x+◁x) - f(x) / ◁x }\n",
        "'''\n",
        "\n",
        "\n",
        "import tensorflow as tf\n",
        "# x, y의 데이터 값\n",
        "data = [[2, 81], [4, 93], [6, 91], [8, 97]]\n",
        "x_data = [x_row[0] for x_row in data]\n",
        "y_data = [y_row[1] for y_row in data]\n",
        "\n",
        "# 기울기 a와 y 절편 b의 값을 임의로 정한다.\n",
        "# 단, 기울기의 범위는 0 ~ 10 사이이며, y 절편은 0 ~ 100 사이에서 변하게 한다.\n",
        "a = tf.Variable(tf.random_uniform([1], 0, 10, dtype = tf.float64,seed = 0))\n",
        "b = tf.Variable(tf.random_uniform([1], 0, 100, dtype = tf.float64, seed = 0))\n",
        "\n",
        "# y에 대한 일차 방정식 ax+b의 식을 세운다.\n",
        "y = a * x_data + b\n",
        "\n",
        "# 텐서플로 RMSE 함수\n",
        "rmse = tf.sqrt(tf.reduce_mean(tf.square( y - y_data )))\n",
        "\n",
        "# 학습률 값\n",
        "learning_rate = 0.1\n",
        "\n",
        "# RMSE 값을 최소로 하는 값 찾기\n",
        "gradient_decent = tf.train.GradientDescentOptimizer(learning_rate).minimize(rmse)\n",
        "\n",
        "# 텐서플로를 이용한 학습\n",
        "with tf.Session() as sess:\n",
        "    # 변수 초기화\n",
        "    sess.run(tf.global_variables_initializer())\n",
        "    # 2001번 실행(0번째를 포함하므로)\n",
        "    for step in range(2001):\n",
        "        sess.run(gradient_decent)\n",
        "        # 100번마다 결과 출력\n",
        "        if step % 100 == 0:\n",
        "            print(\"Epoch: %.f, RMSE = %.04f, 기울기 a = %.4f, y 절편b = %.4f\" % (step,sess.run(rmse),\n",
        "                                                                                 sess.run(a),sess.run(b)))\n",
        "        # 약 1300번째부터 기울기 값이,\n",
        "        #    1800번째부터 절편 값이 고정되는 것을 확인 "
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<p style=\"color: red;\">\n",
              "The default version of TensorFlow in Colab will soon switch to TensorFlow 2.x.<br>\n",
              "We recommend you <a href=\"https://www.tensorflow.org/guide/migrate\" target=\"_blank\">upgrade</a> now \n",
              "or ensure your notebook will continue to use TensorFlow 1.x via the <code>%tensorflow_version 1.x</code> magic:\n",
              "<a href=\"https://colab.research.google.com/notebooks/tensorflow_version.ipynb\" target=\"_blank\">more info</a>.</p>\n"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch: 0, RMSE = 30.2139, 기울기 a = 7.5235, y 절편b = 80.5984\n",
            "Epoch: 100, RMSE = 2.8860, 기울기 a = 2.2299, y 절편b = 79.4181\n",
            "Epoch: 200, RMSE = 2.8826, 기울기 a = 2.2601, y 절편b = 79.2379\n",
            "Epoch: 300, RMSE = 2.8815, 기울기 a = 2.2773, y 절편b = 79.1353\n",
            "Epoch: 400, RMSE = 2.8811, 기울기 a = 2.2871, y 절편b = 79.0770\n",
            "Epoch: 500, RMSE = 2.8810, 기울기 a = 2.2927, y 절편b = 79.0438\n",
            "Epoch: 600, RMSE = 2.8810, 기울기 a = 2.2958, y 절편b = 79.0249\n",
            "Epoch: 700, RMSE = 2.8810, 기울기 a = 2.2976, y 절편b = 79.0142\n",
            "Epoch: 800, RMSE = 2.8810, 기울기 a = 2.2987, y 절편b = 79.0081\n",
            "Epoch: 900, RMSE = 2.8810, 기울기 a = 2.2992, y 절편b = 79.0046\n",
            "Epoch: 1000, RMSE = 2.8810, 기울기 a = 2.2996, y 절편b = 79.0026\n",
            "Epoch: 1100, RMSE = 2.8810, 기울기 a = 2.2998, y 절편b = 79.0015\n",
            "Epoch: 1200, RMSE = 2.8810, 기울기 a = 2.2999, y 절편b = 79.0008\n",
            "Epoch: 1300, RMSE = 2.8810, 기울기 a = 2.2999, y 절편b = 79.0005\n",
            "Epoch: 1400, RMSE = 2.8810, 기울기 a = 2.3000, y 절편b = 79.0003\n",
            "Epoch: 1500, RMSE = 2.8810, 기울기 a = 2.3000, y 절편b = 79.0002\n",
            "Epoch: 1600, RMSE = 2.8810, 기울기 a = 2.3000, y 절편b = 79.0001\n",
            "Epoch: 1700, RMSE = 2.8810, 기울기 a = 2.3000, y 절편b = 79.0001\n",
            "Epoch: 1800, RMSE = 2.8810, 기울기 a = 2.3000, y 절편b = 79.0000\n",
            "Epoch: 1900, RMSE = 2.8810, 기울기 a = 2.3000, y 절편b = 79.0000\n",
            "Epoch: 2000, RMSE = 2.8810, 기울기 a = 2.3000, y 절편b = 79.0000\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1ZWPPX3jm8Oq",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 395
        },
        "outputId": "25788b6f-7c79-4196-8519-323d094d8f6e"
      },
      "source": [
        "# 경사 하강법으로 다중 선형 회귀 구현 코드\n",
        "'''\n",
        "ex/ 공부한 시간 = x1, 과외수업횟수 = x2 성적 = y 로 독립변수와 종속변수 지정 \n",
        "y = a1*x1 + a2*x2 + b\n",
        "'''\n",
        "\n",
        "import tensorflow as tf\n",
        "\n",
        "# x1, x2, y의 데이터 값\n",
        "data = [[2, 0, 81], [4, 4, 93], [6, 2, 91], [8, 3, 97]]\n",
        "x1 = [x_row1[0] for x_row1 in data]\n",
        "x2 = [x_row2[1] for x_row2 in data] # 새로 추가되는 값\n",
        "y_data = [y_row[2] for y_row in data]\n",
        "\n",
        "# 기울기 a와 y 절편 b의 값을 임의로 정한다.\n",
        "# 단, 기울기의 범위는 0 ~ 10 사이이며, y 절편은 0 ~ 100 사이에서 변하게 한다.\n",
        "a1 = tf.Variable(tf.random_uniform([1], 0, 10, dtype=tf.float64, seed=0))\n",
        "a2 = tf.Variable(tf.random_uniform([1], 0, 10, dtype=tf.float64, seed=0)) # 새로 추가되는 값\n",
        "b = tf.Variable(tf.random_uniform([1], 0, 100, dtype=tf.float64, seed=0))\n",
        "\n",
        "# 새로운 방정식\n",
        "y = a1 * x1 + a2 * x2+ b\n",
        "\n",
        "# 텐서플로 RMSE 함수\n",
        "rmse = tf.sqrt(tf.reduce_mean(tf.square( y - y_data )))\n",
        "\n",
        "# 학습률 값\n",
        "learning_rate = 0.1\n",
        "\n",
        "# RMSE 값을 최소로 하는 값 찾기\n",
        "gradient_decent = tf.train.GradientDescentOptimizer(learning_rate).minimize(rmse)\n",
        "\n",
        "# 학습이 진행되는 부분\n",
        "with tf.Session() as sess:\n",
        "    sess.run(tf.global_variables_initializer())\n",
        "    for step in range(2001):\n",
        "        sess.run(gradient_decent)\n",
        "        if step % 100 == 0:\n",
        "            print(\"Epoch: %.f, RMSE = %.04f, 기울기 a1 = %.4f, 기울기 a2 = %.4f, y 절편 b = %.4f\" \n",
        "                  % (step,sess.run(rmse), sess.run(a1), sess.run(a2),sess.run(b)))\n",
        "            # 오차율이 가장 적은 지점은 약 100번 지점익 그 이후부터는 오히려 조금 늘어났다가 고정됨 "
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch: 0, RMSE = 49.1842, 기울기 a1 = 7.5270, 기울기 a2 = 7.8160, y 절편 b = 80.5980\n",
            "Epoch: 100, RMSE = 1.8368, 기울기 a1 = 1.1306, 기울기 a2 = 2.1316, y 절편 b = 78.5119\n",
            "Epoch: 200, RMSE = 1.8370, 기울기 a1 = 1.1879, 기울기 a2 = 2.1487, y 절편 b = 78.1057\n",
            "Epoch: 300, RMSE = 1.8370, 기울기 a1 = 1.2122, 기울기 a2 = 2.1571, y 절편 b = 77.9352\n",
            "Epoch: 400, RMSE = 1.8370, 기울기 a1 = 1.2226, 기울기 a2 = 2.1607, y 절편 b = 77.8636\n",
            "Epoch: 500, RMSE = 1.8370, 기울기 a1 = 1.2269, 기울기 a2 = 2.1622, y 절편 b = 77.8335\n",
            "Epoch: 600, RMSE = 1.8370, 기울기 a1 = 1.2288, 기울기 a2 = 2.1628, y 절편 b = 77.8208\n",
            "Epoch: 700, RMSE = 1.8370, 기울기 a1 = 1.2295, 기울기 a2 = 2.1631, y 절편 b = 77.8155\n",
            "Epoch: 800, RMSE = 1.8370, 기울기 a1 = 1.2299, 기울기 a2 = 2.1632, y 절편 b = 77.8133\n",
            "Epoch: 900, RMSE = 1.8370, 기울기 a1 = 1.2300, 기울기 a2 = 2.1632, y 절편 b = 77.8124\n",
            "Epoch: 1000, RMSE = 1.8370, 기울기 a1 = 1.2301, 기울기 a2 = 2.1633, y 절편 b = 77.8120\n",
            "Epoch: 1100, RMSE = 1.8370, 기울기 a1 = 1.2301, 기울기 a2 = 2.1633, y 절편 b = 77.8118\n",
            "Epoch: 1200, RMSE = 1.8370, 기울기 a1 = 1.2301, 기울기 a2 = 2.1633, y 절편 b = 77.8117\n",
            "Epoch: 1300, RMSE = 1.8370, 기울기 a1 = 1.2301, 기울기 a2 = 2.1633, y 절편 b = 77.8117\n",
            "Epoch: 1400, RMSE = 1.8370, 기울기 a1 = 1.2301, 기울기 a2 = 2.1633, y 절편 b = 77.8117\n",
            "Epoch: 1500, RMSE = 1.8370, 기울기 a1 = 1.2301, 기울기 a2 = 2.1633, y 절편 b = 77.8117\n",
            "Epoch: 1600, RMSE = 1.8370, 기울기 a1 = 1.2301, 기울기 a2 = 2.1633, y 절편 b = 77.8117\n",
            "Epoch: 1700, RMSE = 1.8370, 기울기 a1 = 1.2301, 기울기 a2 = 2.1633, y 절편 b = 77.8117\n",
            "Epoch: 1800, RMSE = 1.8370, 기울기 a1 = 1.2301, 기울기 a2 = 2.1633, y 절편 b = 77.8117\n",
            "Epoch: 1900, RMSE = 1.8370, 기울기 a1 = 1.2301, 기울기 a2 = 2.1633, y 절편 b = 77.8117\n",
            "Epoch: 2000, RMSE = 1.8370, 기울기 a1 = 1.2301, 기울기 a2 = 2.1633, y 절편 b = 77.8117\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ja7jDILEvs3o",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 269
        },
        "outputId": "6e2588c2-050b-49c6-e7c6-b2a56a781908"
      },
      "source": [
        "'''\n",
        "로지스틱 회귀(logistic regression)\n",
        " 로지스틱 회귀는 선형 회귀와 마찬가지로 적절한 선을 그려가는 과정입니다. 다만, 직선이 아니라, 참(1)과\n",
        "거짓(0) 사이를 구분하는 S자 형태의 선을 그어 주는 작업입니다.\n",
        "'''\n",
        "'''\n",
        "시그모이드 함수(sigmoid function)\n",
        " S자 형태로 그래프가 그려지는 함수\n",
        " e는 자연 상수라고 불리는 무리수로 값은 2.71828...입니다.\n",
        " a는 그래프의 경사도를 결정합니다.\n",
        " a 값이 커지면 경사가 커지고 a 값이 작아지면 경사가 작아집니다\n",
        " b는 그래프의 좌우 이동을 의미합니다.\n",
        " b 값이 크고 작아짐에 따라 그래프가 이동합니다.\n",
        "\n",
        "기울기와 절편 \n",
        "a가 작아질수록 오차는 무한대로 커지지만, a가 커진다고 해서 오차가 무한대로 커지지는 않는다.\n",
        "b 값이 너무 작아지거나 커지면 오차도 무한대로 커진다. 이차 함수 그래프로 표현할 수 있습니다.\n",
        "'''\n",
        "\n",
        "'''\n",
        "오차공식\n",
        " 시그모이드 함수에서 a와 b의 값은 경사 하강법(오차를 구한 다음 오차가 작은 쪽으로 이동시키는 방법)\n",
        "으로 구합니다\n",
        " 시그모이드 함수의 특징은 y 값이 0과 1 사이 입니다.\n",
        " 실제 값이 1일 때 예측 값이 0에 가까워지면 오차가 커져야 합니다.\n",
        " 실제 값이 0일 때 예측 값이 1에 가까워지는 경우에도 오차는 커져야 합니다.\n",
        "\n",
        "로그함수\n",
        "예측 값이 1일 때 오차가 0이고, 예측 값이 0에 가까울수록 오차는 커지는 로그함수 그래프\n",
        "-log h\n",
        "\n",
        "예측 값이 0일 때 오차가 없고, 1에 가까워질수록 오차가 매우 커지는 로그함수 그래프\n",
        "-log (1 - h)\n",
        "\n",
        " y의 실제 값이 1일 때 -log h 그래프를 쓰고, 0일 때 -log (1 - h) 그래프를 써야 합니다.\n",
        " 실제 값 y가 1이면 B 부분이 없어집니다. 반대로 0이면 A 부분이 없어집니다.\n",
        "'''\n",
        "\n",
        "# 로지스틱 회귀 구현 코드\n",
        "\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "\n",
        "# x, y의 데이터 값\n",
        "data = [[2, 0], [4, 0], [6, 0], [8, 1], [10, 1], [12, 1], [14,1]]\n",
        "x_data = [x_row[0] for x_row in data]\n",
        "y_data = [y_row[1] for y_row in data]\n",
        "\n",
        "# a와 b의 값을 임의로 정한다.\n",
        "a = tf.Variable(tf.random_normal([1], dtype=tf.float64, seed=0))\n",
        "b = tf.Variable(tf.random_normal([1], dtype=tf.float64, seed=0))\n",
        "\n",
        "# y 시그모이드 함수의 방정식을 세운다.\n",
        "y = 1/(1 + np.e**(a * x_data + b))\n",
        "\n",
        "# loss를 구하는 함수\n",
        "loss = -tf.reduce_mean(np.array(y_data) * tf.log(y) + (1 -np.array(y_data)) * tf.log(1 - y))\n",
        "\n",
        "# 학습률 값\n",
        "learning_rate = 0.5\n",
        "\n",
        "# loss를 최소로 하는 값 찾기\n",
        "gradient_decent = tf.train.GradientDescentOptimizer(learning_rate).minimize(loss)\n",
        "\n",
        "# 학습\n",
        "with tf.Session() as sess:\n",
        "    sess.run(tf.global_variables_initializer())\n",
        "    for i in range(78001):\n",
        "        sess.run(gradient_decent)\n",
        "        if i % 6000 == 0:\n",
        "            print(\"Epoch: %.f, loss = %.4f, 기울기 a = %.4f, y 절편= %.4f\" \n",
        "                            % (i, sess.run(loss), sess.run(a), sess.run(b)))"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch: 0, loss = 1.2676, 기울기 a = 0.1849, y 절편= -0.4334\n",
            "Epoch: 6000, loss = 0.0152, 기울기 a = -2.9211, y 절편= 20.2981\n",
            "Epoch: 12000, loss = 0.0081, 기울기 a = -3.5637, y 절편= 24.8010\n",
            "Epoch: 18000, loss = 0.0055, 기울기 a = -3.9557, y 절편= 27.5463\n",
            "Epoch: 24000, loss = 0.0041, 기울기 a = -4.2380, y 절편= 29.5231\n",
            "Epoch: 30000, loss = 0.0033, 기울기 a = -4.4586, y 절편= 31.0675\n",
            "Epoch: 36000, loss = 0.0028, 기울기 a = -4.6396, y 절편= 32.3346\n",
            "Epoch: 42000, loss = 0.0024, 기울기 a = -4.7930, y 절편= 33.4086\n",
            "Epoch: 48000, loss = 0.0021, 기울기 a = -4.9261, y 절편= 34.3406\n",
            "Epoch: 54000, loss = 0.0019, 기울기 a = -5.0436, y 절편= 35.1636\n",
            "Epoch: 60000, loss = 0.0017, 기울기 a = -5.1489, y 절편= 35.9005\n",
            "Epoch: 66000, loss = nan, 기울기 a = nan, y 절편= nan\n",
            "Epoch: 72000, loss = nan, 기울기 a = nan, y 절편= nan\n",
            "Epoch: 78000, loss = nan, 기울기 a = nan, y 절편= nan\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ePqRtqncxcRd",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 197
        },
        "outputId": "d628d9c7-946f-4bbb-c08b-5aba6acef65d"
      },
      "source": [
        "'''\n",
        "여러 입력 값을 갖는 로지스틱 회귀 코드 구현\n",
        " tf.placeholder(‘데이터형’, ’행렬의 차원’, ’이름’) 는 입력 값을 저장하는데 사용합니다..\n",
        " a1x1 + a2x2는 행렬곱을 이용해 [a1, a] * [x1, x2]로도 표현할 수 있습니다\n",
        " 텐서플로에서는 matmul() 함수를 이용해 행렬곱을 적용합니다\n",
        " 텐서플로 내장 sigmoid() 함수 - 시그모이드를 계산\n",
        "'''\n",
        "\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "# 실행할 때마다 같은 결과를 출력하기 위한 seed 값 설정\n",
        "seed = 0\n",
        "np.random.seed(seed)\n",
        "tf.set_random_seed(seed)\n",
        "# x, y의 데이터 값\n",
        "x_data = np.array([[2, 3],[4, 3],[6, 4],[8, 6],[10, 7],[12,8],[14, 9]])\n",
        "y_data = np.array([0, 0, 0, 1, 1, 1,1]).reshape(7, 1)\n",
        "# 입력 값을 플레이스 홀더에 저장\n",
        "X = tf.placeholder(tf.float64, shape=[None, 2])\n",
        "Y = tf.placeholder(tf.float64, shape=[None, 1])\n",
        "\n",
        "# 기울기 a와 바이어스 b의 값을 임의로 정함\n",
        "a = tf.Variable(tf.random_uniform([2,1], dtype=tf.float64))\n",
        "\n",
        "# [2,1] 의미: 들어오는 값은 2개, 나가는 값은 1개\n",
        "b = tf.Variable(tf.random_uniform([1], dtype=tf.float64))\n",
        "y = tf.sigmoid(tf.matmul(X, a) + b) # y 시그모이드 함수의 방정식을 세움\n",
        "loss = -tf.reduce_mean(Y * tf.log(y) + (1 - Y) * tf.log(1 - y)) # 오차를 구하는 함수\n",
        "learning_rate=0.1 # 학습률 값\n",
        "\n",
        "# 오차를 최소로 하는 값 찾기\n",
        "gradient_decent = tf.train.GradientDescentOptimizer(learning_rate).minimize(loss)\n",
        "predicted = tf.cast(y > 0.5, dtype=tf.float64)\n",
        "accuracy = tf.reduce_mean(tf.cast(tf.equal(predicted, Y), dtype=tf.float64))\n",
        "\n",
        "sess = tf.Session() #학습(session 안닫히게 with 제외)\n",
        "sess.run(tf.global_variables_initializer())\n",
        "for i in range(3001):\n",
        "    a_, b_, loss_, _ = sess.run([a, b, loss, gradient_decent], feed_dict={X: x_data, Y: y_data})\n",
        "    if (i + 1) % 300 == 0:\n",
        "        print(\"step=%d, a1=%.4f, a2=%.4f, b=%.4f, loss=%.4f\"% (i + 1, a_[0], a_[1], b_, loss_))"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "step=300, a1=0.8731, a2=-0.6778, b=-2.1960, loss=0.2813\n",
            "step=600, a1=0.8668, a2=-0.3881, b=-3.7214, loss=0.1998\n",
            "step=900, a1=0.7702, a2=-0.0451, b=-4.8190, loss=0.1552\n",
            "step=1200, a1=0.6607, a2=0.2738, b=-5.6836, loss=0.1264\n",
            "step=1500, a1=0.5569, a2=0.5569, b=-6.3982, loss=0.1063\n",
            "step=1800, a1=0.4635, a2=0.8058, b=-7.0076, loss=0.0915\n",
            "step=2100, a1=0.3808, a2=1.0248, b=-7.5389, loss=0.0803\n",
            "step=2400, a1=0.3080, a2=1.2186, b=-8.0101, loss=0.0715\n",
            "step=2700, a1=0.2437, a2=1.3913, b=-8.4333, loss=0.0644\n",
            "step=3000, a1=0.1866, a2=1.5464, b=-8.8176, loss=0.0585\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4jAkR79JyR4E",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "outputId": "67cca1b7-2232-435a-8ae5-0e48b09fe8e3"
      },
      "source": [
        "#기존 세션 종류 \n",
        "new_x = np.array([7, 6.]).reshape(1, 2) # [7, 6]은 각각 공부한 시간과 과외 수업 횟수\n",
        "new_y = sess.run(y, feed_dict={X: new_x})\n",
        "\n",
        "print(\"공부한 시간: %d, 과외 수업 횟수: %d\" % (new_x[:,0], new_x[:,1]))\n",
        "print(\"합격 가능성: %6.2f %%\" % (new_y*100))"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "공부한 시간: 7, 과외 수업 횟수: 6\n",
            "합격 가능성:  85.42 %\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yTz63gZtIf1T",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 89
        },
        "outputId": "b527e383-aa47-4133-fc93-12e2e206933d"
      },
      "source": [
        "'''\n",
        "퍼셉트론(perceptron)\n",
        " 뉴런과 뉴런이 서로 새로운 연결을 만들기도 하고 필요에 따라 위치를 바꾸는 것처럼, 여러 층의 퍼셉트\n",
        "론을 서로 연결시키고 복잡하게 조합하여 주어진 입력 값에 대한 판단을 하게 하는 것\n",
        " 퍼셉트론은 입력 값과 활성화 함수를 사용해 출력 값을 다음으로 넘기는 가장 작은 신경망 단위\n",
        " N개의 이진수가 하나의 뉴런을 통과해서 가중합 0보다 크면 활성화되는 가장 간단한 신경망 구조\n",
        " 초평면(hyperplane)으로 구분되는 두 개의 공간을 분리시키는 역할을 한다\n",
        " AND 게이트 , OR 게이트를 만들 수 있다.\n",
        "\n",
        "가중합 - 입력 값(x)과 가중치(w)의 곱을 모두 더한 값에 바이어스(b)를 더한 값\n",
        " 가중합의 결과를 놓고 1 또는 0을 출력해서 다음으로 보냅니다.\n",
        " 활성화 함수(activation function) - 0과 1을 판단하는 함수\n",
        "'''\n",
        "\n",
        "'''\n",
        "exclusive OR) 문제\n",
        " XOR(exclusive OR) - 둘 중 하나만 1일 때 1이 출력\n",
        " 결괏값이 0이면 흰점으로, 1이면 검은점으로 각각 그래프로 좌표 평면에 표현\n",
        "AND와 OR 게이트는 직선을 그어 결괏값이 1인 값(검은점)을 구별할 수 있습니다.\n",
        "XOR의 경우 선을 그어 구분할 수 없습니다\n",
        "\n",
        " 좌표 평면 자체에 변화를 주는 것\n",
        " XOR 문제를 해결하기 위해서 우리는 두 개의 퍼셉트론을 한 번에 계산할 수 있어야 합니다.\n",
        "\n",
        " 은닉층(hidden layer)\n",
        "퍼셉트론(perceptron)\n",
        " 입력 값(input)을 놓고 파란색(위)과 빨간색(아래)의 영역을 구분한다고 할 때, 왼쪽은 어떤 직선으로도 이\n",
        "를 해결할 수 없습니다.\n",
        " 은닉층을 만들어 공간을 왜곡하면 두 영역을 가로지르는 선이 직선으로 바뀝니다\n",
        " 은닉층이 좌표 평면을 왜곡시키는 결과를 가져옵니다.\n",
        "'''\n",
        "\n",
        "'''\n",
        "다층 퍼셉트론(perceptron)\n",
        " 은닉층으로 퍼셉트론이 각각 자신의 가중치(w)와 바이어스(b) 값을 보내고\n",
        " 은닉층에서 모인 값이 한 번 더 시그모이드 함수(기호로 σ 라고 표시합니다)를 이용해 최종 값으로 결과\n",
        "를 보냅니다.\n",
        " 은닉층에 모이는 중간 정거장을 노드(node)라고 하며, (n1과 n2)\n",
        " 은닉층의 노드 n1과 n2의 값은 각각 단일 퍼셉트론의 값과 같습니다.\n",
        "74\n",
        "딥러닝 동작원리\n",
        " 두 식의 결괏값이 출력층으로 보내집니다.\n",
        " 출력층에서는 시그모이드 함수를 통해 y 값이 정해\n",
        "집니다.\n",
        "''' \n",
        "\n",
        "# XOR 문제의 해결 코드 구현\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "# 가중치와 바이어스\n",
        "w11 = np.array([-2, -2])\n",
        "w12 = np.array([2, 2])\n",
        "w2 = np.array([1, 1])\n",
        "b1 = 3\n",
        "b2 = -1\n",
        "b3 = -1\n",
        "\n",
        "# 퍼셉트론\n",
        "def MLP(x, w, b):\n",
        "    y = np.sum(w * x) + b\n",
        "    if y <= 0:\n",
        "        return 0\n",
        "    else:\n",
        "        return 1\n",
        "\n",
        "# NAND 게이트\n",
        "def NAND(x1, x2):\n",
        "    return MLP(np.array([x1, x2]), w11, b1)\n",
        "\n",
        "# OR 게이트\n",
        "def OR(x1, x2):\n",
        "    return MLP(np.array([x1, x2]), w12, b2)\n",
        "\n",
        "# AND 게이트\n",
        "def AND(x1, x2):\n",
        "    return MLP(np.array([x1, x2]), w2, b3)\n",
        "\n",
        "# XOR 게이트\n",
        "def XOR(x1, x2):\n",
        "    return AND(NAND(x1, x2),OR(x1, x2))\n",
        "\n",
        "# x1, x2 값을 번갈아 대입해 가며 최종값 출력\n",
        "if __name__ == '__main__':\n",
        "    for x in [(0, 0), (1, 0), (0, 1), (1, 1)]:\n",
        "        y = XOR(x[0], x[1])\n",
        "        print(\"입력 값: \" + str(x) + \" 출력 값: \" + str(y))"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "입력 값: (0, 0) 출력 값: 0\n",
            "입력 값: (1, 0) 출력 값: 1\n",
            "입력 값: (0, 1) 출력 값: 1\n",
            "입력 값: (1, 1) 출력 값: 0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Qt_b2-U7M-r-",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 143
        },
        "outputId": "b10b36dc-f114-4377-fe79-14904d188a3f"
      },
      "source": [
        "'''\n",
        "오차 역전파 (back propagation)\n",
        " 다층 퍼셉트론에서의 최적화 과정\n",
        " 가중치에서 기울기를 빼도 값의 변화가 없을 때까지 계속해서 가중치 수정 작업을 반복하는 것\n",
        " 출력층으로부터 하나씩 앞으로 되돌아가며 각 층의 가중치를 수정하는 방법\n",
        " 가중치를 수정하려면 미분 값, 즉 기울기가 필요합니다\n",
        "\n",
        " 신경망 내부의 가중치는 오차 역전파 방법을 사용해 수정합니다.\n",
        " 임의의 가중치를 선언하고 최소 제곱법을 이용해 오차를 구한 뒤 이 오차가 최소인 지점으로 계속해서 조\n",
        "금씩 이동시킵니다.\n",
        " 오차가 최소가 되는 점(미분했을 때 기울기가 0이 되는 지점)이 우리가 알고자 하는 답입니다\n",
        "'''\n",
        "\n",
        "'''\n",
        "활성화 함수 - 기울기 소실 문제 해결\n",
        " 활성화 함수 시그모이드는 층이 늘어나면서 기울기가 중간에 0이 되어버리는 기울기 소실(vanishing\n",
        "gradient) 문제가 발생합니다.\n",
        " 시그모이드를 미분하면 최대치가 0.3입니다. 1보다 작으므로 계속 곱하다 보면 0에 가까워집니다.\n",
        " 시그모이드는 층을 거쳐 갈수록 기울기가 사라져 가중치를 수정하기가 어려워집니다.\n",
        "\n",
        "→ 기울기가 사라지는 문제를 해결하기 위해 활성화 함수를 시그모이드가 아닌 여러 함수로 대체하기 시작\n",
        "\n",
        "• 하이퍼볼릭 탄젠트(tanh) - 시그모이드 함수의 범위를 -1에서 1로 확장, 미분한 값의 범위가 함께 확장 되는 효과를 가져왔습니다.\n",
        "여전히 1보다 작은 값이 존재하므로 기울기 소실 문제는 사라지지 않습니다.\n",
        "• 렐루(ReLU) - x가 0보다 작을 때는 모든 값을 0으로 처리하고, 0보다 큰 값은 x를 그대로 사용하는 방법\n",
        "x가 0보다 크기만 하면 미분 값이 1이 됩니다. 따라서 여러 은닉층을 거치며 곱해지더라도 맨 처음 층까지 사라지지 않고\n",
        "남아있을 수 있습니다.\n",
        "'''\n",
        "\n",
        "\n",
        "'''\n",
        "확률적 경사 하강법(SGD)\n",
        " 경사 하강법은 정확하게 가중치를 찾아가지만, 한 번 업데이트할 때마다 전체 데이터를 미분해야 하므로\n",
        "계산량이 매우 많다는 단점이 있습니다.\n",
        " 확률적 경사 하강법은 전체 데이터를 사용하는 것이 아니라, 랜덤하게 추출한 일부 데이터를 사용합니다.\n",
        "일부 데이터를 사용하므로 더 빨리, 자주 업데이트를 하는 것이 가능해졌습니다.\n",
        " 확률적 경사 하강법은 중간 결과의 진폭이 크고 불안정해 보일 수도 있습니다.\n",
        " 속도가 빠르면서도 최적 해에 근사한 값을 찾아낸다는 장점 때문에 경사 하강법의 대안으로 사용되고 있\n",
        "습니다.\n",
        "\n",
        "모멘텀(momentum)\n",
        " 관성, 탄력, 가속도라는 뜻\n",
        " 모멘텀 SGD- 경사 하강법에 탄력을 더해 주는 것\n",
        "경사 하강법과 마찬가지로 매번 기울기를 구하지만, 이를 통해 오차를 수정하기 전 바로 앞\n",
        "수정 값과 방향(+, -)을 참고하여 같은 방향으로 일정한 비율만 수정되게 하는 방법\n",
        "수정 방향이 양수(+) 방향으로 한 번, 음수(-) 방향으로 한 번 지그재그로 일어나는 현상이\n",
        "줄어들고, 이전 이동 값을 고려하여 일정 비율만큼만 다음 값을 결정하므로 관성의 효과를\n",
        "낼 수 있습니다.\n",
        "\n",
        "이외 고급 경사 하강법의 Keras 사용법 p.84 참고\n",
        "'''\n",
        "\n",
        "# Keras를 활용한 실습 \n",
        "# 폐암 수술 환자의 생존율 예측하기 실습\n",
        "# 딥러닝을 구동하는 데 필요한 케라스 함수를 불러옵니다.\n",
        "\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense\n",
        "\n",
        "# 필요한 라이브러리를 불러옵니다.\n",
        "import numpy\n",
        "import tensorflow as tf\n",
        "\n",
        "# 실행할 때마다 같은 결과를 출력하기 위해 설정하는 부분입니다.\n",
        "seed = 0\n",
        "numpy.random.seed(seed)\n",
        "tf.set_random_seed(seed)\n",
        "\n",
        "# 준비된 수술 환자 데이터를 불러들입니다.\n",
        "\n",
        "#in jupyter \n",
        "#Data_set = numpy.loadtxt(\"data/ThoraricSurgery.csv\" , delimiter=\",\") \n",
        "\n",
        "#in colab\n",
        "Data_set = numpy.loadtxt(\"/content/drive/My Drive/Colab Notebooks/싸이킷런 머신러닝(박태정T)/data/ThoraricSurgery.csv\" , delimiter=\",\") \n",
        "display(Data_set)\n",
        "\n",
        "# 환자의 기록과 수술 결과를 X와 Y로 구분하여 저장합니다.\n",
        "X = Data_set[:,0:17]\n",
        "Y = Data_set[:,17]"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "array([[293.  ,   1.  ,   3.8 , ...,   0.  ,  62.  ,   0.  ],\n",
              "       [  1.  ,   2.  ,   2.88, ...,   0.  ,  60.  ,   0.  ],\n",
              "       [  8.  ,   2.  ,   3.19, ...,   0.  ,  66.  ,   1.  ],\n",
              "       ...,\n",
              "       [406.  ,   6.  ,   5.36, ...,   0.  ,  62.  ,   0.  ],\n",
              "       [ 25.  ,   8.  ,   4.32, ...,   0.  ,  58.  ,   1.  ],\n",
              "       [447.  ,   8.  ,   5.2 , ...,   0.  ,  49.  ,   0.  ]])"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JP3_1LpkRU2s",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "010ccea4-6652-4400-9558-4414d503d236"
      },
      "source": [
        "# 딥러닝 구조를 결정합니다(모델을 설정하고 실행).\n",
        "model = Sequential() #딥러닝의 구조를 짜고 층을 설정\n",
        "\n",
        "# 첫 번째 은닉층에 input_dim을 적어 줌으로써 첫 번째 Dense가 은닉층 + 입력층의 역할을 겸합니다.\n",
        "# 데이터에서 17개의 값을 받아 은닉층의 30개 노드로 보낸다\n",
        "model.add(Dense(30, input_dim=17, activation='relu')) #activation : 출력층으로 전달할 때 사용할 활성화 함수\n",
        "model.add(Dense(1, activation='sigmoid')) #출력층의 노드 수는 1개, 최종 출력 값에 사용될 활성화 함수\n",
        "\n",
        "# 딥러닝을 실행합니다. (오차 함수 : 평균 제곱 오차 함수 사용)\n",
        "model.compile(loss='mean_squared_error', optimizer='adam', metrics=['accuracy'])\n",
        "model.fit(X, Y, epochs=30, batch_size=10)\n",
        "\n",
        "# 결과를 출력합니다.\n",
        "print(\"\\n Accuracy: %.4f\" % (model.evaluate(X, Y)[1]))\n",
        "\n",
        "\n",
        "''' 실습 복기 \n",
        " 평균 제곱 오차는 수렴하기까지 속도가 많이 걸린다는 단점이 있다\n",
        " 교차 엔트로피는 출력 값에 로그를 취해서, 오차가 커지면 수렴 속도가 빨라지고 오차가 작아지면 속도가 감소하게\n",
        "끔 만듭니다.\n",
        " metrics 함수는 모델이 컴파일될 때 모델 수행 결과를 나타내게끔 설정하는 부분입니다. 정확도를 측정하기 위해 사\n",
        "용되는 테스트 샘플을 학습 과정에서 제외시킴으로써 과적합 문제(over fitting, 특정 데이터에서는 잘 작동하나 다른\n",
        "데이터에서는 잘 작동하지 않는 문제)를 방지하는 기능을 담고 있습니다.\n",
        "'''\n",
        "\n",
        "''' epoch과 batch\n",
        " 학습 프로세스가 모든 샘플에 대해 한 번 실행되는 것을 1 epoch(‘에포크’라고 읽습니다)라고 합니다.\n",
        " epochs=1000은 각 샘플이 처음부터 끝까지 1,000번 재사용될 때까지 실행을 반복하라는 뜻입니다.\n",
        " batch_size는 샘플을 한 번에 몇 개씩 처리할지를 정하는 부분입니다.\n",
        " batch_size=10은 전체 470개의 샘플을 10개씩 끊어서 집어넣으라는 뜻이 됩니다.\n",
        " batch_size가 너무 크면 학습 속도가 느려지고, 너무 작으면 각 실행 값의 편차가 생겨서 전체 결괏값이 불안정해질\n",
        "수 있습니다.\n",
        "'''"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:66: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:541: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:4432: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/optimizers.py:793: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:1033: The name tf.assign_add is deprecated. Please use tf.compat.v1.assign_add instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:1020: The name tf.assign is deprecated. Please use tf.compat.v1.assign instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3005: The name tf.Session is deprecated. Please use tf.compat.v1.Session instead.\n",
            "\n",
            "Epoch 1/30\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:190: The name tf.get_default_session is deprecated. Please use tf.compat.v1.get_default_session instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:197: The name tf.ConfigProto is deprecated. Please use tf.compat.v1.ConfigProto instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:207: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:216: The name tf.is_variable_initialized is deprecated. Please use tf.compat.v1.is_variable_initialized instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:223: The name tf.variables_initializer is deprecated. Please use tf.compat.v1.variables_initializer instead.\n",
            "\n",
            "470/470 [==============================] - 8s 18ms/step - loss: 0.6614 - acc: 0.3149\n",
            "Epoch 2/30\n",
            "470/470 [==============================] - 0s 385us/step - loss: 0.1488 - acc: 0.8511\n",
            "Epoch 3/30\n",
            "470/470 [==============================] - 0s 340us/step - loss: 0.1488 - acc: 0.8511\n",
            "Epoch 4/30\n",
            "470/470 [==============================] - 0s 336us/step - loss: 0.1488 - acc: 0.8511\n",
            "Epoch 5/30\n",
            "470/470 [==============================] - 0s 340us/step - loss: 0.1488 - acc: 0.8511\n",
            "Epoch 6/30\n",
            "470/470 [==============================] - 0s 374us/step - loss: 0.1487 - acc: 0.8511\n",
            "Epoch 7/30\n",
            "470/470 [==============================] - 0s 336us/step - loss: 0.1487 - acc: 0.8511\n",
            "Epoch 8/30\n",
            "470/470 [==============================] - 0s 379us/step - loss: 0.1487 - acc: 0.8511\n",
            "Epoch 9/30\n",
            "470/470 [==============================] - 0s 335us/step - loss: 0.1487 - acc: 0.8511\n",
            "Epoch 10/30\n",
            "470/470 [==============================] - 0s 339us/step - loss: 0.1486 - acc: 0.8511\n",
            "Epoch 11/30\n",
            "470/470 [==============================] - 0s 335us/step - loss: 0.1498 - acc: 0.8447\n",
            "Epoch 12/30\n",
            "470/470 [==============================] - 0s 350us/step - loss: 0.1486 - acc: 0.8511\n",
            "Epoch 13/30\n",
            "470/470 [==============================] - 0s 340us/step - loss: 0.1485 - acc: 0.8511\n",
            "Epoch 14/30\n",
            "470/470 [==============================] - 0s 404us/step - loss: 0.1483 - acc: 0.8511\n",
            "Epoch 15/30\n",
            "470/470 [==============================] - 0s 358us/step - loss: 0.1485 - acc: 0.8511\n",
            "Epoch 16/30\n",
            "470/470 [==============================] - 0s 339us/step - loss: 0.1490 - acc: 0.8447\n",
            "Epoch 17/30\n",
            "470/470 [==============================] - 0s 339us/step - loss: 0.1479 - acc: 0.8489\n",
            "Epoch 18/30\n",
            "470/470 [==============================] - 0s 321us/step - loss: 0.1482 - acc: 0.8468\n",
            "Epoch 19/30\n",
            "470/470 [==============================] - 0s 358us/step - loss: 0.1476 - acc: 0.8511\n",
            "Epoch 20/30\n",
            "470/470 [==============================] - 0s 414us/step - loss: 0.1480 - acc: 0.8511\n",
            "Epoch 21/30\n",
            "470/470 [==============================] - 0s 374us/step - loss: 0.1475 - acc: 0.8511\n",
            "Epoch 22/30\n",
            "470/470 [==============================] - 0s 387us/step - loss: 0.1469 - acc: 0.8511\n",
            "Epoch 23/30\n",
            "470/470 [==============================] - 0s 387us/step - loss: 0.1466 - acc: 0.8511\n",
            "Epoch 24/30\n",
            "470/470 [==============================] - 0s 407us/step - loss: 0.1475 - acc: 0.8489\n",
            "Epoch 25/30\n",
            "470/470 [==============================] - 0s 348us/step - loss: 0.1470 - acc: 0.8511\n",
            "Epoch 26/30\n",
            "470/470 [==============================] - 0s 369us/step - loss: 0.1466 - acc: 0.8511\n",
            "Epoch 27/30\n",
            "470/470 [==============================] - 0s 336us/step - loss: 0.1472 - acc: 0.8511\n",
            "Epoch 28/30\n",
            "470/470 [==============================] - 0s 376us/step - loss: 0.1471 - acc: 0.8511\n",
            "Epoch 29/30\n",
            "470/470 [==============================] - 0s 334us/step - loss: 0.1470 - acc: 0.8489\n",
            "Epoch 30/30\n",
            "470/470 [==============================] - 0s 371us/step - loss: 0.1461 - acc: 0.8532\n",
            "470/470 [==============================] - 0s 107us/step\n",
            "\n",
            " Accuracy: 0.8511\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\n\\uf0a7 평균 제곱 오차는 수렴하기까지 속도가 많이 걸린다는 단점이 있다\\n\\uf0a7 교차 엔트로피는 출력 값에 로그를 취해서, 오차가 커지면 수렴 속도가 빨라지고 오차가 작아지면 속도가 감소하게\\n끔 만듭니다.\\n\\uf0a7 metrics 함수는 모델이 컴파일될 때 모델 수행 결과를 나타내게끔 설정하는 부분입니다. 정확도를 측정하기 위해 사\\n용되는 테스트 샘플을 학습 과정에서 제외시킴으로써 과적합 문제(over fitting, 특정 데이터에서는 잘 작동하나 다른\\n데이터에서는 잘 작동하지 않는 문제)를 방지하는 기능을 담고 있습니다.\\n'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "26w3QJgwSsez",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "cb029ca8-5d17-40a7-a97a-5780e006671f"
      },
      "source": [
        "# 피마 인디언 데이터 분석 실습 \n",
        "'''\n",
        " 피마 인디언은 1950년대까지만 해도 비만인 사람이 단 한 명도 없는 민족이었습니다.\n",
        " 생존하기 위해 영양분을 체내에 저장하는 뛰어난 능력을 물려받은 인디언들이 미국의 기름진 패스트푸드\n",
        "문화를 만나면서\n",
        " 지금은 전체 부족의 60%가 당뇨, 80%가 비만으로 고통받고 있습니다.\n",
        "\n",
        "샘플 수: 768\n",
        "• 속성: 8\n",
        "- 정보 1 (pregnant): 과거 임신 횟수\n",
        "- 정보 2 (plasma): 포도당 부하 검사 2시간 후 공복 혈당 농\n",
        "도(mm Hg)\n",
        "- 정보 3 (pressure): 확장기 혈압(mm Hg)\n",
        "- 정보 4 (thickness): 삼두근 피부 주름 두께(mm)\n",
        "- 정보 5 (insulin): 혈청 인슐린(2-hour, mu U/ml)\n",
        "- 정보 6 (BMI): 체질량 지수(BMI, weight in kg/(height in\n",
        "m)2)\n",
        "- 정보 7 (pedigree): 당뇨병 가족력\n",
        "- 정보 8 (age): 나이\n",
        "• 클래스: 당뇨(1), 당뇨 아님(0)\n",
        "'''\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense\n",
        "import numpy\n",
        "import tensorflow as tf\n",
        "\n",
        "seed = 0\n",
        "numpy.random.seed(seed) # seed 값 생성\n",
        "tf.set_random_seed(seed)\n",
        "\n",
        "# dataset = numpy.loadtxt(\"data/pima-indians-diabetes.csv\", delimiter=\",\") # 데이터 로드\n",
        "dataset = numpy.loadtxt(\"/content/drive/My Drive/Colab Notebooks/싸이킷런 머신러닝(박태정T)/data/pima-indians-diabetes.csv\", delimiter=\",\")\n",
        "X = dataset[:,0:8]\n",
        "Y = dataset[:,8]\n",
        "\n",
        "model = Sequential() # 모델의 설정\n",
        "model.add(Dense(12, input_dim=8, activation='relu'))\n",
        "model.add(Dense(8, activation='relu'))\n",
        "model.add(Dense(1, activation='sigmoid'))\n",
        "\n",
        "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy']) # 모델 컴파일\n",
        "model.fit(X, Y, epochs=200, batch_size=10) # 모델 실행\n",
        "print(\"\\n Accuracy: %.4f\" % (model.evaluate(X, Y)[1])) # 결과 출력"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3657: The name tf.log is deprecated. Please use tf.math.log instead.\n",
            "\n",
            "WARNING:tensorflow:From /tensorflow-1.15.0/python3.6/tensorflow_core/python/ops/nn_impl.py:183: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
            "Epoch 1/200\n",
            "768/768 [==============================] - 1s 798us/step - loss: 2.4333 - acc: 0.5143\n",
            "Epoch 2/200\n",
            "768/768 [==============================] - 0s 459us/step - loss: 0.9162 - acc: 0.6393\n",
            "Epoch 3/200\n",
            "768/768 [==============================] - 0s 450us/step - loss: 0.7950 - acc: 0.6380\n",
            "Epoch 4/200\n",
            "768/768 [==============================] - 0s 413us/step - loss: 0.7412 - acc: 0.6081\n",
            "Epoch 5/200\n",
            "768/768 [==============================] - 0s 431us/step - loss: 0.6796 - acc: 0.6406\n",
            "Epoch 6/200\n",
            "768/768 [==============================] - 0s 454us/step - loss: 0.6504 - acc: 0.6315\n",
            "Epoch 7/200\n",
            "768/768 [==============================] - 0s 442us/step - loss: 0.6382 - acc: 0.6471\n",
            "Epoch 8/200\n",
            "768/768 [==============================] - 0s 433us/step - loss: 0.6371 - acc: 0.6341\n",
            "Epoch 9/200\n",
            "768/768 [==============================] - 0s 448us/step - loss: 0.6304 - acc: 0.6510\n",
            "Epoch 10/200\n",
            "768/768 [==============================] - 0s 408us/step - loss: 0.6213 - acc: 0.6471\n",
            "Epoch 11/200\n",
            "768/768 [==============================] - 0s 465us/step - loss: 0.6276 - acc: 0.6419\n",
            "Epoch 12/200\n",
            "768/768 [==============================] - 0s 444us/step - loss: 0.6196 - acc: 0.6471\n",
            "Epoch 13/200\n",
            "768/768 [==============================] - 0s 411us/step - loss: 0.6094 - acc: 0.6549\n",
            "Epoch 14/200\n",
            "768/768 [==============================] - 0s 438us/step - loss: 0.6126 - acc: 0.6471\n",
            "Epoch 15/200\n",
            "768/768 [==============================] - 0s 425us/step - loss: 0.6055 - acc: 0.6771\n",
            "Epoch 16/200\n",
            "768/768 [==============================] - 0s 453us/step - loss: 0.6034 - acc: 0.6536\n",
            "Epoch 17/200\n",
            "768/768 [==============================] - 0s 481us/step - loss: 0.5971 - acc: 0.6680\n",
            "Epoch 18/200\n",
            "768/768 [==============================] - 0s 431us/step - loss: 0.5836 - acc: 0.6927\n",
            "Epoch 19/200\n",
            "768/768 [==============================] - 0s 417us/step - loss: 0.5968 - acc: 0.6901\n",
            "Epoch 20/200\n",
            "768/768 [==============================] - 0s 454us/step - loss: 0.5907 - acc: 0.6966\n",
            "Epoch 21/200\n",
            "768/768 [==============================] - 0s 435us/step - loss: 0.5866 - acc: 0.6823\n",
            "Epoch 22/200\n",
            "768/768 [==============================] - 0s 399us/step - loss: 0.5983 - acc: 0.6901\n",
            "Epoch 23/200\n",
            "768/768 [==============================] - 0s 455us/step - loss: 0.5805 - acc: 0.6966\n",
            "Epoch 24/200\n",
            "768/768 [==============================] - 0s 430us/step - loss: 0.6065 - acc: 0.6706\n",
            "Epoch 25/200\n",
            "768/768 [==============================] - 0s 425us/step - loss: 0.5769 - acc: 0.6953\n",
            "Epoch 26/200\n",
            "768/768 [==============================] - 0s 440us/step - loss: 0.5773 - acc: 0.6940\n",
            "Epoch 27/200\n",
            "768/768 [==============================] - 0s 407us/step - loss: 0.5699 - acc: 0.7018\n",
            "Epoch 28/200\n",
            "768/768 [==============================] - 0s 458us/step - loss: 0.5777 - acc: 0.6901\n",
            "Epoch 29/200\n",
            "768/768 [==============================] - 0s 460us/step - loss: 0.5575 - acc: 0.7188\n",
            "Epoch 30/200\n",
            "768/768 [==============================] - 0s 404us/step - loss: 0.5643 - acc: 0.7031\n",
            "Epoch 31/200\n",
            "768/768 [==============================] - 0s 423us/step - loss: 0.5665 - acc: 0.7109\n",
            "Epoch 32/200\n",
            "768/768 [==============================] - 0s 463us/step - loss: 0.5685 - acc: 0.7227\n",
            "Epoch 33/200\n",
            "768/768 [==============================] - 0s 430us/step - loss: 0.5717 - acc: 0.7109\n",
            "Epoch 34/200\n",
            "768/768 [==============================] - 0s 434us/step - loss: 0.5647 - acc: 0.7096\n",
            "Epoch 35/200\n",
            "768/768 [==============================] - 0s 426us/step - loss: 0.5594 - acc: 0.6940\n",
            "Epoch 36/200\n",
            "768/768 [==============================] - 0s 410us/step - loss: 0.5582 - acc: 0.7253\n",
            "Epoch 37/200\n",
            "768/768 [==============================] - 0s 418us/step - loss: 0.5604 - acc: 0.7214\n",
            "Epoch 38/200\n",
            "768/768 [==============================] - 0s 455us/step - loss: 0.5671 - acc: 0.7187\n",
            "Epoch 39/200\n",
            "768/768 [==============================] - 0s 480us/step - loss: 0.5495 - acc: 0.7148\n",
            "Epoch 40/200\n",
            "768/768 [==============================] - 0s 410us/step - loss: 0.5729 - acc: 0.7096\n",
            "Epoch 41/200\n",
            "768/768 [==============================] - 0s 435us/step - loss: 0.5519 - acc: 0.6966\n",
            "Epoch 42/200\n",
            "768/768 [==============================] - 0s 437us/step - loss: 0.5464 - acc: 0.7174\n",
            "Epoch 43/200\n",
            "768/768 [==============================] - 0s 416us/step - loss: 0.5377 - acc: 0.7305\n",
            "Epoch 44/200\n",
            "768/768 [==============================] - 0s 430us/step - loss: 0.5530 - acc: 0.7266\n",
            "Epoch 45/200\n",
            "768/768 [==============================] - 0s 439us/step - loss: 0.5506 - acc: 0.7331\n",
            "Epoch 46/200\n",
            "768/768 [==============================] - 0s 425us/step - loss: 0.5391 - acc: 0.7305\n",
            "Epoch 47/200\n",
            "768/768 [==============================] - 0s 425us/step - loss: 0.5693 - acc: 0.7057\n",
            "Epoch 48/200\n",
            "768/768 [==============================] - 0s 454us/step - loss: 0.5494 - acc: 0.7122\n",
            "Epoch 49/200\n",
            "768/768 [==============================] - 0s 444us/step - loss: 0.5404 - acc: 0.7305\n",
            "Epoch 50/200\n",
            "768/768 [==============================] - 0s 536us/step - loss: 0.5367 - acc: 0.7292\n",
            "Epoch 51/200\n",
            "768/768 [==============================] - 0s 460us/step - loss: 0.5374 - acc: 0.7357\n",
            "Epoch 52/200\n",
            "768/768 [==============================] - 0s 443us/step - loss: 0.5306 - acc: 0.7292\n",
            "Epoch 53/200\n",
            "768/768 [==============================] - 0s 441us/step - loss: 0.5341 - acc: 0.7279\n",
            "Epoch 54/200\n",
            "768/768 [==============================] - 0s 412us/step - loss: 0.5324 - acc: 0.7279\n",
            "Epoch 55/200\n",
            "768/768 [==============================] - 0s 407us/step - loss: 0.5354 - acc: 0.7266\n",
            "Epoch 56/200\n",
            "768/768 [==============================] - 0s 496us/step - loss: 0.5338 - acc: 0.7448\n",
            "Epoch 57/200\n",
            "768/768 [==============================] - 0s 437us/step - loss: 0.5343 - acc: 0.7344\n",
            "Epoch 58/200\n",
            "768/768 [==============================] - 0s 431us/step - loss: 0.5274 - acc: 0.7461\n",
            "Epoch 59/200\n",
            "768/768 [==============================] - 0s 431us/step - loss: 0.5420 - acc: 0.7240\n",
            "Epoch 60/200\n",
            "768/768 [==============================] - 0s 429us/step - loss: 0.5552 - acc: 0.7305\n",
            "Epoch 61/200\n",
            "768/768 [==============================] - 0s 423us/step - loss: 0.5396 - acc: 0.7227\n",
            "Epoch 62/200\n",
            "768/768 [==============================] - 0s 427us/step - loss: 0.5311 - acc: 0.7383\n",
            "Epoch 63/200\n",
            "768/768 [==============================] - 0s 417us/step - loss: 0.5276 - acc: 0.7409\n",
            "Epoch 64/200\n",
            "768/768 [==============================] - 0s 435us/step - loss: 0.5313 - acc: 0.7305\n",
            "Epoch 65/200\n",
            "768/768 [==============================] - 0s 416us/step - loss: 0.5305 - acc: 0.7279\n",
            "Epoch 66/200\n",
            "768/768 [==============================] - 0s 436us/step - loss: 0.5315 - acc: 0.7344\n",
            "Epoch 67/200\n",
            "768/768 [==============================] - 0s 430us/step - loss: 0.5226 - acc: 0.7344\n",
            "Epoch 68/200\n",
            "768/768 [==============================] - 0s 444us/step - loss: 0.5258 - acc: 0.7448\n",
            "Epoch 69/200\n",
            "768/768 [==============================] - 0s 447us/step - loss: 0.5266 - acc: 0.7383\n",
            "Epoch 70/200\n",
            "768/768 [==============================] - 0s 480us/step - loss: 0.5218 - acc: 0.7383\n",
            "Epoch 71/200\n",
            "768/768 [==============================] - 0s 434us/step - loss: 0.5375 - acc: 0.7279\n",
            "Epoch 72/200\n",
            "768/768 [==============================] - 0s 448us/step - loss: 0.5213 - acc: 0.7370\n",
            "Epoch 73/200\n",
            "768/768 [==============================] - 0s 416us/step - loss: 0.5267 - acc: 0.7253\n",
            "Epoch 74/200\n",
            "768/768 [==============================] - 0s 410us/step - loss: 0.5268 - acc: 0.7396\n",
            "Epoch 75/200\n",
            "768/768 [==============================] - 0s 425us/step - loss: 0.5189 - acc: 0.7409\n",
            "Epoch 76/200\n",
            "768/768 [==============================] - 0s 411us/step - loss: 0.5213 - acc: 0.7461\n",
            "Epoch 77/200\n",
            "768/768 [==============================] - 0s 430us/step - loss: 0.5174 - acc: 0.7370\n",
            "Epoch 78/200\n",
            "768/768 [==============================] - 0s 436us/step - loss: 0.5135 - acc: 0.7396\n",
            "Epoch 79/200\n",
            "768/768 [==============================] - 0s 423us/step - loss: 0.5214 - acc: 0.7435\n",
            "Epoch 80/200\n",
            "768/768 [==============================] - 0s 439us/step - loss: 0.5120 - acc: 0.7474\n",
            "Epoch 81/200\n",
            "768/768 [==============================] - 0s 436us/step - loss: 0.5158 - acc: 0.7383\n",
            "Epoch 82/200\n",
            "768/768 [==============================] - 0s 423us/step - loss: 0.5065 - acc: 0.7422\n",
            "Epoch 83/200\n",
            "768/768 [==============================] - 0s 419us/step - loss: 0.5232 - acc: 0.7370\n",
            "Epoch 84/200\n",
            "768/768 [==============================] - 0s 454us/step - loss: 0.5051 - acc: 0.7396\n",
            "Epoch 85/200\n",
            "768/768 [==============================] - 0s 438us/step - loss: 0.5096 - acc: 0.7422\n",
            "Epoch 86/200\n",
            "768/768 [==============================] - 0s 405us/step - loss: 0.5117 - acc: 0.7500\n",
            "Epoch 87/200\n",
            "768/768 [==============================] - 0s 439us/step - loss: 0.5112 - acc: 0.7500\n",
            "Epoch 88/200\n",
            "768/768 [==============================] - 0s 444us/step - loss: 0.5100 - acc: 0.7474\n",
            "Epoch 89/200\n",
            "768/768 [==============================] - 0s 413us/step - loss: 0.5045 - acc: 0.7591\n",
            "Epoch 90/200\n",
            "768/768 [==============================] - 0s 441us/step - loss: 0.5066 - acc: 0.7435\n",
            "Epoch 91/200\n",
            "768/768 [==============================] - 0s 426us/step - loss: 0.5026 - acc: 0.7617\n",
            "Epoch 92/200\n",
            "768/768 [==============================] - 0s 407us/step - loss: 0.5105 - acc: 0.7292\n",
            "Epoch 93/200\n",
            "768/768 [==============================] - 0s 443us/step - loss: 0.5001 - acc: 0.7552\n",
            "Epoch 94/200\n",
            "768/768 [==============================] - 0s 430us/step - loss: 0.5003 - acc: 0.7487\n",
            "Epoch 95/200\n",
            "768/768 [==============================] - 0s 434us/step - loss: 0.4992 - acc: 0.7409\n",
            "Epoch 96/200\n",
            "768/768 [==============================] - 0s 441us/step - loss: 0.5014 - acc: 0.7500\n",
            "Epoch 97/200\n",
            "768/768 [==============================] - 0s 414us/step - loss: 0.4977 - acc: 0.7526\n",
            "Epoch 98/200\n",
            "768/768 [==============================] - 0s 464us/step - loss: 0.5006 - acc: 0.7500\n",
            "Epoch 99/200\n",
            "768/768 [==============================] - 0s 431us/step - loss: 0.4946 - acc: 0.7656\n",
            "Epoch 100/200\n",
            "768/768 [==============================] - 0s 407us/step - loss: 0.4945 - acc: 0.7487\n",
            "Epoch 101/200\n",
            "768/768 [==============================] - 0s 451us/step - loss: 0.4986 - acc: 0.7604\n",
            "Epoch 102/200\n",
            "768/768 [==============================] - 0s 428us/step - loss: 0.5040 - acc: 0.7396\n",
            "Epoch 103/200\n",
            "768/768 [==============================] - 0s 412us/step - loss: 0.4991 - acc: 0.7617\n",
            "Epoch 104/200\n",
            "768/768 [==============================] - 0s 403us/step - loss: 0.4932 - acc: 0.7591\n",
            "Epoch 105/200\n",
            "768/768 [==============================] - 0s 427us/step - loss: 0.4961 - acc: 0.7695\n",
            "Epoch 106/200\n",
            "768/768 [==============================] - 0s 443us/step - loss: 0.5055 - acc: 0.7474\n",
            "Epoch 107/200\n",
            "768/768 [==============================] - 0s 419us/step - loss: 0.4927 - acc: 0.7630\n",
            "Epoch 108/200\n",
            "768/768 [==============================] - 0s 436us/step - loss: 0.4953 - acc: 0.7526\n",
            "Epoch 109/200\n",
            "768/768 [==============================] - 0s 454us/step - loss: 0.4901 - acc: 0.7617\n",
            "Epoch 110/200\n",
            "768/768 [==============================] - 0s 413us/step - loss: 0.4950 - acc: 0.7669\n",
            "Epoch 111/200\n",
            "768/768 [==============================] - 0s 424us/step - loss: 0.4981 - acc: 0.7656\n",
            "Epoch 112/200\n",
            "768/768 [==============================] - 0s 429us/step - loss: 0.4866 - acc: 0.7695\n",
            "Epoch 113/200\n",
            "768/768 [==============================] - 0s 423us/step - loss: 0.5067 - acc: 0.7448\n",
            "Epoch 114/200\n",
            "768/768 [==============================] - 0s 432us/step - loss: 0.4906 - acc: 0.7695\n",
            "Epoch 115/200\n",
            "768/768 [==============================] - 0s 433us/step - loss: 0.4869 - acc: 0.7617\n",
            "Epoch 116/200\n",
            "768/768 [==============================] - 0s 430us/step - loss: 0.5117 - acc: 0.7409\n",
            "Epoch 117/200\n",
            "768/768 [==============================] - 0s 432us/step - loss: 0.4872 - acc: 0.7682\n",
            "Epoch 118/200\n",
            "768/768 [==============================] - 0s 451us/step - loss: 0.4938 - acc: 0.7487\n",
            "Epoch 119/200\n",
            "768/768 [==============================] - 0s 435us/step - loss: 0.4841 - acc: 0.7604\n",
            "Epoch 120/200\n",
            "768/768 [==============================] - 0s 442us/step - loss: 0.4886 - acc: 0.7721\n",
            "Epoch 121/200\n",
            "768/768 [==============================] - 0s 448us/step - loss: 0.4873 - acc: 0.7630\n",
            "Epoch 122/200\n",
            "768/768 [==============================] - 0s 420us/step - loss: 0.4862 - acc: 0.7695\n",
            "Epoch 123/200\n",
            "768/768 [==============================] - 0s 423us/step - loss: 0.4824 - acc: 0.7721\n",
            "Epoch 124/200\n",
            "768/768 [==============================] - 0s 446us/step - loss: 0.4922 - acc: 0.7526\n",
            "Epoch 125/200\n",
            "768/768 [==============================] - 0s 415us/step - loss: 0.4864 - acc: 0.7591\n",
            "Epoch 126/200\n",
            "768/768 [==============================] - 0s 420us/step - loss: 0.4813 - acc: 0.7630\n",
            "Epoch 127/200\n",
            "768/768 [==============================] - 0s 440us/step - loss: 0.4928 - acc: 0.7604\n",
            "Epoch 128/200\n",
            "768/768 [==============================] - 0s 441us/step - loss: 0.4883 - acc: 0.7695\n",
            "Epoch 129/200\n",
            "768/768 [==============================] - 0s 437us/step - loss: 0.4874 - acc: 0.7630\n",
            "Epoch 130/200\n",
            "768/768 [==============================] - 0s 437us/step - loss: 0.4868 - acc: 0.7591\n",
            "Epoch 131/200\n",
            "768/768 [==============================] - 0s 407us/step - loss: 0.4831 - acc: 0.7565\n",
            "Epoch 132/200\n",
            "768/768 [==============================] - 0s 449us/step - loss: 0.4791 - acc: 0.7643\n",
            "Epoch 133/200\n",
            "768/768 [==============================] - 0s 451us/step - loss: 0.4822 - acc: 0.7643\n",
            "Epoch 134/200\n",
            "768/768 [==============================] - 0s 436us/step - loss: 0.4762 - acc: 0.7669\n",
            "Epoch 135/200\n",
            "768/768 [==============================] - 0s 425us/step - loss: 0.4883 - acc: 0.7682\n",
            "Epoch 136/200\n",
            "768/768 [==============================] - 0s 440us/step - loss: 0.4776 - acc: 0.7617\n",
            "Epoch 137/200\n",
            "768/768 [==============================] - 0s 400us/step - loss: 0.4817 - acc: 0.7617\n",
            "Epoch 138/200\n",
            "768/768 [==============================] - 0s 430us/step - loss: 0.4879 - acc: 0.7604\n",
            "Epoch 139/200\n",
            "768/768 [==============================] - 0s 419us/step - loss: 0.4877 - acc: 0.7708\n",
            "Epoch 140/200\n",
            "768/768 [==============================] - 0s 467us/step - loss: 0.4881 - acc: 0.7565\n",
            "Epoch 141/200\n",
            "768/768 [==============================] - 0s 427us/step - loss: 0.4692 - acc: 0.7656\n",
            "Epoch 142/200\n",
            "768/768 [==============================] - 0s 419us/step - loss: 0.4862 - acc: 0.7630\n",
            "Epoch 143/200\n",
            "768/768 [==============================] - 0s 430us/step - loss: 0.4770 - acc: 0.7669\n",
            "Epoch 144/200\n",
            "768/768 [==============================] - 0s 420us/step - loss: 0.4743 - acc: 0.7812\n",
            "Epoch 145/200\n",
            "768/768 [==============================] - 0s 425us/step - loss: 0.4784 - acc: 0.7669\n",
            "Epoch 146/200\n",
            "768/768 [==============================] - 0s 453us/step - loss: 0.4787 - acc: 0.7630\n",
            "Epoch 147/200\n",
            "768/768 [==============================] - 0s 432us/step - loss: 0.4772 - acc: 0.7708\n",
            "Epoch 148/200\n",
            "768/768 [==============================] - 0s 434us/step - loss: 0.4859 - acc: 0.7734\n",
            "Epoch 149/200\n",
            "768/768 [==============================] - 0s 437us/step - loss: 0.4733 - acc: 0.7734\n",
            "Epoch 150/200\n",
            "768/768 [==============================] - 0s 434us/step - loss: 0.4739 - acc: 0.7773\n",
            "Epoch 151/200\n",
            "768/768 [==============================] - 0s 415us/step - loss: 0.4861 - acc: 0.7812\n",
            "Epoch 152/200\n",
            "768/768 [==============================] - 0s 448us/step - loss: 0.4861 - acc: 0.7799\n",
            "Epoch 153/200\n",
            "768/768 [==============================] - 0s 423us/step - loss: 0.4782 - acc: 0.7682\n",
            "Epoch 154/200\n",
            "768/768 [==============================] - 0s 401us/step - loss: 0.4719 - acc: 0.7630\n",
            "Epoch 155/200\n",
            "768/768 [==============================] - 0s 438us/step - loss: 0.4737 - acc: 0.7799\n",
            "Epoch 156/200\n",
            "768/768 [==============================] - 0s 448us/step - loss: 0.4776 - acc: 0.7721\n",
            "Epoch 157/200\n",
            "768/768 [==============================] - 0s 465us/step - loss: 0.4795 - acc: 0.7721\n",
            "Epoch 158/200\n",
            "768/768 [==============================] - 0s 449us/step - loss: 0.4700 - acc: 0.7812\n",
            "Epoch 159/200\n",
            "768/768 [==============================] - 0s 401us/step - loss: 0.4728 - acc: 0.7669\n",
            "Epoch 160/200\n",
            "768/768 [==============================] - 0s 426us/step - loss: 0.4727 - acc: 0.7747\n",
            "Epoch 161/200\n",
            "768/768 [==============================] - 0s 430us/step - loss: 0.4761 - acc: 0.7656\n",
            "Epoch 162/200\n",
            "768/768 [==============================] - 0s 409us/step - loss: 0.4694 - acc: 0.7812\n",
            "Epoch 163/200\n",
            "768/768 [==============================] - 0s 413us/step - loss: 0.4670 - acc: 0.7721\n",
            "Epoch 164/200\n",
            "768/768 [==============================] - 0s 466us/step - loss: 0.4700 - acc: 0.7721\n",
            "Epoch 165/200\n",
            "768/768 [==============================] - 0s 461us/step - loss: 0.4791 - acc: 0.7604\n",
            "Epoch 166/200\n",
            "768/768 [==============================] - 0s 407us/step - loss: 0.4718 - acc: 0.7708\n",
            "Epoch 167/200\n",
            "768/768 [==============================] - 0s 447us/step - loss: 0.4770 - acc: 0.7604\n",
            "Epoch 168/200\n",
            "768/768 [==============================] - 0s 416us/step - loss: 0.4669 - acc: 0.7682\n",
            "Epoch 169/200\n",
            "768/768 [==============================] - 0s 428us/step - loss: 0.4743 - acc: 0.7669\n",
            "Epoch 170/200\n",
            "768/768 [==============================] - 0s 476us/step - loss: 0.4705 - acc: 0.7669\n",
            "Epoch 171/200\n",
            "768/768 [==============================] - 0s 409us/step - loss: 0.4704 - acc: 0.7721\n",
            "Epoch 172/200\n",
            "768/768 [==============================] - 0s 444us/step - loss: 0.4704 - acc: 0.7708\n",
            "Epoch 173/200\n",
            "768/768 [==============================] - 0s 467us/step - loss: 0.4730 - acc: 0.7760\n",
            "Epoch 174/200\n",
            "768/768 [==============================] - 0s 408us/step - loss: 0.4679 - acc: 0.7760\n",
            "Epoch 175/200\n",
            "768/768 [==============================] - 0s 448us/step - loss: 0.4715 - acc: 0.7669\n",
            "Epoch 176/200\n",
            "768/768 [==============================] - 0s 440us/step - loss: 0.4663 - acc: 0.7708\n",
            "Epoch 177/200\n",
            "768/768 [==============================] - 0s 431us/step - loss: 0.4636 - acc: 0.7747\n",
            "Epoch 178/200\n",
            "768/768 [==============================] - 0s 409us/step - loss: 0.4766 - acc: 0.7734\n",
            "Epoch 179/200\n",
            "768/768 [==============================] - 0s 425us/step - loss: 0.4694 - acc: 0.7891\n",
            "Epoch 180/200\n",
            "768/768 [==============================] - 0s 446us/step - loss: 0.4637 - acc: 0.7747\n",
            "Epoch 181/200\n",
            "768/768 [==============================] - 0s 395us/step - loss: 0.4742 - acc: 0.7839\n",
            "Epoch 182/200\n",
            "768/768 [==============================] - 0s 411us/step - loss: 0.4697 - acc: 0.7747\n",
            "Epoch 183/200\n",
            "768/768 [==============================] - 0s 449us/step - loss: 0.4700 - acc: 0.7747\n",
            "Epoch 184/200\n",
            "768/768 [==============================] - 0s 420us/step - loss: 0.4645 - acc: 0.7826\n",
            "Epoch 185/200\n",
            "768/768 [==============================] - 0s 443us/step - loss: 0.4591 - acc: 0.7878\n",
            "Epoch 186/200\n",
            "768/768 [==============================] - 0s 445us/step - loss: 0.4758 - acc: 0.7656\n",
            "Epoch 187/200\n",
            "768/768 [==============================] - 0s 404us/step - loss: 0.4716 - acc: 0.7695\n",
            "Epoch 188/200\n",
            "768/768 [==============================] - 0s 446us/step - loss: 0.4701 - acc: 0.7656\n",
            "Epoch 189/200\n",
            "768/768 [==============================] - 0s 423us/step - loss: 0.4690 - acc: 0.7852\n",
            "Epoch 190/200\n",
            "768/768 [==============================] - 0s 434us/step - loss: 0.4662 - acc: 0.7734\n",
            "Epoch 191/200\n",
            "768/768 [==============================] - 0s 406us/step - loss: 0.4577 - acc: 0.7760\n",
            "Epoch 192/200\n",
            "768/768 [==============================] - 0s 432us/step - loss: 0.4581 - acc: 0.7878\n",
            "Epoch 193/200\n",
            "768/768 [==============================] - 0s 431us/step - loss: 0.4555 - acc: 0.7812\n",
            "Epoch 194/200\n",
            "768/768 [==============================] - 0s 412us/step - loss: 0.4645 - acc: 0.7799\n",
            "Epoch 195/200\n",
            "768/768 [==============================] - 0s 437us/step - loss: 0.4719 - acc: 0.7721\n",
            "Epoch 196/200\n",
            "768/768 [==============================] - 0s 407us/step - loss: 0.4675 - acc: 0.7799\n",
            "Epoch 197/200\n",
            "768/768 [==============================] - 0s 475us/step - loss: 0.4637 - acc: 0.7852\n",
            "Epoch 198/200\n",
            "768/768 [==============================] - 0s 442us/step - loss: 0.4625 - acc: 0.7669\n",
            "Epoch 199/200\n",
            "768/768 [==============================] - 0s 450us/step - loss: 0.4728 - acc: 0.7747\n",
            "Epoch 200/200\n",
            "768/768 [==============================] - 0s 407us/step - loss: 0.4654 - acc: 0.7747\n",
            "768/768 [==============================] - 0s 110us/step\n",
            "\n",
            " Accuracy: 0.7904\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BOJps0uhaCci",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 856
        },
        "outputId": "3ecdbbc1-f349-4f1c-eba3-e797a1b47aa4"
      },
      "source": [
        "'''\n",
        "다중 분류 분석 실습 → softmax\n",
        " 아이리스 데이터의 샘플, 속성, 클래스 구분\n",
        "\n",
        "샘플 수: 150\n",
        "• 속성 수: 4\n",
        "- 정보 1: 꽃받침 길이 (sepal length, 단위: cm)\n",
        "- 정보 2: 꽃받침 넓이 (sepal width, 단위: cm)\n",
        "- 정보 3: 꽃잎 길이 (petal length, 단위: cm)\n",
        "- 정보 4: 꽃잎 넓이 (petal width, 단위: cm)\n",
        "• 클래스: Iris-setosa, Iris-versicolor, Iris-virginica\n",
        "'''\n",
        "\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "'''\n",
        "df = pd.read_csv('data/iris.csv', \n",
        "                 names = [\"sepal_length\", \"sepal_width\", \"petal_length\", \"petal_width\", \"species\"])\n",
        "'''\n",
        "df = pd.read_csv('/content/drive/My Drive/Colab Notebooks/싸이킷런 머신러닝(박태정T)/data/iris.csv', \n",
        "                 names = [\"sepal_length\", \"sepal_width\", \"petal_length\", \"petal_width\", \"species\"])\n",
        "print(df.head())\n",
        "\n",
        "sns.pairplot(df, hue='species') #속성별 연관성 파악\n",
        "plt.show()\n",
        "\n",
        "dataset = df.values\n",
        "X = dataset[:,0:4].astype(float)\n",
        "Y_obj = dataset[:,4]\n",
        "\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "e = LabelEncoder() # array(['Iris-setosa', 'Iris-versicolor', 'Iris-virginica'])가 array([1,2,3])로 변환\n",
        "e.fit(Y_obj)\n",
        "Y = e.transform(Y_obj)\n",
        "\n",
        "from keras.utils import np_utils\n",
        "# array([1,2,3])가 다시 array([[1., 0., 0.], [0., 1., 0.], [0., 0., 1.]])로 원-핫 인코딩(one-hot-encoding) 변환\n",
        "Y_encoded = np_utils.to_categorical(Y)"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "   sepal_length  sepal_width  petal_length  petal_width      species\n",
            "0           5.1          3.5           1.4          0.2  Iris-setosa\n",
            "1           4.9          3.0           1.4          0.2  Iris-setosa\n",
            "2           4.7          3.2           1.3          0.2  Iris-setosa\n",
            "3           4.6          3.1           1.5          0.2  Iris-setosa\n",
            "4           5.0          3.6           1.4          0.2  Iris-setosa\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAzsAAALbCAYAAADdHJ4ZAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nGGNMVrLBjjHGGGOMMSYr/S8IGXs86uHSkgAAAABJRU5ErkJggg==\n",
            "text/plain": [
              "<Figure size 823.25x720 with 20 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g1wp_5i9bMSQ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "b5e93312-ca14-4af9-8a83-470ded717c3a"
      },
      "source": [
        "'''(iris 이어서)\n",
        " 소프트맥스(softmax) - 총합이 1인 형태로 바꿔서 계산해 주는 함수\n",
        "합계가 1인 형태로 변환하면 큰 값이 두드러지게 나타나고 작은 값은 더 작아집니다.\n",
        "이 값이 교차 엔트로피를 지나 [1., 0., 0.]으로 변화하게 되면 우리가 원하는 원-핫 인코딩 값,\n",
        "즉 하나만 1이고 나머지는 모두 0인 형태로 전환시킬 수 있습니다.\n",
        "'''\n",
        "from keras.models import Sequential\n",
        "from keras.layers.core import Dense\n",
        "import numpy\n",
        "import tensorflow as tf\n",
        "\n",
        "seed = 0\n",
        "numpy.random.seed(seed) # seed 값 설정\n",
        "tf.set_random_seed(seed)\n",
        "\n",
        "model = Sequential() # 모델의 설정\n",
        "model.add(Dense(16, input_dim=4, activation='relu'))\n",
        "#최종 출력 값이 3개 중 하나여야 하므로 출력층에 해당하는 Dense의 노드 수를 3으로 설정, 소프트맥스 함수 사요요 \n",
        "model.add(Dense(3, activation='softmax'))\n",
        "# 모델 컴파일(다중 분류에 적절한 오차 함수인 categorical_crossentropy를 사용, 최적화 함수로 adam 사용)\n",
        "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "\n",
        "# 모델 실행(한 번에 입력되는 값은 1개, 전체 샘플이 50회 반복될 때까지 실험을 진행\n",
        "model.fit(X, Y_encoded, epochs=50, batch_size=1)\n",
        "print(\"\\n Accuracy: %.4f\" % (model.evaluate(X, Y_encoded)[1])) # 결과 출력"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/50\n",
            "150/150 [==============================] - 1s 6ms/step - loss: 1.6259 - acc: 0.3267\n",
            "Epoch 2/50\n",
            "150/150 [==============================] - 1s 4ms/step - loss: 1.1405 - acc: 0.4867\n",
            "Epoch 3/50\n",
            "150/150 [==============================] - 1s 4ms/step - loss: 0.9412 - acc: 0.4600\n",
            "Epoch 4/50\n",
            "150/150 [==============================] - 1s 4ms/step - loss: 0.7898 - acc: 0.7467\n",
            "Epoch 5/50\n",
            "150/150 [==============================] - 1s 4ms/step - loss: 0.6669 - acc: 0.7267\n",
            "Epoch 6/50\n",
            "150/150 [==============================] - 1s 4ms/step - loss: 0.5636 - acc: 0.8800\n",
            "Epoch 7/50\n",
            "150/150 [==============================] - 1s 4ms/step - loss: 0.5080 - acc: 0.8267\n",
            "Epoch 8/50\n",
            "150/150 [==============================] - 1s 4ms/step - loss: 0.4558 - acc: 0.9333\n",
            "Epoch 9/50\n",
            "150/150 [==============================] - 1s 4ms/step - loss: 0.4287 - acc: 0.8867\n",
            "Epoch 10/50\n",
            "150/150 [==============================] - 1s 4ms/step - loss: 0.4070 - acc: 0.8867\n",
            "Epoch 11/50\n",
            "150/150 [==============================] - 1s 4ms/step - loss: 0.3852 - acc: 0.9333\n",
            "Epoch 12/50\n",
            "150/150 [==============================] - 1s 4ms/step - loss: 0.3677 - acc: 0.9467\n",
            "Epoch 13/50\n",
            "150/150 [==============================] - 1s 4ms/step - loss: 0.3505 - acc: 0.9467\n",
            "Epoch 14/50\n",
            "150/150 [==============================] - 1s 4ms/step - loss: 0.3368 - acc: 0.9067\n",
            "Epoch 15/50\n",
            "150/150 [==============================] - 1s 4ms/step - loss: 0.3265 - acc: 0.9333\n",
            "Epoch 16/50\n",
            "150/150 [==============================] - 1s 4ms/step - loss: 0.3136 - acc: 0.9533\n",
            "Epoch 17/50\n",
            "150/150 [==============================] - 1s 4ms/step - loss: 0.3060 - acc: 0.9533\n",
            "Epoch 18/50\n",
            "150/150 [==============================] - 1s 4ms/step - loss: 0.2965 - acc: 0.9667\n",
            "Epoch 19/50\n",
            "150/150 [==============================] - 1s 4ms/step - loss: 0.2854 - acc: 0.9333\n",
            "Epoch 20/50\n",
            "150/150 [==============================] - 1s 4ms/step - loss: 0.2761 - acc: 0.9600\n",
            "Epoch 21/50\n",
            "150/150 [==============================] - 1s 4ms/step - loss: 0.2678 - acc: 0.9733\n",
            "Epoch 22/50\n",
            "150/150 [==============================] - 1s 4ms/step - loss: 0.2609 - acc: 0.9667\n",
            "Epoch 23/50\n",
            "150/150 [==============================] - 1s 4ms/step - loss: 0.2529 - acc: 0.9467\n",
            "Epoch 24/50\n",
            "150/150 [==============================] - 1s 4ms/step - loss: 0.2437 - acc: 0.9600\n",
            "Epoch 25/50\n",
            "150/150 [==============================] - 1s 4ms/step - loss: 0.2361 - acc: 0.9733\n",
            "Epoch 26/50\n",
            "150/150 [==============================] - 1s 4ms/step - loss: 0.2340 - acc: 0.9667\n",
            "Epoch 27/50\n",
            "150/150 [==============================] - 1s 4ms/step - loss: 0.2194 - acc: 0.9867\n",
            "Epoch 28/50\n",
            "150/150 [==============================] - 1s 4ms/step - loss: 0.2207 - acc: 0.9467\n",
            "Epoch 29/50\n",
            "150/150 [==============================] - 1s 4ms/step - loss: 0.2142 - acc: 0.9667\n",
            "Epoch 30/50\n",
            "150/150 [==============================] - 1s 4ms/step - loss: 0.2000 - acc: 0.9600\n",
            "Epoch 31/50\n",
            "150/150 [==============================] - 1s 4ms/step - loss: 0.2000 - acc: 0.9733\n",
            "Epoch 32/50\n",
            "150/150 [==============================] - 1s 4ms/step - loss: 0.1971 - acc: 0.9667\n",
            "Epoch 33/50\n",
            "150/150 [==============================] - 1s 4ms/step - loss: 0.1882 - acc: 0.9733\n",
            "Epoch 34/50\n",
            "150/150 [==============================] - 1s 4ms/step - loss: 0.1807 - acc: 0.9667\n",
            "Epoch 35/50\n",
            "150/150 [==============================] - 1s 4ms/step - loss: 0.1852 - acc: 0.9667\n",
            "Epoch 36/50\n",
            "150/150 [==============================] - 1s 4ms/step - loss: 0.1743 - acc: 0.9800\n",
            "Epoch 37/50\n",
            "150/150 [==============================] - 1s 4ms/step - loss: 0.1685 - acc: 0.9733\n",
            "Epoch 38/50\n",
            "150/150 [==============================] - 1s 4ms/step - loss: 0.1646 - acc: 0.9733\n",
            "Epoch 39/50\n",
            "150/150 [==============================] - 1s 4ms/step - loss: 0.1622 - acc: 0.9733\n",
            "Epoch 40/50\n",
            "150/150 [==============================] - 1s 4ms/step - loss: 0.1561 - acc: 0.9667\n",
            "Epoch 41/50\n",
            "150/150 [==============================] - 1s 4ms/step - loss: 0.1565 - acc: 0.9667\n",
            "Epoch 42/50\n",
            "150/150 [==============================] - 1s 4ms/step - loss: 0.1526 - acc: 0.9667\n",
            "Epoch 43/50\n",
            "150/150 [==============================] - 1s 4ms/step - loss: 0.1435 - acc: 0.9667\n",
            "Epoch 44/50\n",
            "150/150 [==============================] - 1s 4ms/step - loss: 0.1443 - acc: 0.9533\n",
            "Epoch 45/50\n",
            "150/150 [==============================] - 1s 4ms/step - loss: 0.1419 - acc: 0.9800\n",
            "Epoch 46/50\n",
            "150/150 [==============================] - 1s 4ms/step - loss: 0.1450 - acc: 0.9600\n",
            "Epoch 47/50\n",
            "150/150 [==============================] - 1s 4ms/step - loss: 0.1350 - acc: 0.9667\n",
            "Epoch 48/50\n",
            "150/150 [==============================] - 1s 4ms/step - loss: 0.1351 - acc: 0.9800\n",
            "Epoch 49/50\n",
            "150/150 [==============================] - 1s 4ms/step - loss: 0.1332 - acc: 0.9667\n",
            "Epoch 50/50\n",
            "150/150 [==============================] - 1s 4ms/step - loss: 0.1284 - acc: 0.9667\n",
            "150/150 [==============================] - 0s 491us/step\n",
            "\n",
            " Accuracy: 0.9733\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "55mbvRWbbzx6",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "8a9a4ecf-99d1-4e9f-90b2-bf9875f1fa4d"
      },
      "source": [
        "# 초음파 광물 예측 분석 실습\n",
        "'''\n",
        "광물 분류 약 60가지 (p.99 참고)\n",
        "'''\n",
        "\n",
        "from keras.models import Sequential\n",
        "from keras.layers.core import Dense\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "import pandas as pd\n",
        "import numpy\n",
        "import tensorflow as tf\n",
        "\n",
        "seed = 0\n",
        "numpy.random.seed(seed) # seed 값 설정\n",
        "tf.set_random_seed(seed)\n",
        "df = pd.read_csv('/content/drive/My Drive/Colab Notebooks/싸이킷런 머신러닝(박태정T)/data/sonar.csv'\n",
        "                 ,header=None) # 데이터 입력\n",
        "dataset = df.values\n",
        "X = dataset[:,0:60]\n",
        "Y_obj = dataset[:,60]\n",
        "e = LabelEncoder()\n",
        "e.fit(Y_obj)\n",
        "Y = e.transform(Y_obj) # 문자열 변환\n",
        "model = Sequential() # 모델 설정\n",
        "model.add(Dense(24, input_dim=60, activation='relu'))\n",
        "model.add(Dense(10, activation='relu'))\n",
        "model.add(Dense(1, activation='sigmoid'))\n",
        "model.compile(loss='mean_squared_error', optimizer='adam', metrics=['accuracy']) # 모델 컴파일\n",
        "model.fit(X, Y, epochs=200, batch_size=5) # 모델 실행\n",
        "print(\"\\n Accuracy: %.4f\" % (model.evaluate(X, Y)[1])) # 결과 출력\n",
        "\n",
        "# 매우 높은 정확도 출력\n",
        "# 과적합 방지 원리를 배우고 실습 할 것 "
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/200\n",
            "208/208 [==============================] - 0s 2ms/step - loss: 0.2532 - acc: 0.4471\n",
            "Epoch 2/200\n",
            "208/208 [==============================] - 0s 729us/step - loss: 0.2457 - acc: 0.5385\n",
            "Epoch 3/200\n",
            "208/208 [==============================] - 0s 765us/step - loss: 0.2428 - acc: 0.5385\n",
            "Epoch 4/200\n",
            "208/208 [==============================] - 0s 797us/step - loss: 0.2402 - acc: 0.5385\n",
            "Epoch 5/200\n",
            "208/208 [==============================] - 0s 812us/step - loss: 0.2349 - acc: 0.5433\n",
            "Epoch 6/200\n",
            "208/208 [==============================] - 0s 843us/step - loss: 0.2285 - acc: 0.5529\n",
            "Epoch 7/200\n",
            "208/208 [==============================] - 0s 874us/step - loss: 0.2207 - acc: 0.6250\n",
            "Epoch 8/200\n",
            "208/208 [==============================] - 0s 823us/step - loss: 0.2162 - acc: 0.6298\n",
            "Epoch 9/200\n",
            "208/208 [==============================] - 0s 735us/step - loss: 0.2021 - acc: 0.7067\n",
            "Epoch 10/200\n",
            "208/208 [==============================] - 0s 760us/step - loss: 0.1909 - acc: 0.7308\n",
            "Epoch 11/200\n",
            "208/208 [==============================] - 0s 922us/step - loss: 0.1826 - acc: 0.7500\n",
            "Epoch 12/200\n",
            "208/208 [==============================] - 0s 754us/step - loss: 0.1719 - acc: 0.7644\n",
            "Epoch 13/200\n",
            "208/208 [==============================] - 0s 769us/step - loss: 0.1670 - acc: 0.8029\n",
            "Epoch 14/200\n",
            "208/208 [==============================] - 0s 761us/step - loss: 0.1627 - acc: 0.7404\n",
            "Epoch 15/200\n",
            "208/208 [==============================] - 0s 756us/step - loss: 0.1565 - acc: 0.7885\n",
            "Epoch 16/200\n",
            "208/208 [==============================] - 0s 746us/step - loss: 0.1519 - acc: 0.7837\n",
            "Epoch 17/200\n",
            "208/208 [==============================] - 0s 844us/step - loss: 0.1466 - acc: 0.8029\n",
            "Epoch 18/200\n",
            "208/208 [==============================] - 0s 891us/step - loss: 0.1424 - acc: 0.7933\n",
            "Epoch 19/200\n",
            "208/208 [==============================] - 0s 816us/step - loss: 0.1400 - acc: 0.8221\n",
            "Epoch 20/200\n",
            "208/208 [==============================] - 0s 788us/step - loss: 0.1407 - acc: 0.8173\n",
            "Epoch 21/200\n",
            "208/208 [==============================] - 0s 857us/step - loss: 0.1337 - acc: 0.8077\n",
            "Epoch 22/200\n",
            "208/208 [==============================] - 0s 805us/step - loss: 0.1302 - acc: 0.8077\n",
            "Epoch 23/200\n",
            "208/208 [==============================] - 0s 804us/step - loss: 0.1300 - acc: 0.8221\n",
            "Epoch 24/200\n",
            "208/208 [==============================] - 0s 779us/step - loss: 0.1304 - acc: 0.8365\n",
            "Epoch 25/200\n",
            "208/208 [==============================] - 0s 768us/step - loss: 0.1246 - acc: 0.8654\n",
            "Epoch 26/200\n",
            "208/208 [==============================] - 0s 759us/step - loss: 0.1217 - acc: 0.8365\n",
            "Epoch 27/200\n",
            "208/208 [==============================] - 0s 772us/step - loss: 0.1209 - acc: 0.8317\n",
            "Epoch 28/200\n",
            "208/208 [==============================] - 0s 818us/step - loss: 0.1222 - acc: 0.8365\n",
            "Epoch 29/200\n",
            "208/208 [==============================] - 0s 830us/step - loss: 0.1213 - acc: 0.8462\n",
            "Epoch 30/200\n",
            "208/208 [==============================] - 0s 774us/step - loss: 0.1208 - acc: 0.8413\n",
            "Epoch 31/200\n",
            "208/208 [==============================] - 0s 767us/step - loss: 0.1154 - acc: 0.8317\n",
            "Epoch 32/200\n",
            "208/208 [==============================] - 0s 772us/step - loss: 0.1110 - acc: 0.8654\n",
            "Epoch 33/200\n",
            "208/208 [==============================] - 0s 800us/step - loss: 0.1127 - acc: 0.8462\n",
            "Epoch 34/200\n",
            "208/208 [==============================] - 0s 766us/step - loss: 0.1111 - acc: 0.8606\n",
            "Epoch 35/200\n",
            "208/208 [==============================] - 0s 840us/step - loss: 0.1125 - acc: 0.8269\n",
            "Epoch 36/200\n",
            "208/208 [==============================] - 0s 790us/step - loss: 0.1105 - acc: 0.8269\n",
            "Epoch 37/200\n",
            "208/208 [==============================] - 0s 894us/step - loss: 0.1072 - acc: 0.8702\n",
            "Epoch 38/200\n",
            "208/208 [==============================] - 0s 793us/step - loss: 0.1056 - acc: 0.8558\n",
            "Epoch 39/200\n",
            "208/208 [==============================] - 0s 797us/step - loss: 0.1036 - acc: 0.8750\n",
            "Epoch 40/200\n",
            "208/208 [==============================] - 0s 816us/step - loss: 0.1005 - acc: 0.8702\n",
            "Epoch 41/200\n",
            "208/208 [==============================] - 0s 851us/step - loss: 0.0996 - acc: 0.8846\n",
            "Epoch 42/200\n",
            "208/208 [==============================] - 0s 739us/step - loss: 0.1001 - acc: 0.8654\n",
            "Epoch 43/200\n",
            "208/208 [==============================] - 0s 742us/step - loss: 0.0981 - acc: 0.8750\n",
            "Epoch 44/200\n",
            "208/208 [==============================] - 0s 792us/step - loss: 0.0963 - acc: 0.8846\n",
            "Epoch 45/200\n",
            "208/208 [==============================] - 0s 752us/step - loss: 0.0938 - acc: 0.8750\n",
            "Epoch 46/200\n",
            "208/208 [==============================] - 0s 746us/step - loss: 0.0925 - acc: 0.8894\n",
            "Epoch 47/200\n",
            "208/208 [==============================] - 0s 836us/step - loss: 0.0912 - acc: 0.8990\n",
            "Epoch 48/200\n",
            "208/208 [==============================] - 0s 819us/step - loss: 0.0930 - acc: 0.8798\n",
            "Epoch 49/200\n",
            "208/208 [==============================] - 0s 760us/step - loss: 0.0913 - acc: 0.9087\n",
            "Epoch 50/200\n",
            "208/208 [==============================] - 0s 803us/step - loss: 0.0929 - acc: 0.8798\n",
            "Epoch 51/200\n",
            "208/208 [==============================] - 0s 799us/step - loss: 0.0885 - acc: 0.8750\n",
            "Epoch 52/200\n",
            "208/208 [==============================] - 0s 761us/step - loss: 0.0871 - acc: 0.8894\n",
            "Epoch 53/200\n",
            "208/208 [==============================] - 0s 747us/step - loss: 0.0855 - acc: 0.8942\n",
            "Epoch 54/200\n",
            "208/208 [==============================] - 0s 877us/step - loss: 0.0854 - acc: 0.9135\n",
            "Epoch 55/200\n",
            "208/208 [==============================] - 0s 860us/step - loss: 0.0826 - acc: 0.9087\n",
            "Epoch 56/200\n",
            "208/208 [==============================] - 0s 858us/step - loss: 0.0824 - acc: 0.9087\n",
            "Epoch 57/200\n",
            "208/208 [==============================] - 0s 749us/step - loss: 0.0805 - acc: 0.9087\n",
            "Epoch 58/200\n",
            "208/208 [==============================] - 0s 809us/step - loss: 0.0794 - acc: 0.9135\n",
            "Epoch 59/200\n",
            "208/208 [==============================] - 0s 744us/step - loss: 0.0821 - acc: 0.9135\n",
            "Epoch 60/200\n",
            "208/208 [==============================] - 0s 832us/step - loss: 0.0788 - acc: 0.9038\n",
            "Epoch 61/200\n",
            "208/208 [==============================] - 0s 795us/step - loss: 0.0820 - acc: 0.9038\n",
            "Epoch 62/200\n",
            "208/208 [==============================] - 0s 805us/step - loss: 0.0791 - acc: 0.9038\n",
            "Epoch 63/200\n",
            "208/208 [==============================] - 0s 773us/step - loss: 0.0755 - acc: 0.9038\n",
            "Epoch 64/200\n",
            "208/208 [==============================] - 0s 779us/step - loss: 0.0749 - acc: 0.9279\n",
            "Epoch 65/200\n",
            "208/208 [==============================] - 0s 758us/step - loss: 0.0733 - acc: 0.9279\n",
            "Epoch 66/200\n",
            "208/208 [==============================] - 0s 826us/step - loss: 0.0746 - acc: 0.9231\n",
            "Epoch 67/200\n",
            "208/208 [==============================] - 0s 809us/step - loss: 0.0768 - acc: 0.9087\n",
            "Epoch 68/200\n",
            "208/208 [==============================] - 0s 806us/step - loss: 0.0711 - acc: 0.9279\n",
            "Epoch 69/200\n",
            "208/208 [==============================] - 0s 925us/step - loss: 0.0698 - acc: 0.9183\n",
            "Epoch 70/200\n",
            "208/208 [==============================] - 0s 832us/step - loss: 0.0692 - acc: 0.9327\n",
            "Epoch 71/200\n",
            "208/208 [==============================] - 0s 791us/step - loss: 0.0683 - acc: 0.9375\n",
            "Epoch 72/200\n",
            "208/208 [==============================] - 0s 929us/step - loss: 0.0740 - acc: 0.8990\n",
            "Epoch 73/200\n",
            "208/208 [==============================] - 0s 777us/step - loss: 0.0686 - acc: 0.9231\n",
            "Epoch 74/200\n",
            "208/208 [==============================] - 0s 768us/step - loss: 0.0687 - acc: 0.9135\n",
            "Epoch 75/200\n",
            "208/208 [==============================] - 0s 803us/step - loss: 0.0672 - acc: 0.9279\n",
            "Epoch 76/200\n",
            "208/208 [==============================] - 0s 771us/step - loss: 0.0671 - acc: 0.9231\n",
            "Epoch 77/200\n",
            "208/208 [==============================] - 0s 736us/step - loss: 0.0630 - acc: 0.9375\n",
            "Epoch 78/200\n",
            "208/208 [==============================] - 0s 817us/step - loss: 0.0631 - acc: 0.9375\n",
            "Epoch 79/200\n",
            "208/208 [==============================] - 0s 768us/step - loss: 0.0627 - acc: 0.9375\n",
            "Epoch 80/200\n",
            "208/208 [==============================] - 0s 822us/step - loss: 0.0621 - acc: 0.9375\n",
            "Epoch 81/200\n",
            "208/208 [==============================] - 0s 776us/step - loss: 0.0619 - acc: 0.9375\n",
            "Epoch 82/200\n",
            "208/208 [==============================] - 0s 812us/step - loss: 0.0611 - acc: 0.9375\n",
            "Epoch 83/200\n",
            "208/208 [==============================] - 0s 807us/step - loss: 0.0599 - acc: 0.9183\n",
            "Epoch 84/200\n",
            "208/208 [==============================] - 0s 851us/step - loss: 0.0623 - acc: 0.9327\n",
            "Epoch 85/200\n",
            "208/208 [==============================] - 0s 854us/step - loss: 0.0603 - acc: 0.9327\n",
            "Epoch 86/200\n",
            "208/208 [==============================] - 0s 765us/step - loss: 0.0581 - acc: 0.9423\n",
            "Epoch 87/200\n",
            "208/208 [==============================] - 0s 843us/step - loss: 0.0565 - acc: 0.9519\n",
            "Epoch 88/200\n",
            "208/208 [==============================] - 0s 811us/step - loss: 0.0570 - acc: 0.9519\n",
            "Epoch 89/200\n",
            "208/208 [==============================] - 0s 819us/step - loss: 0.0555 - acc: 0.9471\n",
            "Epoch 90/200\n",
            "208/208 [==============================] - 0s 947us/step - loss: 0.0558 - acc: 0.9423\n",
            "Epoch 91/200\n",
            "208/208 [==============================] - 0s 764us/step - loss: 0.0530 - acc: 0.9519\n",
            "Epoch 92/200\n",
            "208/208 [==============================] - 0s 848us/step - loss: 0.0585 - acc: 0.9279\n",
            "Epoch 93/200\n",
            "208/208 [==============================] - 0s 790us/step - loss: 0.0543 - acc: 0.9471\n",
            "Epoch 94/200\n",
            "208/208 [==============================] - 0s 846us/step - loss: 0.0551 - acc: 0.9471\n",
            "Epoch 95/200\n",
            "208/208 [==============================] - 0s 780us/step - loss: 0.0505 - acc: 0.9471\n",
            "Epoch 96/200\n",
            "208/208 [==============================] - 0s 860us/step - loss: 0.0507 - acc: 0.9519\n",
            "Epoch 97/200\n",
            "208/208 [==============================] - 0s 810us/step - loss: 0.0529 - acc: 0.9519\n",
            "Epoch 98/200\n",
            "208/208 [==============================] - 0s 858us/step - loss: 0.0479 - acc: 0.9471\n",
            "Epoch 99/200\n",
            "208/208 [==============================] - 0s 792us/step - loss: 0.0456 - acc: 0.9567\n",
            "Epoch 100/200\n",
            "208/208 [==============================] - 0s 871us/step - loss: 0.0476 - acc: 0.9423\n",
            "Epoch 101/200\n",
            "208/208 [==============================] - 0s 746us/step - loss: 0.0465 - acc: 0.9567\n",
            "Epoch 102/200\n",
            "208/208 [==============================] - 0s 825us/step - loss: 0.0429 - acc: 0.9615\n",
            "Epoch 103/200\n",
            "208/208 [==============================] - 0s 831us/step - loss: 0.0417 - acc: 0.9615\n",
            "Epoch 104/200\n",
            "208/208 [==============================] - 0s 768us/step - loss: 0.0437 - acc: 0.9567\n",
            "Epoch 105/200\n",
            "208/208 [==============================] - 0s 745us/step - loss: 0.0404 - acc: 0.9663\n",
            "Epoch 106/200\n",
            "208/208 [==============================] - 0s 896us/step - loss: 0.0409 - acc: 0.9663\n",
            "Epoch 107/200\n",
            "208/208 [==============================] - 0s 859us/step - loss: 0.0424 - acc: 0.9615\n",
            "Epoch 108/200\n",
            "208/208 [==============================] - 0s 865us/step - loss: 0.0422 - acc: 0.9615\n",
            "Epoch 109/200\n",
            "208/208 [==============================] - 0s 777us/step - loss: 0.0398 - acc: 0.9615\n",
            "Epoch 110/200\n",
            "208/208 [==============================] - 0s 822us/step - loss: 0.0404 - acc: 0.9663\n",
            "Epoch 111/200\n",
            "208/208 [==============================] - 0s 834us/step - loss: 0.0405 - acc: 0.9567\n",
            "Epoch 112/200\n",
            "208/208 [==============================] - 0s 807us/step - loss: 0.0378 - acc: 0.9712\n",
            "Epoch 113/200\n",
            "208/208 [==============================] - 0s 757us/step - loss: 0.0352 - acc: 0.9615\n",
            "Epoch 114/200\n",
            "208/208 [==============================] - 0s 860us/step - loss: 0.0346 - acc: 0.9663\n",
            "Epoch 115/200\n",
            "208/208 [==============================] - 0s 748us/step - loss: 0.0339 - acc: 0.9712\n",
            "Epoch 116/200\n",
            "208/208 [==============================] - 0s 795us/step - loss: 0.0344 - acc: 0.9760\n",
            "Epoch 117/200\n",
            "208/208 [==============================] - 0s 797us/step - loss: 0.0323 - acc: 0.9808\n",
            "Epoch 118/200\n",
            "208/208 [==============================] - 0s 762us/step - loss: 0.0358 - acc: 0.9615\n",
            "Epoch 119/200\n",
            "208/208 [==============================] - 0s 818us/step - loss: 0.0318 - acc: 0.9760\n",
            "Epoch 120/200\n",
            "208/208 [==============================] - 0s 842us/step - loss: 0.0301 - acc: 0.9760\n",
            "Epoch 121/200\n",
            "208/208 [==============================] - 0s 744us/step - loss: 0.0297 - acc: 0.9760\n",
            "Epoch 122/200\n",
            "208/208 [==============================] - 0s 767us/step - loss: 0.0306 - acc: 0.9712\n",
            "Epoch 123/200\n",
            "208/208 [==============================] - 0s 849us/step - loss: 0.0293 - acc: 0.9856\n",
            "Epoch 124/200\n",
            "208/208 [==============================] - 0s 744us/step - loss: 0.0274 - acc: 0.9760\n",
            "Epoch 125/200\n",
            "208/208 [==============================] - 0s 790us/step - loss: 0.0298 - acc: 0.9712\n",
            "Epoch 126/200\n",
            "208/208 [==============================] - 0s 857us/step - loss: 0.0284 - acc: 0.9760\n",
            "Epoch 127/200\n",
            "208/208 [==============================] - 0s 864us/step - loss: 0.0264 - acc: 0.9856\n",
            "Epoch 128/200\n",
            "208/208 [==============================] - 0s 780us/step - loss: 0.0262 - acc: 0.9760\n",
            "Epoch 129/200\n",
            "208/208 [==============================] - 0s 745us/step - loss: 0.0257 - acc: 0.9856\n",
            "Epoch 130/200\n",
            "208/208 [==============================] - 0s 809us/step - loss: 0.0235 - acc: 0.9856\n",
            "Epoch 131/200\n",
            "208/208 [==============================] - 0s 756us/step - loss: 0.0261 - acc: 0.9808\n",
            "Epoch 132/200\n",
            "208/208 [==============================] - 0s 829us/step - loss: 0.0245 - acc: 0.9856\n",
            "Epoch 133/200\n",
            "208/208 [==============================] - 0s 785us/step - loss: 0.0268 - acc: 0.9808\n",
            "Epoch 134/200\n",
            "208/208 [==============================] - 0s 744us/step - loss: 0.0237 - acc: 0.9808\n",
            "Epoch 135/200\n",
            "208/208 [==============================] - 0s 752us/step - loss: 0.0236 - acc: 0.9904\n",
            "Epoch 136/200\n",
            "208/208 [==============================] - 0s 808us/step - loss: 0.0221 - acc: 0.9904\n",
            "Epoch 137/200\n",
            "208/208 [==============================] - 0s 752us/step - loss: 0.0207 - acc: 0.9904\n",
            "Epoch 138/200\n",
            "208/208 [==============================] - 0s 841us/step - loss: 0.0206 - acc: 0.9904\n",
            "Epoch 139/200\n",
            "208/208 [==============================] - 0s 924us/step - loss: 0.0210 - acc: 0.9856\n",
            "Epoch 140/200\n",
            "208/208 [==============================] - 0s 845us/step - loss: 0.0214 - acc: 0.9904\n",
            "Epoch 141/200\n",
            "208/208 [==============================] - 0s 872us/step - loss: 0.0216 - acc: 0.9856\n",
            "Epoch 142/200\n",
            "208/208 [==============================] - 0s 897us/step - loss: 0.0196 - acc: 0.9904\n",
            "Epoch 143/200\n",
            "208/208 [==============================] - 0s 746us/step - loss: 0.0182 - acc: 0.9904\n",
            "Epoch 144/200\n",
            "208/208 [==============================] - 0s 897us/step - loss: 0.0224 - acc: 0.9856\n",
            "Epoch 145/200\n",
            "208/208 [==============================] - 0s 860us/step - loss: 0.0184 - acc: 0.9904\n",
            "Epoch 146/200\n",
            "208/208 [==============================] - 0s 829us/step - loss: 0.0184 - acc: 0.9904\n",
            "Epoch 147/200\n",
            "208/208 [==============================] - 0s 804us/step - loss: 0.0179 - acc: 0.9904\n",
            "Epoch 148/200\n",
            "208/208 [==============================] - 0s 765us/step - loss: 0.0178 - acc: 0.9904\n",
            "Epoch 149/200\n",
            "208/208 [==============================] - 0s 761us/step - loss: 0.0177 - acc: 0.9904\n",
            "Epoch 150/200\n",
            "208/208 [==============================] - 0s 874us/step - loss: 0.0177 - acc: 0.9904\n",
            "Epoch 151/200\n",
            "208/208 [==============================] - 0s 862us/step - loss: 0.0166 - acc: 0.9904\n",
            "Epoch 152/200\n",
            "208/208 [==============================] - 0s 746us/step - loss: 0.0166 - acc: 0.9904\n",
            "Epoch 153/200\n",
            "208/208 [==============================] - 0s 764us/step - loss: 0.0161 - acc: 0.9904\n",
            "Epoch 154/200\n",
            "208/208 [==============================] - 0s 849us/step - loss: 0.0158 - acc: 0.9904\n",
            "Epoch 155/200\n",
            "208/208 [==============================] - 0s 797us/step - loss: 0.0166 - acc: 0.9904\n",
            "Epoch 156/200\n",
            "208/208 [==============================] - 0s 882us/step - loss: 0.0167 - acc: 0.9904\n",
            "Epoch 157/200\n",
            "208/208 [==============================] - 0s 800us/step - loss: 0.0169 - acc: 0.9904\n",
            "Epoch 158/200\n",
            "208/208 [==============================] - 0s 821us/step - loss: 0.0184 - acc: 0.9856\n",
            "Epoch 159/200\n",
            "208/208 [==============================] - 0s 826us/step - loss: 0.0154 - acc: 0.9904\n",
            "Epoch 160/200\n",
            "208/208 [==============================] - 0s 764us/step - loss: 0.0148 - acc: 0.9904\n",
            "Epoch 161/200\n",
            "208/208 [==============================] - 0s 755us/step - loss: 0.0146 - acc: 0.9904\n",
            "Epoch 162/200\n",
            "208/208 [==============================] - 0s 888us/step - loss: 0.0146 - acc: 0.9904\n",
            "Epoch 163/200\n",
            "208/208 [==============================] - 0s 761us/step - loss: 0.0146 - acc: 0.9904\n",
            "Epoch 164/200\n",
            "208/208 [==============================] - 0s 762us/step - loss: 0.0141 - acc: 0.9904\n",
            "Epoch 165/200\n",
            "208/208 [==============================] - 0s 803us/step - loss: 0.0137 - acc: 0.9904\n",
            "Epoch 166/200\n",
            "208/208 [==============================] - 0s 789us/step - loss: 0.0142 - acc: 0.9904\n",
            "Epoch 167/200\n",
            "208/208 [==============================] - 0s 785us/step - loss: 0.0144 - acc: 0.9904\n",
            "Epoch 168/200\n",
            "208/208 [==============================] - 0s 875us/step - loss: 0.0149 - acc: 0.9904\n",
            "Epoch 169/200\n",
            "208/208 [==============================] - 0s 802us/step - loss: 0.0134 - acc: 0.9904\n",
            "Epoch 170/200\n",
            "208/208 [==============================] - 0s 852us/step - loss: 0.0128 - acc: 0.9904\n",
            "Epoch 171/200\n",
            "208/208 [==============================] - 0s 786us/step - loss: 0.0130 - acc: 0.9904\n",
            "Epoch 172/200\n",
            "208/208 [==============================] - 0s 744us/step - loss: 0.0129 - acc: 0.9904\n",
            "Epoch 173/200\n",
            "208/208 [==============================] - 0s 822us/step - loss: 0.0128 - acc: 0.9904\n",
            "Epoch 174/200\n",
            "208/208 [==============================] - 0s 808us/step - loss: 0.0119 - acc: 0.9904\n",
            "Epoch 175/200\n",
            "208/208 [==============================] - 0s 859us/step - loss: 0.0114 - acc: 0.9904\n",
            "Epoch 176/200\n",
            "208/208 [==============================] - 0s 820us/step - loss: 0.0113 - acc: 0.9904\n",
            "Epoch 177/200\n",
            "208/208 [==============================] - 0s 766us/step - loss: 0.0109 - acc: 0.9904\n",
            "Epoch 178/200\n",
            "208/208 [==============================] - 0s 773us/step - loss: 0.0109 - acc: 0.9904\n",
            "Epoch 179/200\n",
            "208/208 [==============================] - 0s 743us/step - loss: 0.0103 - acc: 0.9952\n",
            "Epoch 180/200\n",
            "208/208 [==============================] - 0s 816us/step - loss: 0.0126 - acc: 0.9904\n",
            "Epoch 181/200\n",
            "208/208 [==============================] - 0s 736us/step - loss: 0.0257 - acc: 0.9760\n",
            "Epoch 182/200\n",
            "208/208 [==============================] - 0s 865us/step - loss: 0.0107 - acc: 0.9904\n",
            "Epoch 183/200\n",
            "208/208 [==============================] - 0s 811us/step - loss: 0.0103 - acc: 0.9952\n",
            "Epoch 184/200\n",
            "208/208 [==============================] - 0s 842us/step - loss: 0.0092 - acc: 0.9952\n",
            "Epoch 185/200\n",
            "208/208 [==============================] - 0s 759us/step - loss: 0.0090 - acc: 0.9952\n",
            "Epoch 186/200\n",
            "208/208 [==============================] - 0s 836us/step - loss: 0.0092 - acc: 0.9952\n",
            "Epoch 187/200\n",
            "208/208 [==============================] - 0s 876us/step - loss: 0.0110 - acc: 0.9952\n",
            "Epoch 188/200\n",
            "208/208 [==============================] - 0s 858us/step - loss: 0.0082 - acc: 0.9952\n",
            "Epoch 189/200\n",
            "208/208 [==============================] - 0s 813us/step - loss: 0.0086 - acc: 0.9952\n",
            "Epoch 190/200\n",
            "208/208 [==============================] - 0s 776us/step - loss: 0.0081 - acc: 0.9952\n",
            "Epoch 191/200\n",
            "208/208 [==============================] - 0s 816us/step - loss: 0.0086 - acc: 0.9952\n",
            "Epoch 192/200\n",
            "208/208 [==============================] - 0s 841us/step - loss: 0.0081 - acc: 0.9952\n",
            "Epoch 193/200\n",
            "208/208 [==============================] - 0s 765us/step - loss: 0.0076 - acc: 0.9952\n",
            "Epoch 194/200\n",
            "208/208 [==============================] - 0s 734us/step - loss: 0.0080 - acc: 0.9952\n",
            "Epoch 195/200\n",
            "208/208 [==============================] - 0s 792us/step - loss: 0.0073 - acc: 0.9952\n",
            "Epoch 196/200\n",
            "208/208 [==============================] - 0s 781us/step - loss: 0.0071 - acc: 0.9952\n",
            "Epoch 197/200\n",
            "208/208 [==============================] - 0s 765us/step - loss: 0.0076 - acc: 0.9952\n",
            "Epoch 198/200\n",
            "208/208 [==============================] - 0s 770us/step - loss: 0.0068 - acc: 0.9952\n",
            "Epoch 199/200\n",
            "208/208 [==============================] - 0s 869us/step - loss: 0.0073 - acc: 0.9952\n",
            "Epoch 200/200\n",
            "208/208 [==============================] - 0s 846us/step - loss: 0.0075 - acc: 0.9952\n",
            "208/208 [==============================] - 0s 420us/step\n",
            "\n",
            " Accuracy: 0.9952\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QTsFMCQvdBg9",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "7526d002-9b13-4d8f-fc49-e44d16847389"
      },
      "source": [
        "'''\n",
        "과적합 피하기\n",
        " 과적합(over fitting) - 모델이 학습 데이터셋 안에서는 일정 수준 이상의 예측 정확도를 보이지만, 새로운\n",
        "데이터에 적용하면 잘 맞지 않는 것\n",
        " 과적합은 층이 너무 많거나 변수가 복잡해서 발생하기도 하고 테스트셋과 학습셋이 중복될 때 생기기도\n",
        "합니다\n",
        " 과적합을 방지 방법 - 학습을 하는 데이터셋과 이를 테스트할 데이터셋을 완전히 구분한 다음 학습과 동\n",
        "시에 테스트를 병행하며 진행\n",
        "'''\n",
        "'''\n",
        " 머신러닝의 최종 목적 - 과거의 데이터를 토대로 새로운 데이터를 예측하는 것\n",
        "새로운 데이터에 사용할 모델을 만드는 것\n",
        " 학습셋만 가지고 평가할때, 층을 더하거나 에포크(epoch) 값을 높여 실행 횟수를 늘리면 정확도가 계속해\n",
        "서 올라갈 수 있습니다.\n",
        " 학습 데이터셋만으로 평가한 예측 성공률이 테스트셋에서도 그대로 나타나지는 않습니다.\n",
        " 학습이 깊어져서 학습셋 내부에서의 성공률은 높아져도 테스트셋에서는 효과가 없다면 과적합이 일어나\n",
        "고 있는 것입니다.\n",
        "'''\n",
        "\n",
        "# 초음파 광물 예측 분석(학습셋과 테스트셋 구분) 실습\n",
        "from keras.models import Sequential\n",
        "from keras.layers.core import Dense\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.model_selection import train_test_split\n",
        "import pandas as pd\n",
        "import numpy\n",
        "import tensorflow as tf\n",
        "\n",
        "seed = 0\n",
        "numpy.random.seed(seed) # seed 값 설정\n",
        "tf.set_random_seed(seed)\n",
        "\n",
        "df = pd.read_csv('/content/drive/My Drive/Colab Notebooks/싸이킷런 머신러닝(박태정T)/data/sonar.csv'\n",
        "                 , header=None)\n",
        "dataset = df.values\n",
        "X = dataset[:,0:60]\n",
        "Y_obj = dataset[:,60]\n",
        "\n",
        "e = LabelEncoder()\n",
        "e.fit(Y_obj)\n",
        "Y = e.transform(Y_obj)\n",
        "\n",
        "# 학습셋과 테스트셋의 구분\n",
        "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.3, random_state=seed)\n",
        "model = Sequential()\n",
        "model.add(Dense(24, input_dim=60, activation='relu'))\n",
        "model.add(Dense(10, activation='relu'))\n",
        "model.add(Dense(1, activation='sigmoid'))\n",
        "\n",
        "model.compile(loss='mean_squared_error', optimizer='adam', metrics=['accuracy'])\n",
        "model.fit(X_train, Y_train, epochs=130, batch_size=5)\n",
        "\n",
        "# 테스트셋에 모델 적용\n",
        "print(\"\\n Test Accuracy: %.4f\" % (model.evaluate(X_test, Y_test)[1]))"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/130\n",
            "145/145 [==============================] - 0s 3ms/step - loss: 0.2522 - acc: 0.5310\n",
            "Epoch 2/130\n",
            "145/145 [==============================] - 0s 810us/step - loss: 0.2463 - acc: 0.5172\n",
            "Epoch 3/130\n",
            "145/145 [==============================] - 0s 821us/step - loss: 0.2440 - acc: 0.5379\n",
            "Epoch 4/130\n",
            "145/145 [==============================] - 0s 762us/step - loss: 0.2420 - acc: 0.5310\n",
            "Epoch 5/130\n",
            "145/145 [==============================] - 0s 755us/step - loss: 0.2398 - acc: 0.5448\n",
            "Epoch 6/130\n",
            "145/145 [==============================] - 0s 780us/step - loss: 0.2382 - acc: 0.5517\n",
            "Epoch 7/130\n",
            "145/145 [==============================] - 0s 813us/step - loss: 0.2340 - acc: 0.5448\n",
            "Epoch 8/130\n",
            "145/145 [==============================] - 0s 800us/step - loss: 0.2302 - acc: 0.5586\n",
            "Epoch 9/130\n",
            "145/145 [==============================] - 0s 873us/step - loss: 0.2209 - acc: 0.6483\n",
            "Epoch 10/130\n",
            "145/145 [==============================] - 0s 751us/step - loss: 0.2127 - acc: 0.6552\n",
            "Epoch 11/130\n",
            "145/145 [==============================] - 0s 837us/step - loss: 0.2080 - acc: 0.6690\n",
            "Epoch 12/130\n",
            "145/145 [==============================] - 0s 741us/step - loss: 0.2015 - acc: 0.7172\n",
            "Epoch 13/130\n",
            "145/145 [==============================] - 0s 731us/step - loss: 0.1892 - acc: 0.7310\n",
            "Epoch 14/130\n",
            "145/145 [==============================] - 0s 976us/step - loss: 0.1787 - acc: 0.7448\n",
            "Epoch 15/130\n",
            "145/145 [==============================] - 0s 900us/step - loss: 0.1721 - acc: 0.7448\n",
            "Epoch 16/130\n",
            "145/145 [==============================] - 0s 889us/step - loss: 0.1659 - acc: 0.7793\n",
            "Epoch 17/130\n",
            "145/145 [==============================] - 0s 872us/step - loss: 0.1613 - acc: 0.7655\n",
            "Epoch 18/130\n",
            "145/145 [==============================] - 0s 918us/step - loss: 0.1566 - acc: 0.7655\n",
            "Epoch 19/130\n",
            "145/145 [==============================] - 0s 759us/step - loss: 0.1549 - acc: 0.7862\n",
            "Epoch 20/130\n",
            "145/145 [==============================] - 0s 776us/step - loss: 0.1498 - acc: 0.7724\n",
            "Epoch 21/130\n",
            "145/145 [==============================] - 0s 807us/step - loss: 0.1506 - acc: 0.8069\n",
            "Epoch 22/130\n",
            "145/145 [==============================] - 0s 789us/step - loss: 0.1454 - acc: 0.8069\n",
            "Epoch 23/130\n",
            "145/145 [==============================] - 0s 839us/step - loss: 0.1446 - acc: 0.7931\n",
            "Epoch 24/130\n",
            "145/145 [==============================] - 0s 804us/step - loss: 0.1380 - acc: 0.8276\n",
            "Epoch 25/130\n",
            "145/145 [==============================] - 0s 879us/step - loss: 0.1372 - acc: 0.8276\n",
            "Epoch 26/130\n",
            "145/145 [==============================] - 0s 867us/step - loss: 0.1365 - acc: 0.8276\n",
            "Epoch 27/130\n",
            "145/145 [==============================] - 0s 818us/step - loss: 0.1326 - acc: 0.8483\n",
            "Epoch 28/130\n",
            "145/145 [==============================] - 0s 760us/step - loss: 0.1370 - acc: 0.8138\n",
            "Epoch 29/130\n",
            "145/145 [==============================] - 0s 790us/step - loss: 0.1296 - acc: 0.8345\n",
            "Epoch 30/130\n",
            "145/145 [==============================] - 0s 796us/step - loss: 0.1366 - acc: 0.8207\n",
            "Epoch 31/130\n",
            "145/145 [==============================] - 0s 779us/step - loss: 0.1277 - acc: 0.8345\n",
            "Epoch 32/130\n",
            "145/145 [==============================] - 0s 788us/step - loss: 0.1284 - acc: 0.8276\n",
            "Epoch 33/130\n",
            "145/145 [==============================] - 0s 893us/step - loss: 0.1226 - acc: 0.8621\n",
            "Epoch 34/130\n",
            "145/145 [==============================] - 0s 833us/step - loss: 0.1222 - acc: 0.8690\n",
            "Epoch 35/130\n",
            "145/145 [==============================] - 0s 842us/step - loss: 0.1211 - acc: 0.8690\n",
            "Epoch 36/130\n",
            "145/145 [==============================] - 0s 742us/step - loss: 0.1237 - acc: 0.8483\n",
            "Epoch 37/130\n",
            "145/145 [==============================] - 0s 765us/step - loss: 0.1232 - acc: 0.8207\n",
            "Epoch 38/130\n",
            "145/145 [==============================] - 0s 766us/step - loss: 0.1132 - acc: 0.8690\n",
            "Epoch 39/130\n",
            "145/145 [==============================] - 0s 764us/step - loss: 0.1157 - acc: 0.8552\n",
            "Epoch 40/130\n",
            "145/145 [==============================] - 0s 809us/step - loss: 0.1133 - acc: 0.8690\n",
            "Epoch 41/130\n",
            "145/145 [==============================] - 0s 837us/step - loss: 0.1132 - acc: 0.8483\n",
            "Epoch 42/130\n",
            "145/145 [==============================] - 0s 801us/step - loss: 0.1108 - acc: 0.8414\n",
            "Epoch 43/130\n",
            "145/145 [==============================] - 0s 910us/step - loss: 0.1109 - acc: 0.8552\n",
            "Epoch 44/130\n",
            "145/145 [==============================] - 0s 831us/step - loss: 0.1092 - acc: 0.8621\n",
            "Epoch 45/130\n",
            "145/145 [==============================] - 0s 962us/step - loss: 0.1091 - acc: 0.8552\n",
            "Epoch 46/130\n",
            "145/145 [==============================] - 0s 804us/step - loss: 0.1104 - acc: 0.8552\n",
            "Epoch 47/130\n",
            "145/145 [==============================] - 0s 785us/step - loss: 0.1082 - acc: 0.8483\n",
            "Epoch 48/130\n",
            "145/145 [==============================] - 0s 839us/step - loss: 0.1040 - acc: 0.8828\n",
            "Epoch 49/130\n",
            "145/145 [==============================] - 0s 852us/step - loss: 0.1049 - acc: 0.8483\n",
            "Epoch 50/130\n",
            "145/145 [==============================] - 0s 807us/step - loss: 0.1074 - acc: 0.8483\n",
            "Epoch 51/130\n",
            "145/145 [==============================] - 0s 897us/step - loss: 0.1027 - acc: 0.8759\n",
            "Epoch 52/130\n",
            "145/145 [==============================] - 0s 836us/step - loss: 0.1051 - acc: 0.8621\n",
            "Epoch 53/130\n",
            "145/145 [==============================] - 0s 780us/step - loss: 0.0985 - acc: 0.8828\n",
            "Epoch 54/130\n",
            "145/145 [==============================] - 0s 774us/step - loss: 0.0988 - acc: 0.8690\n",
            "Epoch 55/130\n",
            "145/145 [==============================] - 0s 747us/step - loss: 0.0980 - acc: 0.8897\n",
            "Epoch 56/130\n",
            "145/145 [==============================] - 0s 771us/step - loss: 0.0974 - acc: 0.8759\n",
            "Epoch 57/130\n",
            "145/145 [==============================] - 0s 814us/step - loss: 0.0950 - acc: 0.8828\n",
            "Epoch 58/130\n",
            "145/145 [==============================] - 0s 762us/step - loss: 0.0927 - acc: 0.8759\n",
            "Epoch 59/130\n",
            "145/145 [==============================] - 0s 869us/step - loss: 0.0954 - acc: 0.8690\n",
            "Epoch 60/130\n",
            "145/145 [==============================] - 0s 886us/step - loss: 0.0930 - acc: 0.8828\n",
            "Epoch 61/130\n",
            "145/145 [==============================] - 0s 751us/step - loss: 0.0896 - acc: 0.8828\n",
            "Epoch 62/130\n",
            "145/145 [==============================] - 0s 889us/step - loss: 0.0896 - acc: 0.8897\n",
            "Epoch 63/130\n",
            "145/145 [==============================] - 0s 900us/step - loss: 0.0957 - acc: 0.8759\n",
            "Epoch 64/130\n",
            "145/145 [==============================] - 0s 890us/step - loss: 0.0919 - acc: 0.8897\n",
            "Epoch 65/130\n",
            "145/145 [==============================] - 0s 826us/step - loss: 0.0916 - acc: 0.8690\n",
            "Epoch 66/130\n",
            "145/145 [==============================] - 0s 817us/step - loss: 0.0899 - acc: 0.8966\n",
            "Epoch 67/130\n",
            "145/145 [==============================] - 0s 748us/step - loss: 0.0856 - acc: 0.8897\n",
            "Epoch 68/130\n",
            "145/145 [==============================] - 0s 953us/step - loss: 0.0847 - acc: 0.8966\n",
            "Epoch 69/130\n",
            "145/145 [==============================] - 0s 827us/step - loss: 0.0825 - acc: 0.9103\n",
            "Epoch 70/130\n",
            "145/145 [==============================] - 0s 933us/step - loss: 0.0825 - acc: 0.9034\n",
            "Epoch 71/130\n",
            "145/145 [==============================] - 0s 829us/step - loss: 0.0795 - acc: 0.8897\n",
            "Epoch 72/130\n",
            "145/145 [==============================] - 0s 811us/step - loss: 0.0840 - acc: 0.8897\n",
            "Epoch 73/130\n",
            "145/145 [==============================] - 0s 743us/step - loss: 0.0786 - acc: 0.9103\n",
            "Epoch 74/130\n",
            "145/145 [==============================] - 0s 839us/step - loss: 0.0823 - acc: 0.8897\n",
            "Epoch 75/130\n",
            "145/145 [==============================] - 0s 757us/step - loss: 0.0787 - acc: 0.8966\n",
            "Epoch 76/130\n",
            "145/145 [==============================] - 0s 844us/step - loss: 0.0776 - acc: 0.9034\n",
            "Epoch 77/130\n",
            "145/145 [==============================] - 0s 882us/step - loss: 0.0748 - acc: 0.9172\n",
            "Epoch 78/130\n",
            "145/145 [==============================] - 0s 872us/step - loss: 0.0747 - acc: 0.8966\n",
            "Epoch 79/130\n",
            "145/145 [==============================] - 0s 946us/step - loss: 0.0737 - acc: 0.9034\n",
            "Epoch 80/130\n",
            "145/145 [==============================] - 0s 822us/step - loss: 0.0730 - acc: 0.9034\n",
            "Epoch 81/130\n",
            "145/145 [==============================] - 0s 822us/step - loss: 0.0722 - acc: 0.9310\n",
            "Epoch 82/130\n",
            "145/145 [==============================] - 0s 813us/step - loss: 0.0682 - acc: 0.9103\n",
            "Epoch 83/130\n",
            "145/145 [==============================] - 0s 842us/step - loss: 0.0702 - acc: 0.9172\n",
            "Epoch 84/130\n",
            "145/145 [==============================] - 0s 801us/step - loss: 0.0662 - acc: 0.9241\n",
            "Epoch 85/130\n",
            "145/145 [==============================] - 0s 926us/step - loss: 0.0652 - acc: 0.9310\n",
            "Epoch 86/130\n",
            "145/145 [==============================] - 0s 787us/step - loss: 0.0657 - acc: 0.9172\n",
            "Epoch 87/130\n",
            "145/145 [==============================] - 0s 884us/step - loss: 0.0638 - acc: 0.9310\n",
            "Epoch 88/130\n",
            "145/145 [==============================] - 0s 827us/step - loss: 0.0629 - acc: 0.9241\n",
            "Epoch 89/130\n",
            "145/145 [==============================] - 0s 814us/step - loss: 0.0595 - acc: 0.9310\n",
            "Epoch 90/130\n",
            "145/145 [==============================] - 0s 824us/step - loss: 0.0598 - acc: 0.9517\n",
            "Epoch 91/130\n",
            "145/145 [==============================] - 0s 848us/step - loss: 0.0663 - acc: 0.9034\n",
            "Epoch 92/130\n",
            "145/145 [==============================] - 0s 740us/step - loss: 0.0586 - acc: 0.9310\n",
            "Epoch 93/130\n",
            "145/145 [==============================] - 0s 845us/step - loss: 0.0575 - acc: 0.9241\n",
            "Epoch 94/130\n",
            "145/145 [==============================] - 0s 733us/step - loss: 0.0549 - acc: 0.9448\n",
            "Epoch 95/130\n",
            "145/145 [==============================] - 0s 767us/step - loss: 0.0549 - acc: 0.9379\n",
            "Epoch 96/130\n",
            "145/145 [==============================] - 0s 800us/step - loss: 0.0536 - acc: 0.9448\n",
            "Epoch 97/130\n",
            "145/145 [==============================] - 0s 751us/step - loss: 0.0507 - acc: 0.9586\n",
            "Epoch 98/130\n",
            "145/145 [==============================] - 0s 758us/step - loss: 0.0490 - acc: 0.9586\n",
            "Epoch 99/130\n",
            "145/145 [==============================] - 0s 802us/step - loss: 0.0484 - acc: 0.9655\n",
            "Epoch 100/130\n",
            "145/145 [==============================] - 0s 725us/step - loss: 0.0490 - acc: 0.9517\n",
            "Epoch 101/130\n",
            "145/145 [==============================] - 0s 761us/step - loss: 0.0531 - acc: 0.9517\n",
            "Epoch 102/130\n",
            "145/145 [==============================] - 0s 842us/step - loss: 0.0506 - acc: 0.9448\n",
            "Epoch 103/130\n",
            "145/145 [==============================] - 0s 902us/step - loss: 0.0485 - acc: 0.9448\n",
            "Epoch 104/130\n",
            "145/145 [==============================] - 0s 906us/step - loss: 0.0494 - acc: 0.9448\n",
            "Epoch 105/130\n",
            "145/145 [==============================] - 0s 833us/step - loss: 0.0465 - acc: 0.9517\n",
            "Epoch 106/130\n",
            "145/145 [==============================] - 0s 935us/step - loss: 0.0485 - acc: 0.9586\n",
            "Epoch 107/130\n",
            "145/145 [==============================] - 0s 871us/step - loss: 0.0430 - acc: 0.9517\n",
            "Epoch 108/130\n",
            "145/145 [==============================] - 0s 875us/step - loss: 0.0421 - acc: 0.9862\n",
            "Epoch 109/130\n",
            "145/145 [==============================] - 0s 858us/step - loss: 0.0499 - acc: 0.9310\n",
            "Epoch 110/130\n",
            "145/145 [==============================] - 0s 967us/step - loss: 0.0437 - acc: 0.9448\n",
            "Epoch 111/130\n",
            "145/145 [==============================] - 0s 780us/step - loss: 0.0401 - acc: 0.9517\n",
            "Epoch 112/130\n",
            "145/145 [==============================] - 0s 849us/step - loss: 0.0388 - acc: 0.9724\n",
            "Epoch 113/130\n",
            "145/145 [==============================] - 0s 826us/step - loss: 0.0394 - acc: 0.9655\n",
            "Epoch 114/130\n",
            "145/145 [==============================] - 0s 807us/step - loss: 0.0392 - acc: 0.9793\n",
            "Epoch 115/130\n",
            "145/145 [==============================] - 0s 743us/step - loss: 0.0377 - acc: 0.9931\n",
            "Epoch 116/130\n",
            "145/145 [==============================] - 0s 735us/step - loss: 0.0394 - acc: 0.9793\n",
            "Epoch 117/130\n",
            "145/145 [==============================] - 0s 755us/step - loss: 0.0344 - acc: 0.9862\n",
            "Epoch 118/130\n",
            "145/145 [==============================] - 0s 811us/step - loss: 0.0347 - acc: 0.9862\n",
            "Epoch 119/130\n",
            "145/145 [==============================] - 0s 893us/step - loss: 0.0352 - acc: 0.9655\n",
            "Epoch 120/130\n",
            "145/145 [==============================] - 0s 818us/step - loss: 0.0331 - acc: 0.9931\n",
            "Epoch 121/130\n",
            "145/145 [==============================] - 0s 807us/step - loss: 0.0348 - acc: 0.9724\n",
            "Epoch 122/130\n",
            "145/145 [==============================] - 0s 832us/step - loss: 0.0339 - acc: 0.9793\n",
            "Epoch 123/130\n",
            "145/145 [==============================] - 0s 805us/step - loss: 0.0325 - acc: 0.9793\n",
            "Epoch 124/130\n",
            "145/145 [==============================] - 0s 744us/step - loss: 0.0333 - acc: 0.9862\n",
            "Epoch 125/130\n",
            "145/145 [==============================] - 0s 799us/step - loss: 0.0316 - acc: 0.9793\n",
            "Epoch 126/130\n",
            "145/145 [==============================] - 0s 814us/step - loss: 0.0313 - acc: 0.9862\n",
            "Epoch 127/130\n",
            "145/145 [==============================] - 0s 898us/step - loss: 0.0317 - acc: 0.9862\n",
            "Epoch 128/130\n",
            "145/145 [==============================] - 0s 802us/step - loss: 0.0307 - acc: 0.9931\n",
            "Epoch 129/130\n",
            "145/145 [==============================] - 0s 854us/step - loss: 0.0287 - acc: 0.9862\n",
            "Epoch 130/130\n",
            "145/145 [==============================] - 0s 828us/step - loss: 0.0264 - acc: 0.9931\n",
            "63/63 [==============================] - 0s 2ms/step\n",
            "\n",
            " Test Accuracy: 0.8095\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iZGyvi0Tdi-8",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "7597a982-db90-4eee-9fe6-5df88513cf42"
      },
      "source": [
        "# 초음파 광물 예측 분석 - 모델 저장과 재사용\n",
        "from keras.models import load_model\n",
        "\n",
        "model = Sequential()\n",
        "model.add(Dense(24, input_dim=60, activation='relu'))\n",
        "model.add(Dense(10, activation='relu'))\n",
        "model.add(Dense(1, activation='sigmoid'))\n",
        "\n",
        "model.compile(loss='mean_squared_error', optimizer='adam', metrics=['accuracy'])\n",
        "\n",
        "model.fit(X_train, Y_train, epochs=130, batch_size=5)\n",
        "model.save('/content/drive/My Drive/Colab Notebooks/싸이킷런 머신러닝(박태정T)/data/output/my_model.h5') # 모델을 컴퓨터에 저장\n",
        "\n",
        "del model # 테스트를 위해 메모리 내의 모델을 삭제\n",
        "model = load_model('/content/drive/My Drive/Colab Notebooks/싸이킷런 머신러닝(박태정T)/data/output/my_model.h5') \n",
        "          # 모델을 새로 불러옴\n",
        "\n",
        "print(\"\\n Test Accuracy: %.4f\" % (model.evaluate(X_test, Y_test)[1])) # 불러온 모델로 테스트 실행"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/130\n",
            "145/145 [==============================] - 1s 4ms/step - loss: 0.2474 - acc: 0.5379\n",
            "Epoch 2/130\n",
            "145/145 [==============================] - 0s 862us/step - loss: 0.2377 - acc: 0.6138\n",
            "Epoch 3/130\n",
            "145/145 [==============================] - 0s 742us/step - loss: 0.2334 - acc: 0.6483\n",
            "Epoch 4/130\n",
            "145/145 [==============================] - 0s 753us/step - loss: 0.2254 - acc: 0.6621\n",
            "Epoch 5/130\n",
            "145/145 [==============================] - 0s 902us/step - loss: 0.2159 - acc: 0.6966\n",
            "Epoch 6/130\n",
            "145/145 [==============================] - 0s 744us/step - loss: 0.2058 - acc: 0.6759\n",
            "Epoch 7/130\n",
            "145/145 [==============================] - 0s 790us/step - loss: 0.1954 - acc: 0.7172\n",
            "Epoch 8/130\n",
            "145/145 [==============================] - 0s 918us/step - loss: 0.1879 - acc: 0.6897\n",
            "Epoch 9/130\n",
            "145/145 [==============================] - 0s 954us/step - loss: 0.1870 - acc: 0.7793\n",
            "Epoch 10/130\n",
            "145/145 [==============================] - 0s 1ms/step - loss: 0.1732 - acc: 0.7655\n",
            "Epoch 11/130\n",
            "145/145 [==============================] - 0s 759us/step - loss: 0.1683 - acc: 0.7931\n",
            "Epoch 12/130\n",
            "145/145 [==============================] - 0s 780us/step - loss: 0.1606 - acc: 0.8000\n",
            "Epoch 13/130\n",
            "145/145 [==============================] - 0s 796us/step - loss: 0.1563 - acc: 0.7586\n",
            "Epoch 14/130\n",
            "145/145 [==============================] - 0s 810us/step - loss: 0.1538 - acc: 0.7793\n",
            "Epoch 15/130\n",
            "145/145 [==============================] - 0s 913us/step - loss: 0.1503 - acc: 0.8138\n",
            "Epoch 16/130\n",
            "145/145 [==============================] - 0s 910us/step - loss: 0.1428 - acc: 0.8000\n",
            "Epoch 17/130\n",
            "145/145 [==============================] - 0s 932us/step - loss: 0.1403 - acc: 0.8414\n",
            "Epoch 18/130\n",
            "145/145 [==============================] - 0s 801us/step - loss: 0.1357 - acc: 0.8276\n",
            "Epoch 19/130\n",
            "145/145 [==============================] - 0s 761us/step - loss: 0.1332 - acc: 0.8276\n",
            "Epoch 20/130\n",
            "145/145 [==============================] - 0s 853us/step - loss: 0.1316 - acc: 0.8276\n",
            "Epoch 21/130\n",
            "145/145 [==============================] - 0s 847us/step - loss: 0.1286 - acc: 0.8276\n",
            "Epoch 22/130\n",
            "145/145 [==============================] - 0s 882us/step - loss: 0.1251 - acc: 0.8345\n",
            "Epoch 23/130\n",
            "145/145 [==============================] - 0s 859us/step - loss: 0.1277 - acc: 0.8552\n",
            "Epoch 24/130\n",
            "145/145 [==============================] - 0s 967us/step - loss: 0.1204 - acc: 0.8621\n",
            "Epoch 25/130\n",
            "145/145 [==============================] - 0s 836us/step - loss: 0.1176 - acc: 0.8621\n",
            "Epoch 26/130\n",
            "145/145 [==============================] - 0s 848us/step - loss: 0.1163 - acc: 0.8759\n",
            "Epoch 27/130\n",
            "145/145 [==============================] - 0s 801us/step - loss: 0.1246 - acc: 0.8414\n",
            "Epoch 28/130\n",
            "145/145 [==============================] - 0s 768us/step - loss: 0.1165 - acc: 0.8414\n",
            "Epoch 29/130\n",
            "145/145 [==============================] - 0s 759us/step - loss: 0.1141 - acc: 0.8345\n",
            "Epoch 30/130\n",
            "145/145 [==============================] - 0s 834us/step - loss: 0.1092 - acc: 0.8621\n",
            "Epoch 31/130\n",
            "145/145 [==============================] - 0s 895us/step - loss: 0.1081 - acc: 0.8690\n",
            "Epoch 32/130\n",
            "145/145 [==============================] - 0s 839us/step - loss: 0.1068 - acc: 0.8897\n",
            "Epoch 33/130\n",
            "145/145 [==============================] - 0s 885us/step - loss: 0.1048 - acc: 0.8552\n",
            "Epoch 34/130\n",
            "145/145 [==============================] - 0s 796us/step - loss: 0.1131 - acc: 0.8414\n",
            "Epoch 35/130\n",
            "145/145 [==============================] - 0s 817us/step - loss: 0.1015 - acc: 0.8690\n",
            "Epoch 36/130\n",
            "145/145 [==============================] - 0s 814us/step - loss: 0.1014 - acc: 0.8759\n",
            "Epoch 37/130\n",
            "145/145 [==============================] - 0s 798us/step - loss: 0.1020 - acc: 0.8690\n",
            "Epoch 38/130\n",
            "145/145 [==============================] - 0s 898us/step - loss: 0.0968 - acc: 0.8828\n",
            "Epoch 39/130\n",
            "145/145 [==============================] - 0s 769us/step - loss: 0.0974 - acc: 0.8828\n",
            "Epoch 40/130\n",
            "145/145 [==============================] - 0s 750us/step - loss: 0.0929 - acc: 0.8897\n",
            "Epoch 41/130\n",
            "145/145 [==============================] - 0s 851us/step - loss: 0.0946 - acc: 0.8759\n",
            "Epoch 42/130\n",
            "145/145 [==============================] - 0s 844us/step - loss: 0.0914 - acc: 0.8828\n",
            "Epoch 43/130\n",
            "145/145 [==============================] - 0s 746us/step - loss: 0.0908 - acc: 0.8828\n",
            "Epoch 44/130\n",
            "145/145 [==============================] - 0s 758us/step - loss: 0.0922 - acc: 0.8897\n",
            "Epoch 45/130\n",
            "145/145 [==============================] - 0s 739us/step - loss: 0.0941 - acc: 0.8552\n",
            "Epoch 46/130\n",
            "145/145 [==============================] - 0s 818us/step - loss: 0.0862 - acc: 0.8690\n",
            "Epoch 47/130\n",
            "145/145 [==============================] - 0s 794us/step - loss: 0.0841 - acc: 0.9034\n",
            "Epoch 48/130\n",
            "145/145 [==============================] - 0s 817us/step - loss: 0.0888 - acc: 0.9034\n",
            "Epoch 49/130\n",
            "145/145 [==============================] - 0s 834us/step - loss: 0.0818 - acc: 0.9034\n",
            "Epoch 50/130\n",
            "145/145 [==============================] - 0s 879us/step - loss: 0.0832 - acc: 0.8897\n",
            "Epoch 51/130\n",
            "145/145 [==============================] - 0s 820us/step - loss: 0.0822 - acc: 0.9172\n",
            "Epoch 52/130\n",
            "145/145 [==============================] - 0s 769us/step - loss: 0.0769 - acc: 0.9034\n",
            "Epoch 53/130\n",
            "145/145 [==============================] - 0s 827us/step - loss: 0.0747 - acc: 0.9103\n",
            "Epoch 54/130\n",
            "145/145 [==============================] - 0s 798us/step - loss: 0.0750 - acc: 0.9103\n",
            "Epoch 55/130\n",
            "145/145 [==============================] - 0s 841us/step - loss: 0.0712 - acc: 0.9241\n",
            "Epoch 56/130\n",
            "145/145 [==============================] - 0s 789us/step - loss: 0.0773 - acc: 0.9103\n",
            "Epoch 57/130\n",
            "145/145 [==============================] - 0s 901us/step - loss: 0.0710 - acc: 0.9103\n",
            "Epoch 58/130\n",
            "145/145 [==============================] - 0s 831us/step - loss: 0.0777 - acc: 0.9034\n",
            "Epoch 59/130\n",
            "145/145 [==============================] - 0s 948us/step - loss: 0.0690 - acc: 0.9310\n",
            "Epoch 60/130\n",
            "145/145 [==============================] - 0s 748us/step - loss: 0.0706 - acc: 0.9103\n",
            "Epoch 61/130\n",
            "145/145 [==============================] - 0s 894us/step - loss: 0.0690 - acc: 0.8966\n",
            "Epoch 62/130\n",
            "145/145 [==============================] - 0s 823us/step - loss: 0.0650 - acc: 0.9172\n",
            "Epoch 63/130\n",
            "145/145 [==============================] - 0s 821us/step - loss: 0.0628 - acc: 0.9241\n",
            "Epoch 64/130\n",
            "145/145 [==============================] - 0s 799us/step - loss: 0.0637 - acc: 0.9310\n",
            "Epoch 65/130\n",
            "145/145 [==============================] - 0s 756us/step - loss: 0.0628 - acc: 0.9379\n",
            "Epoch 66/130\n",
            "145/145 [==============================] - 0s 887us/step - loss: 0.0588 - acc: 0.9517\n",
            "Epoch 67/130\n",
            "145/145 [==============================] - 0s 882us/step - loss: 0.0607 - acc: 0.9379\n",
            "Epoch 68/130\n",
            "145/145 [==============================] - 0s 942us/step - loss: 0.0590 - acc: 0.9379\n",
            "Epoch 69/130\n",
            "145/145 [==============================] - 0s 832us/step - loss: 0.0567 - acc: 0.9586\n",
            "Epoch 70/130\n",
            "145/145 [==============================] - 0s 812us/step - loss: 0.0564 - acc: 0.9517\n",
            "Epoch 71/130\n",
            "145/145 [==============================] - 0s 1ms/step - loss: 0.0617 - acc: 0.9241\n",
            "Epoch 72/130\n",
            "145/145 [==============================] - 0s 766us/step - loss: 0.0527 - acc: 0.9517\n",
            "Epoch 73/130\n",
            "145/145 [==============================] - 0s 827us/step - loss: 0.0587 - acc: 0.9379\n",
            "Epoch 74/130\n",
            "145/145 [==============================] - 0s 802us/step - loss: 0.0602 - acc: 0.9241\n",
            "Epoch 75/130\n",
            "145/145 [==============================] - 0s 865us/step - loss: 0.0504 - acc: 0.9586\n",
            "Epoch 76/130\n",
            "145/145 [==============================] - 0s 797us/step - loss: 0.0501 - acc: 0.9448\n",
            "Epoch 77/130\n",
            "145/145 [==============================] - 0s 826us/step - loss: 0.0502 - acc: 0.9586\n",
            "Epoch 78/130\n",
            "145/145 [==============================] - 0s 804us/step - loss: 0.0491 - acc: 0.9517\n",
            "Epoch 79/130\n",
            "145/145 [==============================] - 0s 764us/step - loss: 0.0531 - acc: 0.9310\n",
            "Epoch 80/130\n",
            "145/145 [==============================] - 0s 791us/step - loss: 0.0433 - acc: 0.9586\n",
            "Epoch 81/130\n",
            "145/145 [==============================] - 0s 830us/step - loss: 0.0440 - acc: 0.9724\n",
            "Epoch 82/130\n",
            "145/145 [==============================] - 0s 808us/step - loss: 0.0440 - acc: 0.9655\n",
            "Epoch 83/130\n",
            "145/145 [==============================] - 0s 859us/step - loss: 0.0420 - acc: 0.9655\n",
            "Epoch 84/130\n",
            "145/145 [==============================] - 0s 882us/step - loss: 0.0428 - acc: 0.9655\n",
            "Epoch 85/130\n",
            "145/145 [==============================] - 0s 778us/step - loss: 0.0406 - acc: 0.9724\n",
            "Epoch 86/130\n",
            "145/145 [==============================] - 0s 745us/step - loss: 0.0405 - acc: 0.9655\n",
            "Epoch 87/130\n",
            "145/145 [==============================] - 0s 744us/step - loss: 0.0398 - acc: 0.9724\n",
            "Epoch 88/130\n",
            "145/145 [==============================] - 0s 797us/step - loss: 0.0388 - acc: 0.9586\n",
            "Epoch 89/130\n",
            "145/145 [==============================] - 0s 747us/step - loss: 0.0382 - acc: 0.9586\n",
            "Epoch 90/130\n",
            "145/145 [==============================] - 0s 753us/step - loss: 0.0358 - acc: 0.9655\n",
            "Epoch 91/130\n",
            "145/145 [==============================] - 0s 860us/step - loss: 0.0374 - acc: 0.9517\n",
            "Epoch 92/130\n",
            "145/145 [==============================] - 0s 733us/step - loss: 0.0476 - acc: 0.9517\n",
            "Epoch 93/130\n",
            "145/145 [==============================] - 0s 870us/step - loss: 0.0366 - acc: 0.9517\n",
            "Epoch 94/130\n",
            "145/145 [==============================] - 0s 814us/step - loss: 0.0387 - acc: 0.9655\n",
            "Epoch 95/130\n",
            "145/145 [==============================] - 0s 800us/step - loss: 0.0339 - acc: 0.9586\n",
            "Epoch 96/130\n",
            "145/145 [==============================] - 0s 764us/step - loss: 0.0335 - acc: 0.9724\n",
            "Epoch 97/130\n",
            "145/145 [==============================] - 0s 804us/step - loss: 0.0313 - acc: 0.9724\n",
            "Epoch 98/130\n",
            "145/145 [==============================] - 0s 803us/step - loss: 0.0317 - acc: 0.9655\n",
            "Epoch 99/130\n",
            "145/145 [==============================] - 0s 797us/step - loss: 0.0314 - acc: 0.9724\n",
            "Epoch 100/130\n",
            "145/145 [==============================] - 0s 763us/step - loss: 0.0293 - acc: 0.9793\n",
            "Epoch 101/130\n",
            "145/145 [==============================] - 0s 803us/step - loss: 0.0278 - acc: 0.9793\n",
            "Epoch 102/130\n",
            "145/145 [==============================] - 0s 751us/step - loss: 0.0268 - acc: 0.9862\n",
            "Epoch 103/130\n",
            "145/145 [==============================] - 0s 740us/step - loss: 0.0261 - acc: 0.9862\n",
            "Epoch 104/130\n",
            "145/145 [==============================] - 0s 824us/step - loss: 0.0270 - acc: 0.9793\n",
            "Epoch 105/130\n",
            "145/145 [==============================] - 0s 812us/step - loss: 0.0303 - acc: 0.9724\n",
            "Epoch 106/130\n",
            "145/145 [==============================] - 0s 764us/step - loss: 0.0281 - acc: 0.9862\n",
            "Epoch 107/130\n",
            "145/145 [==============================] - 0s 810us/step - loss: 0.0268 - acc: 0.9862\n",
            "Epoch 108/130\n",
            "145/145 [==============================] - 0s 737us/step - loss: 0.0230 - acc: 0.9862\n",
            "Epoch 109/130\n",
            "145/145 [==============================] - 0s 728us/step - loss: 0.0232 - acc: 0.9931\n",
            "Epoch 110/130\n",
            "145/145 [==============================] - 0s 818us/step - loss: 0.0253 - acc: 0.9862\n",
            "Epoch 111/130\n",
            "145/145 [==============================] - 0s 745us/step - loss: 0.0230 - acc: 0.9931\n",
            "Epoch 112/130\n",
            "145/145 [==============================] - 0s 801us/step - loss: 0.0257 - acc: 0.9793\n",
            "Epoch 113/130\n",
            "145/145 [==============================] - 0s 808us/step - loss: 0.0215 - acc: 0.9931\n",
            "Epoch 114/130\n",
            "145/145 [==============================] - 0s 735us/step - loss: 0.0206 - acc: 0.9862\n",
            "Epoch 115/130\n",
            "145/145 [==============================] - 0s 834us/step - loss: 0.0218 - acc: 0.9862\n",
            "Epoch 116/130\n",
            "145/145 [==============================] - 0s 794us/step - loss: 0.0240 - acc: 0.9793\n",
            "Epoch 117/130\n",
            "145/145 [==============================] - 0s 765us/step - loss: 0.0195 - acc: 0.9931\n",
            "Epoch 118/130\n",
            "145/145 [==============================] - 0s 730us/step - loss: 0.0169 - acc: 0.9931\n",
            "Epoch 119/130\n",
            "145/145 [==============================] - 0s 798us/step - loss: 0.0184 - acc: 0.9862\n",
            "Epoch 120/130\n",
            "145/145 [==============================] - 0s 915us/step - loss: 0.0180 - acc: 0.9931\n",
            "Epoch 121/130\n",
            "145/145 [==============================] - 0s 777us/step - loss: 0.0179 - acc: 0.9931\n",
            "Epoch 122/130\n",
            "145/145 [==============================] - 0s 806us/step - loss: 0.0169 - acc: 0.9931\n",
            "Epoch 123/130\n",
            "145/145 [==============================] - 0s 824us/step - loss: 0.0167 - acc: 0.9931\n",
            "Epoch 124/130\n",
            "145/145 [==============================] - 0s 891us/step - loss: 0.0160 - acc: 0.9931\n",
            "Epoch 125/130\n",
            "145/145 [==============================] - 0s 805us/step - loss: 0.0183 - acc: 0.9862\n",
            "Epoch 126/130\n",
            "145/145 [==============================] - 0s 827us/step - loss: 0.0174 - acc: 0.9862\n",
            "Epoch 127/130\n",
            "145/145 [==============================] - 0s 828us/step - loss: 0.0179 - acc: 0.9862\n",
            "Epoch 128/130\n",
            "145/145 [==============================] - 0s 924us/step - loss: 0.0167 - acc: 0.9862\n",
            "Epoch 129/130\n",
            "145/145 [==============================] - 0s 874us/step - loss: 0.0134 - acc: 0.9931\n",
            "Epoch 130/130\n",
            "145/145 [==============================] - 0s 847us/step - loss: 0.0158 - acc: 0.9862\n",
            "63/63 [==============================] - 0s 2ms/step\n",
            "\n",
            " Test Accuracy: 0.8254\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KtFaX4kzkZxX",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "0ec6808f-aad1-4380-85c7-8e9b1ab10edc"
      },
      "source": [
        "'''\n",
        "k겹 교차 검증\n",
        " k겹 교차 검증(k-fold cross validation) - k겹 교차 검증이란 데이터셋을 여러 개로 나누어 하나씩 테스트\n",
        "셋으로 사용하고 나머지를 모두 합해서 학습셋으로 사용하는 방법\n",
        " 데이터가 충분치 않은 경우, 데이터의 100%를 테스트셋으로 사용할 수 있습니다.\n",
        "'''\n",
        "\n",
        "#초음파 광물 예측 분석 - k겹 교차 검증\n",
        "'''\n",
        "10개의 파일로 쪼개 테스트하는 10-fold cross validation을 실시하도록 n_fold의 값을 10으로 설정한\n",
        "뒤 StratifiedKFold() 함수에 적용했습니다. 그런 다음 모델을 만들고 실행하는 부분을 for 구문으로 묶어 n_fold만큼\n",
        "반복되게 합니다.\n",
        "'''\n",
        "from sklearn.model_selection import StratifiedKFold\n",
        "n_fold = 10\n",
        "skf = StratifiedKFold(n_splits=n_fold, shuffle=True, random_state=seed)\n",
        "\n",
        "accuracy = [] # 빈 accuracy 배열\n",
        "\n",
        "for train, test in skf.split(X, Y): # 모델의 설정, 컴파일, 실행\n",
        "    model = Sequential()\n",
        "    model.add(Dense(24, input_dim=60, activation='relu'))\n",
        "    model.add(Dense(10, activation='relu'))\n",
        "    model.add(Dense(1, activation='sigmoid'))\n",
        "    model.compile(loss='mean_squared_error', optimizer='adam', metrics=['accuracy'])\n",
        "    model.fit(X[train], Y[train], epochs=100, batch_size=5)\n",
        "    k_accuracy = \"%.4f\" % (model.evaluate(X[test], Y[test])[1])\n",
        "    accuracy.append(k_accuracy)\n",
        "\n",
        "# 결과 출력\n",
        "print(\"\\n %.f fold accuracy:\" % n_fold, accuracy)"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/100\n",
            "187/187 [==============================] - 1s 4ms/step - loss: 0.2456 - acc: 0.5348\n",
            "Epoch 2/100\n",
            "187/187 [==============================] - 0s 828us/step - loss: 0.2401 - acc: 0.5829\n",
            "Epoch 3/100\n",
            "187/187 [==============================] - 0s 847us/step - loss: 0.2325 - acc: 0.5775\n",
            "Epoch 4/100\n",
            "187/187 [==============================] - 0s 845us/step - loss: 0.2238 - acc: 0.6578\n",
            "Epoch 5/100\n",
            "187/187 [==============================] - 0s 869us/step - loss: 0.2160 - acc: 0.6791\n",
            "Epoch 6/100\n",
            "187/187 [==============================] - 0s 938us/step - loss: 0.2084 - acc: 0.7059\n",
            "Epoch 7/100\n",
            "187/187 [==============================] - 0s 811us/step - loss: 0.1964 - acc: 0.7594\n",
            "Epoch 8/100\n",
            "187/187 [==============================] - 0s 791us/step - loss: 0.1853 - acc: 0.7968\n",
            "Epoch 9/100\n",
            "187/187 [==============================] - 0s 831us/step - loss: 0.1737 - acc: 0.7914\n",
            "Epoch 10/100\n",
            "187/187 [==============================] - 0s 839us/step - loss: 0.1641 - acc: 0.7914\n",
            "Epoch 11/100\n",
            "187/187 [==============================] - 0s 857us/step - loss: 0.1574 - acc: 0.8128\n",
            "Epoch 12/100\n",
            "187/187 [==============================] - 0s 969us/step - loss: 0.1484 - acc: 0.8128\n",
            "Epoch 13/100\n",
            "187/187 [==============================] - 0s 915us/step - loss: 0.1425 - acc: 0.8342\n",
            "Epoch 14/100\n",
            "187/187 [==============================] - 0s 820us/step - loss: 0.1417 - acc: 0.8289\n",
            "Epoch 15/100\n",
            "187/187 [==============================] - 0s 800us/step - loss: 0.1332 - acc: 0.8182\n",
            "Epoch 16/100\n",
            "187/187 [==============================] - 0s 857us/step - loss: 0.1291 - acc: 0.8289\n",
            "Epoch 17/100\n",
            "187/187 [==============================] - 0s 835us/step - loss: 0.1267 - acc: 0.8503\n",
            "Epoch 18/100\n",
            "187/187 [==============================] - 0s 870us/step - loss: 0.1281 - acc: 0.8289\n",
            "Epoch 19/100\n",
            "187/187 [==============================] - 0s 806us/step - loss: 0.1186 - acc: 0.8342\n",
            "Epoch 20/100\n",
            "187/187 [==============================] - 0s 837us/step - loss: 0.1148 - acc: 0.8610\n",
            "Epoch 21/100\n",
            "187/187 [==============================] - 0s 822us/step - loss: 0.1177 - acc: 0.8396\n",
            "Epoch 22/100\n",
            "187/187 [==============================] - 0s 828us/step - loss: 0.1114 - acc: 0.8556\n",
            "Epoch 23/100\n",
            "187/187 [==============================] - 0s 853us/step - loss: 0.1094 - acc: 0.8556\n",
            "Epoch 24/100\n",
            "187/187 [==============================] - 0s 947us/step - loss: 0.1076 - acc: 0.8610\n",
            "Epoch 25/100\n",
            "187/187 [==============================] - 0s 778us/step - loss: 0.1142 - acc: 0.8396\n",
            "Epoch 26/100\n",
            "187/187 [==============================] - 0s 807us/step - loss: 0.1050 - acc: 0.8663\n",
            "Epoch 27/100\n",
            "187/187 [==============================] - 0s 838us/step - loss: 0.1029 - acc: 0.8824\n",
            "Epoch 28/100\n",
            "187/187 [==============================] - 0s 804us/step - loss: 0.1012 - acc: 0.8930\n",
            "Epoch 29/100\n",
            "187/187 [==============================] - 0s 839us/step - loss: 0.0995 - acc: 0.8877\n",
            "Epoch 30/100\n",
            "187/187 [==============================] - 0s 817us/step - loss: 0.0945 - acc: 0.8824\n",
            "Epoch 31/100\n",
            "187/187 [==============================] - 0s 869us/step - loss: 0.0948 - acc: 0.8717\n",
            "Epoch 32/100\n",
            "187/187 [==============================] - 0s 835us/step - loss: 0.0939 - acc: 0.8824\n",
            "Epoch 33/100\n",
            "187/187 [==============================] - 0s 927us/step - loss: 0.0899 - acc: 0.8930\n",
            "Epoch 34/100\n",
            "187/187 [==============================] - 0s 908us/step - loss: 0.0891 - acc: 0.8930\n",
            "Epoch 35/100\n",
            "187/187 [==============================] - 0s 859us/step - loss: 0.0850 - acc: 0.8930\n",
            "Epoch 36/100\n",
            "187/187 [==============================] - 0s 955us/step - loss: 0.0993 - acc: 0.8556\n",
            "Epoch 37/100\n",
            "187/187 [==============================] - 0s 874us/step - loss: 0.0836 - acc: 0.8984\n",
            "Epoch 38/100\n",
            "187/187 [==============================] - 0s 808us/step - loss: 0.0821 - acc: 0.8930\n",
            "Epoch 39/100\n",
            "187/187 [==============================] - 0s 866us/step - loss: 0.0835 - acc: 0.8717\n",
            "Epoch 40/100\n",
            "187/187 [==============================] - 0s 808us/step - loss: 0.0802 - acc: 0.8984\n",
            "Epoch 41/100\n",
            "187/187 [==============================] - 0s 825us/step - loss: 0.0814 - acc: 0.8930\n",
            "Epoch 42/100\n",
            "187/187 [==============================] - 0s 879us/step - loss: 0.0763 - acc: 0.9144\n",
            "Epoch 43/100\n",
            "187/187 [==============================] - 0s 802us/step - loss: 0.0736 - acc: 0.9037\n",
            "Epoch 44/100\n",
            "187/187 [==============================] - 0s 885us/step - loss: 0.0761 - acc: 0.9037\n",
            "Epoch 45/100\n",
            "187/187 [==============================] - 0s 838us/step - loss: 0.0703 - acc: 0.9144\n",
            "Epoch 46/100\n",
            "187/187 [==============================] - 0s 779us/step - loss: 0.0695 - acc: 0.9091\n",
            "Epoch 47/100\n",
            "187/187 [==============================] - 0s 863us/step - loss: 0.0743 - acc: 0.9037\n",
            "Epoch 48/100\n",
            "187/187 [==============================] - 0s 787us/step - loss: 0.0719 - acc: 0.9037\n",
            "Epoch 49/100\n",
            "187/187 [==============================] - 0s 823us/step - loss: 0.0680 - acc: 0.9144\n",
            "Epoch 50/100\n",
            "187/187 [==============================] - 0s 850us/step - loss: 0.0694 - acc: 0.9144\n",
            "Epoch 51/100\n",
            "187/187 [==============================] - 0s 811us/step - loss: 0.0649 - acc: 0.9198\n",
            "Epoch 52/100\n",
            "187/187 [==============================] - 0s 798us/step - loss: 0.0687 - acc: 0.9091\n",
            "Epoch 53/100\n",
            "187/187 [==============================] - 0s 850us/step - loss: 0.0634 - acc: 0.9251\n",
            "Epoch 54/100\n",
            "187/187 [==============================] - 0s 855us/step - loss: 0.0660 - acc: 0.9091\n",
            "Epoch 55/100\n",
            "187/187 [==============================] - 0s 790us/step - loss: 0.0598 - acc: 0.9358\n",
            "Epoch 56/100\n",
            "187/187 [==============================] - 0s 801us/step - loss: 0.0585 - acc: 0.9251\n",
            "Epoch 57/100\n",
            "187/187 [==============================] - 0s 921us/step - loss: 0.0601 - acc: 0.9251\n",
            "Epoch 58/100\n",
            "187/187 [==============================] - 0s 785us/step - loss: 0.0539 - acc: 0.9465\n",
            "Epoch 59/100\n",
            "187/187 [==============================] - 0s 1ms/step - loss: 0.0567 - acc: 0.9412\n",
            "Epoch 60/100\n",
            "187/187 [==============================] - 0s 982us/step - loss: 0.0523 - acc: 0.9626\n",
            "Epoch 61/100\n",
            "187/187 [==============================] - 0s 921us/step - loss: 0.0504 - acc: 0.9358\n",
            "Epoch 62/100\n",
            "187/187 [==============================] - 0s 929us/step - loss: 0.0491 - acc: 0.9519\n",
            "Epoch 63/100\n",
            "187/187 [==============================] - 0s 874us/step - loss: 0.0476 - acc: 0.9465\n",
            "Epoch 64/100\n",
            "187/187 [==============================] - 0s 791us/step - loss: 0.0462 - acc: 0.9465\n",
            "Epoch 65/100\n",
            "187/187 [==============================] - 0s 832us/step - loss: 0.0457 - acc: 0.9519\n",
            "Epoch 66/100\n",
            "187/187 [==============================] - 0s 804us/step - loss: 0.0438 - acc: 0.9626\n",
            "Epoch 67/100\n",
            "187/187 [==============================] - 0s 820us/step - loss: 0.0437 - acc: 0.9679\n",
            "Epoch 68/100\n",
            "187/187 [==============================] - 0s 874us/step - loss: 0.0441 - acc: 0.9572\n",
            "Epoch 69/100\n",
            "187/187 [==============================] - 0s 938us/step - loss: 0.0417 - acc: 0.9519\n",
            "Epoch 70/100\n",
            "187/187 [==============================] - 0s 802us/step - loss: 0.0412 - acc: 0.9626\n",
            "Epoch 71/100\n",
            "187/187 [==============================] - 0s 807us/step - loss: 0.0412 - acc: 0.9679\n",
            "Epoch 72/100\n",
            "187/187 [==============================] - 0s 885us/step - loss: 0.0373 - acc: 0.9572\n",
            "Epoch 73/100\n",
            "187/187 [==============================] - 0s 803us/step - loss: 0.0363 - acc: 0.9733\n",
            "Epoch 74/100\n",
            "187/187 [==============================] - 0s 930us/step - loss: 0.0356 - acc: 0.9733\n",
            "Epoch 75/100\n",
            "187/187 [==============================] - 0s 845us/step - loss: 0.0369 - acc: 0.9786\n",
            "Epoch 76/100\n",
            "187/187 [==============================] - 0s 860us/step - loss: 0.0366 - acc: 0.9626\n",
            "Epoch 77/100\n",
            "187/187 [==============================] - 0s 987us/step - loss: 0.0338 - acc: 0.9626\n",
            "Epoch 78/100\n",
            "187/187 [==============================] - 0s 936us/step - loss: 0.0327 - acc: 0.9679\n",
            "Epoch 79/100\n",
            "187/187 [==============================] - 0s 928us/step - loss: 0.0308 - acc: 0.9786\n",
            "Epoch 80/100\n",
            "187/187 [==============================] - 0s 872us/step - loss: 0.0301 - acc: 0.9733\n",
            "Epoch 81/100\n",
            "187/187 [==============================] - 0s 858us/step - loss: 0.0280 - acc: 0.9786\n",
            "Epoch 82/100\n",
            "187/187 [==============================] - 0s 864us/step - loss: 0.0281 - acc: 0.9733\n",
            "Epoch 83/100\n",
            "187/187 [==============================] - 0s 803us/step - loss: 0.0287 - acc: 0.9786\n",
            "Epoch 84/100\n",
            "187/187 [==============================] - 0s 806us/step - loss: 0.0316 - acc: 0.9733\n",
            "Epoch 85/100\n",
            "187/187 [==============================] - 0s 839us/step - loss: 0.0274 - acc: 0.9786\n",
            "Epoch 86/100\n",
            "187/187 [==============================] - 0s 841us/step - loss: 0.0244 - acc: 0.9893\n",
            "Epoch 87/100\n",
            "187/187 [==============================] - 0s 832us/step - loss: 0.0258 - acc: 0.9786\n",
            "Epoch 88/100\n",
            "187/187 [==============================] - 0s 955us/step - loss: 0.0242 - acc: 0.9786\n",
            "Epoch 89/100\n",
            "187/187 [==============================] - 0s 807us/step - loss: 0.0225 - acc: 0.9786\n",
            "Epoch 90/100\n",
            "187/187 [==============================] - 0s 862us/step - loss: 0.0225 - acc: 0.9893\n",
            "Epoch 91/100\n",
            "187/187 [==============================] - 0s 800us/step - loss: 0.0229 - acc: 0.9840\n",
            "Epoch 92/100\n",
            "187/187 [==============================] - 0s 960us/step - loss: 0.0214 - acc: 0.9786\n",
            "Epoch 93/100\n",
            "187/187 [==============================] - 0s 831us/step - loss: 0.0213 - acc: 0.9840\n",
            "Epoch 94/100\n",
            "187/187 [==============================] - 0s 957us/step - loss: 0.0213 - acc: 0.9840\n",
            "Epoch 95/100\n",
            "187/187 [==============================] - 0s 805us/step - loss: 0.0200 - acc: 0.9840\n",
            "Epoch 96/100\n",
            "187/187 [==============================] - 0s 804us/step - loss: 0.0184 - acc: 0.9840\n",
            "Epoch 97/100\n",
            "187/187 [==============================] - 0s 866us/step - loss: 0.0192 - acc: 0.9840\n",
            "Epoch 98/100\n",
            "187/187 [==============================] - 0s 840us/step - loss: 0.0196 - acc: 0.9840\n",
            "Epoch 99/100\n",
            "187/187 [==============================] - 0s 796us/step - loss: 0.0181 - acc: 0.9840\n",
            "Epoch 100/100\n",
            "187/187 [==============================] - 0s 837us/step - loss: 0.0191 - acc: 0.9840\n",
            "21/21 [==============================] - 0s 8ms/step\n",
            "Epoch 1/100\n",
            "187/187 [==============================] - 1s 4ms/step - loss: 0.2602 - acc: 0.5401\n",
            "Epoch 2/100\n",
            "187/187 [==============================] - 0s 786us/step - loss: 0.2393 - acc: 0.5508\n",
            "Epoch 3/100\n",
            "187/187 [==============================] - 0s 831us/step - loss: 0.2312 - acc: 0.6952\n",
            "Epoch 4/100\n",
            "187/187 [==============================] - 0s 808us/step - loss: 0.2224 - acc: 0.7005\n",
            "Epoch 5/100\n",
            "187/187 [==============================] - 0s 803us/step - loss: 0.2129 - acc: 0.7059\n",
            "Epoch 6/100\n",
            "187/187 [==============================] - 0s 749us/step - loss: 0.2033 - acc: 0.7487\n",
            "Epoch 7/100\n",
            "187/187 [==============================] - 0s 849us/step - loss: 0.1976 - acc: 0.7273\n",
            "Epoch 8/100\n",
            "187/187 [==============================] - 0s 781us/step - loss: 0.1893 - acc: 0.7219\n",
            "Epoch 9/100\n",
            "187/187 [==============================] - 0s 955us/step - loss: 0.1761 - acc: 0.7647\n",
            "Epoch 10/100\n",
            "187/187 [==============================] - 0s 800us/step - loss: 0.1748 - acc: 0.7326\n",
            "Epoch 11/100\n",
            "187/187 [==============================] - 0s 855us/step - loss: 0.1642 - acc: 0.7861\n",
            "Epoch 12/100\n",
            "187/187 [==============================] - 0s 947us/step - loss: 0.1584 - acc: 0.8021\n",
            "Epoch 13/100\n",
            "187/187 [==============================] - 0s 844us/step - loss: 0.1523 - acc: 0.7701\n",
            "Epoch 14/100\n",
            "187/187 [==============================] - 0s 792us/step - loss: 0.1525 - acc: 0.8021\n",
            "Epoch 15/100\n",
            "187/187 [==============================] - 0s 881us/step - loss: 0.1448 - acc: 0.7861\n",
            "Epoch 16/100\n",
            "187/187 [==============================] - 0s 778us/step - loss: 0.1512 - acc: 0.7861\n",
            "Epoch 17/100\n",
            "187/187 [==============================] - 0s 801us/step - loss: 0.1410 - acc: 0.8128\n",
            "Epoch 18/100\n",
            "187/187 [==============================] - 0s 762us/step - loss: 0.1363 - acc: 0.8182\n",
            "Epoch 19/100\n",
            "187/187 [==============================] - 0s 857us/step - loss: 0.1337 - acc: 0.8182\n",
            "Epoch 20/100\n",
            "187/187 [==============================] - 0s 879us/step - loss: 0.1279 - acc: 0.8342\n",
            "Epoch 21/100\n",
            "187/187 [==============================] - 0s 819us/step - loss: 0.1274 - acc: 0.8342\n",
            "Epoch 22/100\n",
            "187/187 [==============================] - 0s 791us/step - loss: 0.1263 - acc: 0.8235\n",
            "Epoch 23/100\n",
            "187/187 [==============================] - 0s 839us/step - loss: 0.1169 - acc: 0.8556\n",
            "Epoch 24/100\n",
            "187/187 [==============================] - 0s 809us/step - loss: 0.1257 - acc: 0.8342\n",
            "Epoch 25/100\n",
            "187/187 [==============================] - 0s 785us/step - loss: 0.1147 - acc: 0.8449\n",
            "Epoch 26/100\n",
            "187/187 [==============================] - 0s 778us/step - loss: 0.1106 - acc: 0.8556\n",
            "Epoch 27/100\n",
            "187/187 [==============================] - 0s 864us/step - loss: 0.1099 - acc: 0.8610\n",
            "Epoch 28/100\n",
            "187/187 [==============================] - 0s 776us/step - loss: 0.1100 - acc: 0.8503\n",
            "Epoch 29/100\n",
            "187/187 [==============================] - 0s 825us/step - loss: 0.1045 - acc: 0.8610\n",
            "Epoch 30/100\n",
            "187/187 [==============================] - 0s 805us/step - loss: 0.1028 - acc: 0.8877\n",
            "Epoch 31/100\n",
            "187/187 [==============================] - 0s 895us/step - loss: 0.1035 - acc: 0.8556\n",
            "Epoch 32/100\n",
            "187/187 [==============================] - 0s 856us/step - loss: 0.0978 - acc: 0.8930\n",
            "Epoch 33/100\n",
            "187/187 [==============================] - 0s 962us/step - loss: 0.0986 - acc: 0.8663\n",
            "Epoch 34/100\n",
            "187/187 [==============================] - 0s 804us/step - loss: 0.0986 - acc: 0.8824\n",
            "Epoch 35/100\n",
            "187/187 [==============================] - 0s 818us/step - loss: 0.0909 - acc: 0.8930\n",
            "Epoch 36/100\n",
            "187/187 [==============================] - 0s 1ms/step - loss: 0.0885 - acc: 0.8930\n",
            "Epoch 37/100\n",
            "187/187 [==============================] - 0s 909us/step - loss: 0.0925 - acc: 0.8717\n",
            "Epoch 38/100\n",
            "187/187 [==============================] - 0s 796us/step - loss: 0.0872 - acc: 0.8984\n",
            "Epoch 39/100\n",
            "187/187 [==============================] - 0s 814us/step - loss: 0.0819 - acc: 0.9144\n",
            "Epoch 40/100\n",
            "187/187 [==============================] - 0s 881us/step - loss: 0.0837 - acc: 0.8984\n",
            "Epoch 41/100\n",
            "187/187 [==============================] - 0s 946us/step - loss: 0.0790 - acc: 0.9037\n",
            "Epoch 42/100\n",
            "187/187 [==============================] - 0s 812us/step - loss: 0.0811 - acc: 0.9144\n",
            "Epoch 43/100\n",
            "187/187 [==============================] - 0s 836us/step - loss: 0.0789 - acc: 0.9091\n",
            "Epoch 44/100\n",
            "187/187 [==============================] - 0s 831us/step - loss: 0.0752 - acc: 0.9305\n",
            "Epoch 45/100\n",
            "187/187 [==============================] - 0s 777us/step - loss: 0.0728 - acc: 0.9198\n",
            "Epoch 46/100\n",
            "187/187 [==============================] - 0s 853us/step - loss: 0.0709 - acc: 0.9144\n",
            "Epoch 47/100\n",
            "187/187 [==============================] - 0s 904us/step - loss: 0.0723 - acc: 0.9144\n",
            "Epoch 48/100\n",
            "187/187 [==============================] - 0s 788us/step - loss: 0.0721 - acc: 0.8930\n",
            "Epoch 49/100\n",
            "187/187 [==============================] - 0s 798us/step - loss: 0.0775 - acc: 0.9037\n",
            "Epoch 50/100\n",
            "187/187 [==============================] - 0s 807us/step - loss: 0.0643 - acc: 0.9412\n",
            "Epoch 51/100\n",
            "187/187 [==============================] - 0s 775us/step - loss: 0.0676 - acc: 0.9198\n",
            "Epoch 52/100\n",
            "187/187 [==============================] - 0s 787us/step - loss: 0.0630 - acc: 0.9305\n",
            "Epoch 53/100\n",
            "187/187 [==============================] - 0s 837us/step - loss: 0.0619 - acc: 0.9198\n",
            "Epoch 54/100\n",
            "187/187 [==============================] - 0s 786us/step - loss: 0.0604 - acc: 0.9572\n",
            "Epoch 55/100\n",
            "187/187 [==============================] - 0s 794us/step - loss: 0.0578 - acc: 0.9305\n",
            "Epoch 56/100\n",
            "187/187 [==============================] - 0s 841us/step - loss: 0.0575 - acc: 0.9465\n",
            "Epoch 57/100\n",
            "187/187 [==============================] - 0s 931us/step - loss: 0.0575 - acc: 0.9465\n",
            "Epoch 58/100\n",
            "187/187 [==============================] - 0s 793us/step - loss: 0.0574 - acc: 0.9412\n",
            "Epoch 59/100\n",
            "187/187 [==============================] - 0s 858us/step - loss: 0.0541 - acc: 0.9412\n",
            "Epoch 60/100\n",
            "187/187 [==============================] - 0s 820us/step - loss: 0.0571 - acc: 0.9412\n",
            "Epoch 61/100\n",
            "187/187 [==============================] - 0s 811us/step - loss: 0.0585 - acc: 0.9412\n",
            "Epoch 62/100\n",
            "187/187 [==============================] - 0s 844us/step - loss: 0.0501 - acc: 0.9572\n",
            "Epoch 63/100\n",
            "187/187 [==============================] - 0s 802us/step - loss: 0.0482 - acc: 0.9626\n",
            "Epoch 64/100\n",
            "187/187 [==============================] - 0s 790us/step - loss: 0.0485 - acc: 0.9519\n",
            "Epoch 65/100\n",
            "187/187 [==============================] - 0s 807us/step - loss: 0.0467 - acc: 0.9572\n",
            "Epoch 66/100\n",
            "187/187 [==============================] - 0s 903us/step - loss: 0.0456 - acc: 0.9733\n",
            "Epoch 67/100\n",
            "187/187 [==============================] - 0s 799us/step - loss: 0.0432 - acc: 0.9679\n",
            "Epoch 68/100\n",
            "187/187 [==============================] - 0s 850us/step - loss: 0.0417 - acc: 0.9733\n",
            "Epoch 69/100\n",
            "187/187 [==============================] - 0s 802us/step - loss: 0.0412 - acc: 0.9679\n",
            "Epoch 70/100\n",
            "187/187 [==============================] - 0s 800us/step - loss: 0.0420 - acc: 0.9679\n",
            "Epoch 71/100\n",
            "187/187 [==============================] - 0s 784us/step - loss: 0.0385 - acc: 0.9786\n",
            "Epoch 72/100\n",
            "187/187 [==============================] - 0s 843us/step - loss: 0.0383 - acc: 0.9679\n",
            "Epoch 73/100\n",
            "187/187 [==============================] - 0s 821us/step - loss: 0.0383 - acc: 0.9679\n",
            "Epoch 74/100\n",
            "187/187 [==============================] - 0s 868us/step - loss: 0.0358 - acc: 0.9786\n",
            "Epoch 75/100\n",
            "187/187 [==============================] - 0s 833us/step - loss: 0.0361 - acc: 0.9786\n",
            "Epoch 76/100\n",
            "187/187 [==============================] - 0s 812us/step - loss: 0.0359 - acc: 0.9840\n",
            "Epoch 77/100\n",
            "187/187 [==============================] - 0s 837us/step - loss: 0.0354 - acc: 0.9679\n",
            "Epoch 78/100\n",
            "187/187 [==============================] - 0s 872us/step - loss: 0.0332 - acc: 0.9733\n",
            "Epoch 79/100\n",
            "187/187 [==============================] - 0s 980us/step - loss: 0.0312 - acc: 0.9840\n",
            "Epoch 80/100\n",
            "187/187 [==============================] - 0s 808us/step - loss: 0.0309 - acc: 0.9786\n",
            "Epoch 81/100\n",
            "187/187 [==============================] - 0s 872us/step - loss: 0.0292 - acc: 0.9786\n",
            "Epoch 82/100\n",
            "187/187 [==============================] - 0s 823us/step - loss: 0.0280 - acc: 0.9893\n",
            "Epoch 83/100\n",
            "187/187 [==============================] - 0s 880us/step - loss: 0.0305 - acc: 0.9840\n",
            "Epoch 84/100\n",
            "187/187 [==============================] - 0s 805us/step - loss: 0.0296 - acc: 0.9786\n",
            "Epoch 85/100\n",
            "187/187 [==============================] - 0s 878us/step - loss: 0.0275 - acc: 0.9786\n",
            "Epoch 86/100\n",
            "187/187 [==============================] - 0s 837us/step - loss: 0.0264 - acc: 0.9893\n",
            "Epoch 87/100\n",
            "187/187 [==============================] - 0s 859us/step - loss: 0.0270 - acc: 0.9786\n",
            "Epoch 88/100\n",
            "187/187 [==============================] - 0s 830us/step - loss: 0.0248 - acc: 0.9893\n",
            "Epoch 89/100\n",
            "187/187 [==============================] - 0s 794us/step - loss: 0.0246 - acc: 0.9893\n",
            "Epoch 90/100\n",
            "187/187 [==============================] - 0s 814us/step - loss: 0.0252 - acc: 0.9893\n",
            "Epoch 91/100\n",
            "187/187 [==============================] - 0s 855us/step - loss: 0.0231 - acc: 0.9840\n",
            "Epoch 92/100\n",
            "187/187 [==============================] - 0s 867us/step - loss: 0.0266 - acc: 0.9840\n",
            "Epoch 93/100\n",
            "187/187 [==============================] - 0s 778us/step - loss: 0.0225 - acc: 0.9840\n",
            "Epoch 94/100\n",
            "187/187 [==============================] - 0s 814us/step - loss: 0.0228 - acc: 0.9893\n",
            "Epoch 95/100\n",
            "187/187 [==============================] - 0s 784us/step - loss: 0.0210 - acc: 0.9893\n",
            "Epoch 96/100\n",
            "187/187 [==============================] - 0s 848us/step - loss: 0.0234 - acc: 0.9893\n",
            "Epoch 97/100\n",
            "187/187 [==============================] - 0s 816us/step - loss: 0.0203 - acc: 0.9893\n",
            "Epoch 98/100\n",
            "187/187 [==============================] - 0s 864us/step - loss: 0.0207 - acc: 0.9893\n",
            "Epoch 99/100\n",
            "187/187 [==============================] - 0s 883us/step - loss: 0.0195 - acc: 0.9893\n",
            "Epoch 100/100\n",
            "187/187 [==============================] - 0s 876us/step - loss: 0.0207 - acc: 0.9893\n",
            "21/21 [==============================] - 0s 10ms/step\n",
            "Epoch 1/100\n",
            "187/187 [==============================] - 1s 4ms/step - loss: 0.2592 - acc: 0.5027\n",
            "Epoch 2/100\n",
            "187/187 [==============================] - 0s 799us/step - loss: 0.2371 - acc: 0.7059\n",
            "Epoch 3/100\n",
            "187/187 [==============================] - 0s 848us/step - loss: 0.2295 - acc: 0.7059\n",
            "Epoch 4/100\n",
            "187/187 [==============================] - 0s 920us/step - loss: 0.2214 - acc: 0.7005\n",
            "Epoch 5/100\n",
            "187/187 [==============================] - 0s 789us/step - loss: 0.2156 - acc: 0.7166\n",
            "Epoch 6/100\n",
            "187/187 [==============================] - 0s 788us/step - loss: 0.2071 - acc: 0.7433\n",
            "Epoch 7/100\n",
            "187/187 [==============================] - 0s 819us/step - loss: 0.2002 - acc: 0.7112\n",
            "Epoch 8/100\n",
            "187/187 [==============================] - 0s 939us/step - loss: 0.1935 - acc: 0.7540\n",
            "Epoch 9/100\n",
            "187/187 [==============================] - 0s 802us/step - loss: 0.1860 - acc: 0.7166\n",
            "Epoch 10/100\n",
            "187/187 [==============================] - 0s 934us/step - loss: 0.1795 - acc: 0.7380\n",
            "Epoch 11/100\n",
            "187/187 [==============================] - 0s 818us/step - loss: 0.1757 - acc: 0.7647\n",
            "Epoch 12/100\n",
            "187/187 [==============================] - 0s 830us/step - loss: 0.1725 - acc: 0.7701\n",
            "Epoch 13/100\n",
            "187/187 [==============================] - 0s 817us/step - loss: 0.1659 - acc: 0.7701\n",
            "Epoch 14/100\n",
            "187/187 [==============================] - 0s 861us/step - loss: 0.1572 - acc: 0.7647\n",
            "Epoch 15/100\n",
            "187/187 [==============================] - 0s 830us/step - loss: 0.1518 - acc: 0.7861\n",
            "Epoch 16/100\n",
            "187/187 [==============================] - 0s 959us/step - loss: 0.1537 - acc: 0.7861\n",
            "Epoch 17/100\n",
            "187/187 [==============================] - 0s 885us/step - loss: 0.1558 - acc: 0.7807\n",
            "Epoch 18/100\n",
            "187/187 [==============================] - 0s 805us/step - loss: 0.1437 - acc: 0.7968\n",
            "Epoch 19/100\n",
            "187/187 [==============================] - 0s 842us/step - loss: 0.1454 - acc: 0.8075\n",
            "Epoch 20/100\n",
            "187/187 [==============================] - 0s 848us/step - loss: 0.1401 - acc: 0.7914\n",
            "Epoch 21/100\n",
            "187/187 [==============================] - 0s 858us/step - loss: 0.1364 - acc: 0.8235\n",
            "Epoch 22/100\n",
            "187/187 [==============================] - 0s 816us/step - loss: 0.1323 - acc: 0.8289\n",
            "Epoch 23/100\n",
            "187/187 [==============================] - 0s 938us/step - loss: 0.1306 - acc: 0.8342\n",
            "Epoch 24/100\n",
            "187/187 [==============================] - 0s 808us/step - loss: 0.1268 - acc: 0.8396\n",
            "Epoch 25/100\n",
            "187/187 [==============================] - 0s 925us/step - loss: 0.1259 - acc: 0.8235\n",
            "Epoch 26/100\n",
            "187/187 [==============================] - 0s 772us/step - loss: 0.1251 - acc: 0.8235\n",
            "Epoch 27/100\n",
            "187/187 [==============================] - 0s 959us/step - loss: 0.1209 - acc: 0.8503\n",
            "Epoch 28/100\n",
            "187/187 [==============================] - 0s 871us/step - loss: 0.1161 - acc: 0.8396\n",
            "Epoch 29/100\n",
            "187/187 [==============================] - 0s 1ms/step - loss: 0.1187 - acc: 0.8449\n",
            "Epoch 30/100\n",
            "187/187 [==============================] - 0s 786us/step - loss: 0.1135 - acc: 0.8396\n",
            "Epoch 31/100\n",
            "187/187 [==============================] - 0s 796us/step - loss: 0.1102 - acc: 0.8449\n",
            "Epoch 32/100\n",
            "187/187 [==============================] - 0s 838us/step - loss: 0.1175 - acc: 0.8396\n",
            "Epoch 33/100\n",
            "187/187 [==============================] - 0s 1ms/step - loss: 0.1078 - acc: 0.8610\n",
            "Epoch 34/100\n",
            "187/187 [==============================] - 0s 1ms/step - loss: 0.1050 - acc: 0.8556\n",
            "Epoch 35/100\n",
            "187/187 [==============================] - 0s 1ms/step - loss: 0.1073 - acc: 0.8449\n",
            "Epoch 36/100\n",
            "187/187 [==============================] - 0s 946us/step - loss: 0.1006 - acc: 0.8663\n",
            "Epoch 37/100\n",
            "187/187 [==============================] - 0s 825us/step - loss: 0.0992 - acc: 0.8824\n",
            "Epoch 38/100\n",
            "187/187 [==============================] - 0s 812us/step - loss: 0.1009 - acc: 0.8824\n",
            "Epoch 39/100\n",
            "187/187 [==============================] - 0s 809us/step - loss: 0.0978 - acc: 0.8717\n",
            "Epoch 40/100\n",
            "187/187 [==============================] - 0s 897us/step - loss: 0.0937 - acc: 0.8877\n",
            "Epoch 41/100\n",
            "187/187 [==============================] - 0s 961us/step - loss: 0.0940 - acc: 0.8770\n",
            "Epoch 42/100\n",
            "187/187 [==============================] - 0s 817us/step - loss: 0.0922 - acc: 0.8770\n",
            "Epoch 43/100\n",
            "187/187 [==============================] - 0s 820us/step - loss: 0.0894 - acc: 0.8770\n",
            "Epoch 44/100\n",
            "187/187 [==============================] - 0s 808us/step - loss: 0.0889 - acc: 0.8770\n",
            "Epoch 45/100\n",
            "187/187 [==============================] - 0s 952us/step - loss: 0.0870 - acc: 0.8930\n",
            "Epoch 46/100\n",
            "187/187 [==============================] - 0s 936us/step - loss: 0.0861 - acc: 0.8984\n",
            "Epoch 47/100\n",
            "187/187 [==============================] - 0s 944us/step - loss: 0.0913 - acc: 0.8824\n",
            "Epoch 48/100\n",
            "187/187 [==============================] - 0s 921us/step - loss: 0.0826 - acc: 0.8930\n",
            "Epoch 49/100\n",
            "187/187 [==============================] - 0s 824us/step - loss: 0.0813 - acc: 0.9198\n",
            "Epoch 50/100\n",
            "187/187 [==============================] - 0s 809us/step - loss: 0.0838 - acc: 0.8877\n",
            "Epoch 51/100\n",
            "187/187 [==============================] - 0s 872us/step - loss: 0.0768 - acc: 0.9037\n",
            "Epoch 52/100\n",
            "187/187 [==============================] - 0s 851us/step - loss: 0.0752 - acc: 0.9037\n",
            "Epoch 53/100\n",
            "187/187 [==============================] - 0s 863us/step - loss: 0.0704 - acc: 0.9198\n",
            "Epoch 54/100\n",
            "187/187 [==============================] - 0s 923us/step - loss: 0.0708 - acc: 0.9251\n",
            "Epoch 55/100\n",
            "187/187 [==============================] - 0s 924us/step - loss: 0.0704 - acc: 0.9091\n",
            "Epoch 56/100\n",
            "187/187 [==============================] - 0s 950us/step - loss: 0.0668 - acc: 0.9412\n",
            "Epoch 57/100\n",
            "187/187 [==============================] - 0s 975us/step - loss: 0.0697 - acc: 0.9358\n",
            "Epoch 58/100\n",
            "187/187 [==============================] - 0s 805us/step - loss: 0.0687 - acc: 0.9305\n",
            "Epoch 59/100\n",
            "187/187 [==============================] - 0s 926us/step - loss: 0.0650 - acc: 0.9251\n",
            "Epoch 60/100\n",
            "187/187 [==============================] - 0s 765us/step - loss: 0.0607 - acc: 0.9412\n",
            "Epoch 61/100\n",
            "187/187 [==============================] - 0s 837us/step - loss: 0.0597 - acc: 0.9412\n",
            "Epoch 62/100\n",
            "187/187 [==============================] - 0s 842us/step - loss: 0.0622 - acc: 0.9358\n",
            "Epoch 63/100\n",
            "187/187 [==============================] - 0s 824us/step - loss: 0.0569 - acc: 0.9465\n",
            "Epoch 64/100\n",
            "187/187 [==============================] - 0s 904us/step - loss: 0.0564 - acc: 0.9626\n",
            "Epoch 65/100\n",
            "187/187 [==============================] - 0s 937us/step - loss: 0.0559 - acc: 0.9572\n",
            "Epoch 66/100\n",
            "187/187 [==============================] - 0s 916us/step - loss: 0.0552 - acc: 0.9412\n",
            "Epoch 67/100\n",
            "187/187 [==============================] - 0s 791us/step - loss: 0.0551 - acc: 0.9519\n",
            "Epoch 68/100\n",
            "187/187 [==============================] - 0s 869us/step - loss: 0.0495 - acc: 0.9679\n",
            "Epoch 69/100\n",
            "187/187 [==============================] - 0s 915us/step - loss: 0.0480 - acc: 0.9572\n",
            "Epoch 70/100\n",
            "187/187 [==============================] - 0s 755us/step - loss: 0.0482 - acc: 0.9626\n",
            "Epoch 71/100\n",
            "187/187 [==============================] - 0s 827us/step - loss: 0.0477 - acc: 0.9572\n",
            "Epoch 72/100\n",
            "187/187 [==============================] - 0s 850us/step - loss: 0.0451 - acc: 0.9626\n",
            "Epoch 73/100\n",
            "187/187 [==============================] - 0s 807us/step - loss: 0.0429 - acc: 0.9679\n",
            "Epoch 74/100\n",
            "187/187 [==============================] - 0s 908us/step - loss: 0.0423 - acc: 0.9679\n",
            "Epoch 75/100\n",
            "187/187 [==============================] - 0s 950us/step - loss: 0.0429 - acc: 0.9733\n",
            "Epoch 76/100\n",
            "187/187 [==============================] - 0s 798us/step - loss: 0.0424 - acc: 0.9733\n",
            "Epoch 77/100\n",
            "187/187 [==============================] - 0s 951us/step - loss: 0.0399 - acc: 0.9733\n",
            "Epoch 78/100\n",
            "187/187 [==============================] - 0s 937us/step - loss: 0.0387 - acc: 0.9733\n",
            "Epoch 79/100\n",
            "187/187 [==============================] - 0s 846us/step - loss: 0.0379 - acc: 0.9733\n",
            "Epoch 80/100\n",
            "187/187 [==============================] - 0s 960us/step - loss: 0.0379 - acc: 0.9733\n",
            "Epoch 81/100\n",
            "187/187 [==============================] - 0s 817us/step - loss: 0.0366 - acc: 0.9733\n",
            "Epoch 82/100\n",
            "187/187 [==============================] - 0s 971us/step - loss: 0.0369 - acc: 0.9786\n",
            "Epoch 83/100\n",
            "187/187 [==============================] - 0s 957us/step - loss: 0.0355 - acc: 0.9733\n",
            "Epoch 84/100\n",
            "187/187 [==============================] - 0s 954us/step - loss: 0.0342 - acc: 0.9679\n",
            "Epoch 85/100\n",
            "187/187 [==============================] - 0s 944us/step - loss: 0.0354 - acc: 0.9786\n",
            "Epoch 86/100\n",
            "187/187 [==============================] - 0s 848us/step - loss: 0.0332 - acc: 0.9840\n",
            "Epoch 87/100\n",
            "187/187 [==============================] - 0s 815us/step - loss: 0.0340 - acc: 0.9679\n",
            "Epoch 88/100\n",
            "187/187 [==============================] - 0s 866us/step - loss: 0.0318 - acc: 0.9786\n",
            "Epoch 89/100\n",
            "187/187 [==============================] - 0s 775us/step - loss: 0.0334 - acc: 0.9733\n",
            "Epoch 90/100\n",
            "187/187 [==============================] - 0s 989us/step - loss: 0.0307 - acc: 0.9840\n",
            "Epoch 91/100\n",
            "187/187 [==============================] - 0s 916us/step - loss: 0.0297 - acc: 0.9786\n",
            "Epoch 92/100\n",
            "187/187 [==============================] - 0s 858us/step - loss: 0.0290 - acc: 0.9786\n",
            "Epoch 93/100\n",
            "187/187 [==============================] - 0s 823us/step - loss: 0.0286 - acc: 0.9786\n",
            "Epoch 94/100\n",
            "187/187 [==============================] - 0s 773us/step - loss: 0.0317 - acc: 0.9733\n",
            "Epoch 95/100\n",
            "187/187 [==============================] - 0s 851us/step - loss: 0.0288 - acc: 0.9786\n",
            "Epoch 96/100\n",
            "187/187 [==============================] - 0s 803us/step - loss: 0.0282 - acc: 0.9786\n",
            "Epoch 97/100\n",
            "187/187 [==============================] - 0s 908us/step - loss: 0.0281 - acc: 0.9733\n",
            "Epoch 98/100\n",
            "187/187 [==============================] - 0s 788us/step - loss: 0.0273 - acc: 0.9840\n",
            "Epoch 99/100\n",
            "187/187 [==============================] - 0s 816us/step - loss: 0.0255 - acc: 0.9840\n",
            "Epoch 100/100\n",
            "187/187 [==============================] - 0s 849us/step - loss: 0.0269 - acc: 0.9733\n",
            "21/21 [==============================] - 0s 10ms/step\n",
            "Epoch 1/100\n",
            "187/187 [==============================] - 1s 4ms/step - loss: 0.2422 - acc: 0.5455\n",
            "Epoch 2/100\n",
            "187/187 [==============================] - 0s 807us/step - loss: 0.2340 - acc: 0.5508\n",
            "Epoch 3/100\n",
            "187/187 [==============================] - 0s 866us/step - loss: 0.2261 - acc: 0.5882\n",
            "Epoch 4/100\n",
            "187/187 [==============================] - 0s 809us/step - loss: 0.2193 - acc: 0.6738\n",
            "Epoch 5/100\n",
            "187/187 [==============================] - 0s 797us/step - loss: 0.2080 - acc: 0.7433\n",
            "Epoch 6/100\n",
            "187/187 [==============================] - 0s 870us/step - loss: 0.2009 - acc: 0.7219\n",
            "Epoch 7/100\n",
            "187/187 [==============================] - 0s 801us/step - loss: 0.1903 - acc: 0.7754\n",
            "Epoch 8/100\n",
            "187/187 [==============================] - 0s 755us/step - loss: 0.1808 - acc: 0.7914\n",
            "Epoch 9/100\n",
            "187/187 [==============================] - 0s 827us/step - loss: 0.1692 - acc: 0.8396\n",
            "Epoch 10/100\n",
            "187/187 [==============================] - 0s 854us/step - loss: 0.1643 - acc: 0.7914\n",
            "Epoch 11/100\n",
            "187/187 [==============================] - 0s 932us/step - loss: 0.1542 - acc: 0.8075\n",
            "Epoch 12/100\n",
            "187/187 [==============================] - 0s 853us/step - loss: 0.1479 - acc: 0.8289\n",
            "Epoch 13/100\n",
            "187/187 [==============================] - 0s 798us/step - loss: 0.1527 - acc: 0.7701\n",
            "Epoch 14/100\n",
            "187/187 [==============================] - 0s 989us/step - loss: 0.1406 - acc: 0.8182\n",
            "Epoch 15/100\n",
            "187/187 [==============================] - 0s 925us/step - loss: 0.1387 - acc: 0.8289\n",
            "Epoch 16/100\n",
            "187/187 [==============================] - 0s 929us/step - loss: 0.1349 - acc: 0.8128\n",
            "Epoch 17/100\n",
            "187/187 [==============================] - 0s 966us/step - loss: 0.1345 - acc: 0.8289\n",
            "Epoch 18/100\n",
            "187/187 [==============================] - 0s 802us/step - loss: 0.1274 - acc: 0.8503\n",
            "Epoch 19/100\n",
            "187/187 [==============================] - 0s 817us/step - loss: 0.1218 - acc: 0.8663\n",
            "Epoch 20/100\n",
            "187/187 [==============================] - 0s 807us/step - loss: 0.1232 - acc: 0.8449\n",
            "Epoch 21/100\n",
            "187/187 [==============================] - 0s 1ms/step - loss: 0.1169 - acc: 0.8556\n",
            "Epoch 22/100\n",
            "187/187 [==============================] - 0s 830us/step - loss: 0.1127 - acc: 0.8503\n",
            "Epoch 23/100\n",
            "187/187 [==============================] - 0s 949us/step - loss: 0.1133 - acc: 0.8503\n",
            "Epoch 24/100\n",
            "187/187 [==============================] - 0s 826us/step - loss: 0.1071 - acc: 0.8663\n",
            "Epoch 25/100\n",
            "187/187 [==============================] - 0s 803us/step - loss: 0.1051 - acc: 0.8770\n",
            "Epoch 26/100\n",
            "187/187 [==============================] - 0s 925us/step - loss: 0.1078 - acc: 0.8449\n",
            "Epoch 27/100\n",
            "187/187 [==============================] - 0s 875us/step - loss: 0.1037 - acc: 0.8824\n",
            "Epoch 28/100\n",
            "187/187 [==============================] - 0s 781us/step - loss: 0.0982 - acc: 0.8717\n",
            "Epoch 29/100\n",
            "187/187 [==============================] - 0s 871us/step - loss: 0.0952 - acc: 0.8824\n",
            "Epoch 30/100\n",
            "187/187 [==============================] - 0s 805us/step - loss: 0.0945 - acc: 0.8984\n",
            "Epoch 31/100\n",
            "187/187 [==============================] - 0s 853us/step - loss: 0.0953 - acc: 0.8877\n",
            "Epoch 32/100\n",
            "187/187 [==============================] - 0s 843us/step - loss: 0.0870 - acc: 0.9037\n",
            "Epoch 33/100\n",
            "187/187 [==============================] - 0s 799us/step - loss: 0.0873 - acc: 0.9037\n",
            "Epoch 34/100\n",
            "187/187 [==============================] - 0s 879us/step - loss: 0.0853 - acc: 0.8877\n",
            "Epoch 35/100\n",
            "187/187 [==============================] - 0s 839us/step - loss: 0.0829 - acc: 0.9091\n",
            "Epoch 36/100\n",
            "187/187 [==============================] - 0s 801us/step - loss: 0.0801 - acc: 0.9144\n",
            "Epoch 37/100\n",
            "187/187 [==============================] - 0s 813us/step - loss: 0.0785 - acc: 0.9144\n",
            "Epoch 38/100\n",
            "187/187 [==============================] - 0s 781us/step - loss: 0.0766 - acc: 0.9091\n",
            "Epoch 39/100\n",
            "187/187 [==============================] - 0s 777us/step - loss: 0.0841 - acc: 0.8984\n",
            "Epoch 40/100\n",
            "187/187 [==============================] - 0s 768us/step - loss: 0.0750 - acc: 0.9198\n",
            "Epoch 41/100\n",
            "187/187 [==============================] - 0s 903us/step - loss: 0.0710 - acc: 0.9144\n",
            "Epoch 42/100\n",
            "187/187 [==============================] - 0s 812us/step - loss: 0.0670 - acc: 0.9412\n",
            "Epoch 43/100\n",
            "187/187 [==============================] - 0s 856us/step - loss: 0.0696 - acc: 0.9037\n",
            "Epoch 44/100\n",
            "187/187 [==============================] - 0s 817us/step - loss: 0.0689 - acc: 0.9091\n",
            "Epoch 45/100\n",
            "187/187 [==============================] - 0s 799us/step - loss: 0.0660 - acc: 0.9251\n",
            "Epoch 46/100\n",
            "187/187 [==============================] - 0s 875us/step - loss: 0.0625 - acc: 0.9251\n",
            "Epoch 47/100\n",
            "187/187 [==============================] - 0s 916us/step - loss: 0.0683 - acc: 0.9198\n",
            "Epoch 48/100\n",
            "187/187 [==============================] - 0s 778us/step - loss: 0.0596 - acc: 0.9358\n",
            "Epoch 49/100\n",
            "187/187 [==============================] - 0s 796us/step - loss: 0.0594 - acc: 0.9358\n",
            "Epoch 50/100\n",
            "187/187 [==============================] - 0s 785us/step - loss: 0.0564 - acc: 0.9412\n",
            "Epoch 51/100\n",
            "187/187 [==============================] - 0s 774us/step - loss: 0.0556 - acc: 0.9412\n",
            "Epoch 52/100\n",
            "187/187 [==============================] - 0s 794us/step - loss: 0.0567 - acc: 0.9358\n",
            "Epoch 53/100\n",
            "187/187 [==============================] - 0s 815us/step - loss: 0.0512 - acc: 0.9626\n",
            "Epoch 54/100\n",
            "187/187 [==============================] - 0s 841us/step - loss: 0.0514 - acc: 0.9465\n",
            "Epoch 55/100\n",
            "187/187 [==============================] - 0s 848us/step - loss: 0.0532 - acc: 0.9305\n",
            "Epoch 56/100\n",
            "187/187 [==============================] - 0s 802us/step - loss: 0.0540 - acc: 0.9412\n",
            "Epoch 57/100\n",
            "187/187 [==============================] - 0s 844us/step - loss: 0.0488 - acc: 0.9572\n",
            "Epoch 58/100\n",
            "187/187 [==============================] - 0s 930us/step - loss: 0.0431 - acc: 0.9626\n",
            "Epoch 59/100\n",
            "187/187 [==============================] - 0s 928us/step - loss: 0.0452 - acc: 0.9626\n",
            "Epoch 60/100\n",
            "187/187 [==============================] - 0s 976us/step - loss: 0.0416 - acc: 0.9519\n",
            "Epoch 61/100\n",
            "187/187 [==============================] - 0s 810us/step - loss: 0.0446 - acc: 0.9465\n",
            "Epoch 62/100\n",
            "187/187 [==============================] - 0s 944us/step - loss: 0.0402 - acc: 0.9572\n",
            "Epoch 63/100\n",
            "187/187 [==============================] - 0s 840us/step - loss: 0.0388 - acc: 0.9679\n",
            "Epoch 64/100\n",
            "187/187 [==============================] - 0s 862us/step - loss: 0.0419 - acc: 0.9679\n",
            "Epoch 65/100\n",
            "187/187 [==============================] - 0s 770us/step - loss: 0.0372 - acc: 0.9733\n",
            "Epoch 66/100\n",
            "187/187 [==============================] - 0s 852us/step - loss: 0.0378 - acc: 0.9733\n",
            "Epoch 67/100\n",
            "187/187 [==============================] - 0s 859us/step - loss: 0.0330 - acc: 0.9733\n",
            "Epoch 68/100\n",
            "187/187 [==============================] - 0s 790us/step - loss: 0.0327 - acc: 0.9733\n",
            "Epoch 69/100\n",
            "187/187 [==============================] - 0s 797us/step - loss: 0.0322 - acc: 0.9786\n",
            "Epoch 70/100\n",
            "187/187 [==============================] - 0s 840us/step - loss: 0.0327 - acc: 0.9786\n",
            "Epoch 71/100\n",
            "187/187 [==============================] - 0s 799us/step - loss: 0.0329 - acc: 0.9626\n",
            "Epoch 72/100\n",
            "187/187 [==============================] - 0s 827us/step - loss: 0.0295 - acc: 0.9786\n",
            "Epoch 73/100\n",
            "187/187 [==============================] - 0s 870us/step - loss: 0.0270 - acc: 0.9733\n",
            "Epoch 74/100\n",
            "187/187 [==============================] - 0s 800us/step - loss: 0.0295 - acc: 0.9733\n",
            "Epoch 75/100\n",
            "187/187 [==============================] - 0s 862us/step - loss: 0.0284 - acc: 0.9733\n",
            "Epoch 76/100\n",
            "187/187 [==============================] - 0s 912us/step - loss: 0.0288 - acc: 0.9786\n",
            "Epoch 77/100\n",
            "187/187 [==============================] - 0s 815us/step - loss: 0.0272 - acc: 0.9733\n",
            "Epoch 78/100\n",
            "187/187 [==============================] - 0s 821us/step - loss: 0.0265 - acc: 0.9840\n",
            "Epoch 79/100\n",
            "187/187 [==============================] - 0s 1ms/step - loss: 0.0247 - acc: 0.9733\n",
            "Epoch 80/100\n",
            "187/187 [==============================] - 0s 831us/step - loss: 0.0223 - acc: 0.9893\n",
            "Epoch 81/100\n",
            "187/187 [==============================] - 0s 828us/step - loss: 0.0203 - acc: 0.9893\n",
            "Epoch 82/100\n",
            "187/187 [==============================] - 0s 811us/step - loss: 0.0214 - acc: 0.9786\n",
            "Epoch 83/100\n",
            "187/187 [==============================] - 0s 818us/step - loss: 0.0225 - acc: 0.9840\n",
            "Epoch 84/100\n",
            "187/187 [==============================] - 0s 802us/step - loss: 0.0212 - acc: 0.9893\n",
            "Epoch 85/100\n",
            "187/187 [==============================] - 0s 892us/step - loss: 0.0177 - acc: 0.9947\n",
            "Epoch 86/100\n",
            "187/187 [==============================] - 0s 841us/step - loss: 0.0174 - acc: 1.0000\n",
            "Epoch 87/100\n",
            "187/187 [==============================] - 0s 836us/step - loss: 0.0156 - acc: 0.9947\n",
            "Epoch 88/100\n",
            "187/187 [==============================] - 0s 872us/step - loss: 0.0185 - acc: 0.9893\n",
            "Epoch 89/100\n",
            "187/187 [==============================] - 0s 772us/step - loss: 0.0179 - acc: 0.9893\n",
            "Epoch 90/100\n",
            "187/187 [==============================] - 0s 805us/step - loss: 0.0164 - acc: 0.9893\n",
            "Epoch 91/100\n",
            "187/187 [==============================] - 0s 829us/step - loss: 0.0142 - acc: 1.0000\n",
            "Epoch 92/100\n",
            "187/187 [==============================] - 0s 915us/step - loss: 0.0135 - acc: 1.0000\n",
            "Epoch 93/100\n",
            "187/187 [==============================] - 0s 842us/step - loss: 0.0123 - acc: 1.0000\n",
            "Epoch 94/100\n",
            "187/187 [==============================] - 0s 769us/step - loss: 0.0129 - acc: 1.0000\n",
            "Epoch 95/100\n",
            "187/187 [==============================] - 0s 860us/step - loss: 0.0126 - acc: 0.9947\n",
            "Epoch 96/100\n",
            "187/187 [==============================] - 0s 791us/step - loss: 0.0142 - acc: 0.9947\n",
            "Epoch 97/100\n",
            "187/187 [==============================] - 0s 796us/step - loss: 0.0116 - acc: 1.0000\n",
            "Epoch 98/100\n",
            "187/187 [==============================] - 0s 829us/step - loss: 0.0120 - acc: 1.0000\n",
            "Epoch 99/100\n",
            "187/187 [==============================] - 0s 923us/step - loss: 0.0116 - acc: 1.0000\n",
            "Epoch 100/100\n",
            "187/187 [==============================] - 0s 842us/step - loss: 0.0107 - acc: 1.0000\n",
            "21/21 [==============================] - 0s 10ms/step\n",
            "Epoch 1/100\n",
            "187/187 [==============================] - 1s 5ms/step - loss: 0.2476 - acc: 0.5615\n",
            "Epoch 2/100\n",
            "187/187 [==============================] - 0s 799us/step - loss: 0.2410 - acc: 0.5722\n",
            "Epoch 3/100\n",
            "187/187 [==============================] - 0s 828us/step - loss: 0.2354 - acc: 0.5561\n",
            "Epoch 4/100\n",
            "187/187 [==============================] - 0s 938us/step - loss: 0.2265 - acc: 0.6631\n",
            "Epoch 5/100\n",
            "187/187 [==============================] - 0s 973us/step - loss: 0.2206 - acc: 0.6524\n",
            "Epoch 6/100\n",
            "187/187 [==============================] - 0s 837us/step - loss: 0.2117 - acc: 0.7326\n",
            "Epoch 7/100\n",
            "187/187 [==============================] - 0s 900us/step - loss: 0.2039 - acc: 0.7005\n",
            "Epoch 8/100\n",
            "187/187 [==============================] - 0s 944us/step - loss: 0.1946 - acc: 0.7647\n",
            "Epoch 9/100\n",
            "187/187 [==============================] - 0s 791us/step - loss: 0.1864 - acc: 0.7914\n",
            "Epoch 10/100\n",
            "187/187 [==============================] - 0s 902us/step - loss: 0.1765 - acc: 0.7647\n",
            "Epoch 11/100\n",
            "187/187 [==============================] - 0s 786us/step - loss: 0.1713 - acc: 0.8235\n",
            "Epoch 12/100\n",
            "187/187 [==============================] - 0s 787us/step - loss: 0.1641 - acc: 0.8021\n",
            "Epoch 13/100\n",
            "187/187 [==============================] - 0s 777us/step - loss: 0.1586 - acc: 0.8235\n",
            "Epoch 14/100\n",
            "187/187 [==============================] - 0s 789us/step - loss: 0.1487 - acc: 0.8289\n",
            "Epoch 15/100\n",
            "187/187 [==============================] - 0s 928us/step - loss: 0.1484 - acc: 0.8021\n",
            "Epoch 16/100\n",
            "187/187 [==============================] - 0s 931us/step - loss: 0.1471 - acc: 0.8235\n",
            "Epoch 17/100\n",
            "187/187 [==============================] - 0s 808us/step - loss: 0.1355 - acc: 0.8128\n",
            "Epoch 18/100\n",
            "187/187 [==============================] - 0s 790us/step - loss: 0.1315 - acc: 0.8289\n",
            "Epoch 19/100\n",
            "187/187 [==============================] - 0s 820us/step - loss: 0.1318 - acc: 0.8449\n",
            "Epoch 20/100\n",
            "187/187 [==============================] - 0s 804us/step - loss: 0.1226 - acc: 0.8396\n",
            "Epoch 21/100\n",
            "187/187 [==============================] - 0s 790us/step - loss: 0.1204 - acc: 0.8663\n",
            "Epoch 22/100\n",
            "187/187 [==============================] - 0s 918us/step - loss: 0.1138 - acc: 0.8556\n",
            "Epoch 23/100\n",
            "187/187 [==============================] - 0s 874us/step - loss: 0.1172 - acc: 0.8235\n",
            "Epoch 24/100\n",
            "187/187 [==============================] - 0s 811us/step - loss: 0.1080 - acc: 0.8663\n",
            "Epoch 25/100\n",
            "187/187 [==============================] - 0s 825us/step - loss: 0.1066 - acc: 0.8770\n",
            "Epoch 26/100\n",
            "187/187 [==============================] - 0s 861us/step - loss: 0.1107 - acc: 0.8556\n",
            "Epoch 27/100\n",
            "187/187 [==============================] - 0s 817us/step - loss: 0.1047 - acc: 0.8663\n",
            "Epoch 28/100\n",
            "187/187 [==============================] - 0s 923us/step - loss: 0.1016 - acc: 0.8770\n",
            "Epoch 29/100\n",
            "187/187 [==============================] - 0s 862us/step - loss: 0.1014 - acc: 0.8717\n",
            "Epoch 30/100\n",
            "187/187 [==============================] - 0s 851us/step - loss: 0.0957 - acc: 0.8984\n",
            "Epoch 31/100\n",
            "187/187 [==============================] - 0s 793us/step - loss: 0.0903 - acc: 0.8930\n",
            "Epoch 32/100\n",
            "187/187 [==============================] - 0s 833us/step - loss: 0.0898 - acc: 0.8930\n",
            "Epoch 33/100\n",
            "187/187 [==============================] - 0s 818us/step - loss: 0.0912 - acc: 0.8930\n",
            "Epoch 34/100\n",
            "187/187 [==============================] - 0s 780us/step - loss: 0.0887 - acc: 0.8930\n",
            "Epoch 35/100\n",
            "187/187 [==============================] - 0s 902us/step - loss: 0.0863 - acc: 0.8877\n",
            "Epoch 36/100\n",
            "187/187 [==============================] - 0s 1ms/step - loss: 0.0834 - acc: 0.9091\n",
            "Epoch 37/100\n",
            "187/187 [==============================] - 0s 990us/step - loss: 0.0816 - acc: 0.9091\n",
            "Epoch 38/100\n",
            "187/187 [==============================] - 0s 828us/step - loss: 0.0798 - acc: 0.8984\n",
            "Epoch 39/100\n",
            "187/187 [==============================] - 0s 819us/step - loss: 0.0789 - acc: 0.8984\n",
            "Epoch 40/100\n",
            "187/187 [==============================] - 0s 792us/step - loss: 0.0761 - acc: 0.9144\n",
            "Epoch 41/100\n",
            "187/187 [==============================] - 0s 813us/step - loss: 0.0770 - acc: 0.9037\n",
            "Epoch 42/100\n",
            "187/187 [==============================] - 0s 931us/step - loss: 0.0739 - acc: 0.9198\n",
            "Epoch 43/100\n",
            "187/187 [==============================] - 0s 802us/step - loss: 0.0773 - acc: 0.9091\n",
            "Epoch 44/100\n",
            "187/187 [==============================] - 0s 815us/step - loss: 0.0700 - acc: 0.9251\n",
            "Epoch 45/100\n",
            "187/187 [==============================] - 0s 817us/step - loss: 0.0656 - acc: 0.9251\n",
            "Epoch 46/100\n",
            "187/187 [==============================] - 0s 796us/step - loss: 0.0729 - acc: 0.9144\n",
            "Epoch 47/100\n",
            "187/187 [==============================] - 0s 889us/step - loss: 0.0661 - acc: 0.9144\n",
            "Epoch 48/100\n",
            "187/187 [==============================] - 0s 927us/step - loss: 0.0617 - acc: 0.9465\n",
            "Epoch 49/100\n",
            "187/187 [==============================] - 0s 846us/step - loss: 0.0618 - acc: 0.9358\n",
            "Epoch 50/100\n",
            "187/187 [==============================] - 0s 832us/step - loss: 0.0586 - acc: 0.9412\n",
            "Epoch 51/100\n",
            "187/187 [==============================] - 0s 849us/step - loss: 0.0640 - acc: 0.9465\n",
            "Epoch 52/100\n",
            "187/187 [==============================] - 0s 951us/step - loss: 0.0567 - acc: 0.9412\n",
            "Epoch 53/100\n",
            "187/187 [==============================] - 0s 972us/step - loss: 0.0558 - acc: 0.9465\n",
            "Epoch 54/100\n",
            "187/187 [==============================] - 0s 920us/step - loss: 0.0550 - acc: 0.9358\n",
            "Epoch 55/100\n",
            "187/187 [==============================] - 0s 806us/step - loss: 0.0565 - acc: 0.9358\n",
            "Epoch 56/100\n",
            "187/187 [==============================] - 0s 855us/step - loss: 0.0513 - acc: 0.9519\n",
            "Epoch 57/100\n",
            "187/187 [==============================] - 0s 789us/step - loss: 0.0497 - acc: 0.9519\n",
            "Epoch 58/100\n",
            "187/187 [==============================] - 0s 788us/step - loss: 0.0505 - acc: 0.9412\n",
            "Epoch 59/100\n",
            "187/187 [==============================] - 0s 825us/step - loss: 0.0591 - acc: 0.9198\n",
            "Epoch 60/100\n",
            "187/187 [==============================] - 0s 817us/step - loss: 0.0503 - acc: 0.9358\n",
            "Epoch 61/100\n",
            "187/187 [==============================] - 0s 962us/step - loss: 0.0463 - acc: 0.9465\n",
            "Epoch 62/100\n",
            "187/187 [==============================] - 0s 963us/step - loss: 0.0434 - acc: 0.9572\n",
            "Epoch 63/100\n",
            "187/187 [==============================] - 0s 798us/step - loss: 0.0459 - acc: 0.9465\n",
            "Epoch 64/100\n",
            "187/187 [==============================] - 0s 866us/step - loss: 0.0430 - acc: 0.9519\n",
            "Epoch 65/100\n",
            "187/187 [==============================] - 0s 797us/step - loss: 0.0405 - acc: 0.9572\n",
            "Epoch 66/100\n",
            "187/187 [==============================] - 0s 869us/step - loss: 0.0373 - acc: 0.9679\n",
            "Epoch 67/100\n",
            "187/187 [==============================] - 0s 953us/step - loss: 0.0415 - acc: 0.9519\n",
            "Epoch 68/100\n",
            "187/187 [==============================] - 0s 802us/step - loss: 0.0373 - acc: 0.9519\n",
            "Epoch 69/100\n",
            "187/187 [==============================] - 0s 823us/step - loss: 0.0425 - acc: 0.9465\n",
            "Epoch 70/100\n",
            "187/187 [==============================] - 0s 801us/step - loss: 0.0356 - acc: 0.9733\n",
            "Epoch 71/100\n",
            "187/187 [==============================] - 0s 835us/step - loss: 0.0352 - acc: 0.9786\n",
            "Epoch 72/100\n",
            "187/187 [==============================] - 0s 853us/step - loss: 0.0370 - acc: 0.9786\n",
            "Epoch 73/100\n",
            "187/187 [==============================] - 0s 965us/step - loss: 0.0351 - acc: 0.9626\n",
            "Epoch 74/100\n",
            "187/187 [==============================] - 0s 822us/step - loss: 0.0368 - acc: 0.9572\n",
            "Epoch 75/100\n",
            "187/187 [==============================] - 0s 829us/step - loss: 0.0320 - acc: 0.9786\n",
            "Epoch 76/100\n",
            "187/187 [==============================] - 0s 812us/step - loss: 0.0327 - acc: 0.9733\n",
            "Epoch 77/100\n",
            "187/187 [==============================] - 0s 841us/step - loss: 0.0308 - acc: 0.9733\n",
            "Epoch 78/100\n",
            "187/187 [==============================] - 0s 845us/step - loss: 0.0306 - acc: 0.9679\n",
            "Epoch 79/100\n",
            "187/187 [==============================] - 0s 838us/step - loss: 0.0275 - acc: 0.9840\n",
            "Epoch 80/100\n",
            "187/187 [==============================] - 0s 948us/step - loss: 0.0273 - acc: 0.9786\n",
            "Epoch 81/100\n",
            "187/187 [==============================] - 0s 961us/step - loss: 0.0298 - acc: 0.9733\n",
            "Epoch 82/100\n",
            "187/187 [==============================] - 0s 959us/step - loss: 0.0305 - acc: 0.9733\n",
            "Epoch 83/100\n",
            "187/187 [==============================] - 0s 874us/step - loss: 0.0257 - acc: 0.9840\n",
            "Epoch 84/100\n",
            "187/187 [==============================] - 0s 903us/step - loss: 0.0235 - acc: 0.9840\n",
            "Epoch 85/100\n",
            "187/187 [==============================] - 0s 929us/step - loss: 0.0238 - acc: 0.9840\n",
            "Epoch 86/100\n",
            "187/187 [==============================] - 0s 941us/step - loss: 0.0231 - acc: 0.9893\n",
            "Epoch 87/100\n",
            "187/187 [==============================] - 0s 917us/step - loss: 0.0210 - acc: 0.9893\n",
            "Epoch 88/100\n",
            "187/187 [==============================] - 0s 864us/step - loss: 0.0252 - acc: 0.9786\n",
            "Epoch 89/100\n",
            "187/187 [==============================] - 0s 931us/step - loss: 0.0217 - acc: 0.9893\n",
            "Epoch 90/100\n",
            "187/187 [==============================] - 0s 836us/step - loss: 0.0186 - acc: 0.9893\n",
            "Epoch 91/100\n",
            "187/187 [==============================] - 0s 865us/step - loss: 0.0205 - acc: 0.9893\n",
            "Epoch 92/100\n",
            "187/187 [==============================] - 0s 968us/step - loss: 0.0196 - acc: 0.9840\n",
            "Epoch 93/100\n",
            "187/187 [==============================] - 0s 859us/step - loss: 0.0179 - acc: 0.9893\n",
            "Epoch 94/100\n",
            "187/187 [==============================] - 0s 918us/step - loss: 0.0168 - acc: 0.9893\n",
            "Epoch 95/100\n",
            "187/187 [==============================] - 0s 874us/step - loss: 0.0172 - acc: 0.9840\n",
            "Epoch 96/100\n",
            "187/187 [==============================] - 0s 877us/step - loss: 0.0147 - acc: 0.9947\n",
            "Epoch 97/100\n",
            "187/187 [==============================] - 0s 821us/step - loss: 0.0140 - acc: 0.9947\n",
            "Epoch 98/100\n",
            "187/187 [==============================] - 0s 859us/step - loss: 0.0133 - acc: 0.9947\n",
            "Epoch 99/100\n",
            "187/187 [==============================] - 0s 809us/step - loss: 0.0124 - acc: 0.9947\n",
            "Epoch 100/100\n",
            "187/187 [==============================] - 0s 952us/step - loss: 0.0124 - acc: 0.9947\n",
            "21/21 [==============================] - 0s 11ms/step\n",
            "Epoch 1/100\n",
            "187/187 [==============================] - 1s 5ms/step - loss: 0.2559 - acc: 0.4920\n",
            "Epoch 2/100\n",
            "187/187 [==============================] - 0s 820us/step - loss: 0.2387 - acc: 0.6096\n",
            "Epoch 3/100\n",
            "187/187 [==============================] - 0s 976us/step - loss: 0.2300 - acc: 0.5882\n",
            "Epoch 4/100\n",
            "187/187 [==============================] - 0s 944us/step - loss: 0.2208 - acc: 0.6738\n",
            "Epoch 5/100\n",
            "187/187 [==============================] - 0s 840us/step - loss: 0.2101 - acc: 0.7273\n",
            "Epoch 6/100\n",
            "187/187 [==============================] - 0s 810us/step - loss: 0.1940 - acc: 0.7380\n",
            "Epoch 7/100\n",
            "187/187 [==============================] - 0s 818us/step - loss: 0.1824 - acc: 0.7594\n",
            "Epoch 8/100\n",
            "187/187 [==============================] - 0s 788us/step - loss: 0.1731 - acc: 0.7914\n",
            "Epoch 9/100\n",
            "187/187 [==============================] - 0s 932us/step - loss: 0.1713 - acc: 0.7861\n",
            "Epoch 10/100\n",
            "187/187 [==============================] - 0s 870us/step - loss: 0.1603 - acc: 0.7968\n",
            "Epoch 11/100\n",
            "187/187 [==============================] - 0s 980us/step - loss: 0.1555 - acc: 0.7968\n",
            "Epoch 12/100\n",
            "187/187 [==============================] - 0s 831us/step - loss: 0.1524 - acc: 0.7861\n",
            "Epoch 13/100\n",
            "187/187 [==============================] - 0s 777us/step - loss: 0.1489 - acc: 0.8182\n",
            "Epoch 14/100\n",
            "187/187 [==============================] - 0s 913us/step - loss: 0.1448 - acc: 0.8182\n",
            "Epoch 15/100\n",
            "187/187 [==============================] - 0s 932us/step - loss: 0.1358 - acc: 0.8075\n",
            "Epoch 16/100\n",
            "187/187 [==============================] - 0s 927us/step - loss: 0.1345 - acc: 0.8021\n",
            "Epoch 17/100\n",
            "187/187 [==============================] - 0s 815us/step - loss: 0.1441 - acc: 0.8075\n",
            "Epoch 18/100\n",
            "187/187 [==============================] - 0s 830us/step - loss: 0.1334 - acc: 0.8075\n",
            "Epoch 19/100\n",
            "187/187 [==============================] - 0s 804us/step - loss: 0.1323 - acc: 0.8128\n",
            "Epoch 20/100\n",
            "187/187 [==============================] - 0s 814us/step - loss: 0.1274 - acc: 0.8289\n",
            "Epoch 21/100\n",
            "187/187 [==============================] - 0s 871us/step - loss: 0.1264 - acc: 0.8342\n",
            "Epoch 22/100\n",
            "187/187 [==============================] - 0s 824us/step - loss: 0.1216 - acc: 0.8449\n",
            "Epoch 23/100\n",
            "187/187 [==============================] - 0s 957us/step - loss: 0.1255 - acc: 0.8235\n",
            "Epoch 24/100\n",
            "187/187 [==============================] - 0s 924us/step - loss: 0.1188 - acc: 0.8396\n",
            "Epoch 25/100\n",
            "187/187 [==============================] - 0s 808us/step - loss: 0.1174 - acc: 0.8449\n",
            "Epoch 26/100\n",
            "187/187 [==============================] - 0s 797us/step - loss: 0.1122 - acc: 0.8663\n",
            "Epoch 27/100\n",
            "187/187 [==============================] - 0s 850us/step - loss: 0.1133 - acc: 0.8503\n",
            "Epoch 28/100\n",
            "187/187 [==============================] - 0s 864us/step - loss: 0.1103 - acc: 0.8449\n",
            "Epoch 29/100\n",
            "187/187 [==============================] - 0s 845us/step - loss: 0.1074 - acc: 0.8663\n",
            "Epoch 30/100\n",
            "187/187 [==============================] - 0s 813us/step - loss: 0.1101 - acc: 0.8503\n",
            "Epoch 31/100\n",
            "187/187 [==============================] - 0s 802us/step - loss: 0.1100 - acc: 0.8610\n",
            "Epoch 32/100\n",
            "187/187 [==============================] - 0s 831us/step - loss: 0.1060 - acc: 0.8717\n",
            "Epoch 33/100\n",
            "187/187 [==============================] - 0s 818us/step - loss: 0.1024 - acc: 0.8824\n",
            "Epoch 34/100\n",
            "187/187 [==============================] - 0s 924us/step - loss: 0.1026 - acc: 0.8717\n",
            "Epoch 35/100\n",
            "187/187 [==============================] - 0s 863us/step - loss: 0.1008 - acc: 0.8930\n",
            "Epoch 36/100\n",
            "187/187 [==============================] - 0s 790us/step - loss: 0.1018 - acc: 0.8877\n",
            "Epoch 37/100\n",
            "187/187 [==============================] - 0s 833us/step - loss: 0.0984 - acc: 0.8877\n",
            "Epoch 38/100\n",
            "187/187 [==============================] - 0s 800us/step - loss: 0.0955 - acc: 0.8877\n",
            "Epoch 39/100\n",
            "187/187 [==============================] - 0s 823us/step - loss: 0.0990 - acc: 0.8663\n",
            "Epoch 40/100\n",
            "187/187 [==============================] - 0s 818us/step - loss: 0.0941 - acc: 0.9091\n",
            "Epoch 41/100\n",
            "187/187 [==============================] - 0s 932us/step - loss: 0.0974 - acc: 0.8824\n",
            "Epoch 42/100\n",
            "187/187 [==============================] - 0s 808us/step - loss: 0.0927 - acc: 0.9091\n",
            "Epoch 43/100\n",
            "187/187 [==============================] - 0s 908us/step - loss: 0.0958 - acc: 0.8877\n",
            "Epoch 44/100\n",
            "187/187 [==============================] - 0s 834us/step - loss: 0.0880 - acc: 0.8984\n",
            "Epoch 45/100\n",
            "187/187 [==============================] - 0s 803us/step - loss: 0.0886 - acc: 0.9037\n",
            "Epoch 46/100\n",
            "187/187 [==============================] - 0s 823us/step - loss: 0.0848 - acc: 0.9037\n",
            "Epoch 47/100\n",
            "187/187 [==============================] - 0s 940us/step - loss: 0.0885 - acc: 0.8984\n",
            "Epoch 48/100\n",
            "187/187 [==============================] - 0s 785us/step - loss: 0.0846 - acc: 0.9251\n",
            "Epoch 49/100\n",
            "187/187 [==============================] - 0s 833us/step - loss: 0.0911 - acc: 0.8663\n",
            "Epoch 50/100\n",
            "187/187 [==============================] - 0s 863us/step - loss: 0.0870 - acc: 0.9091\n",
            "Epoch 51/100\n",
            "187/187 [==============================] - 0s 781us/step - loss: 0.0824 - acc: 0.9144\n",
            "Epoch 52/100\n",
            "187/187 [==============================] - 0s 813us/step - loss: 0.0830 - acc: 0.9144\n",
            "Epoch 53/100\n",
            "187/187 [==============================] - 0s 808us/step - loss: 0.0796 - acc: 0.8984\n",
            "Epoch 54/100\n",
            "187/187 [==============================] - 0s 849us/step - loss: 0.0786 - acc: 0.9251\n",
            "Epoch 55/100\n",
            "187/187 [==============================] - 0s 846us/step - loss: 0.0797 - acc: 0.8930\n",
            "Epoch 56/100\n",
            "187/187 [==============================] - 0s 997us/step - loss: 0.0741 - acc: 0.9198\n",
            "Epoch 57/100\n",
            "187/187 [==============================] - 0s 1ms/step - loss: 0.0757 - acc: 0.9251\n",
            "Epoch 58/100\n",
            "187/187 [==============================] - 0s 867us/step - loss: 0.0743 - acc: 0.9091\n",
            "Epoch 59/100\n",
            "187/187 [==============================] - 0s 968us/step - loss: 0.0730 - acc: 0.9091\n",
            "Epoch 60/100\n",
            "187/187 [==============================] - 0s 906us/step - loss: 0.0714 - acc: 0.9198\n",
            "Epoch 61/100\n",
            "187/187 [==============================] - 0s 809us/step - loss: 0.0712 - acc: 0.9091\n",
            "Epoch 62/100\n",
            "187/187 [==============================] - 0s 833us/step - loss: 0.0723 - acc: 0.9412\n",
            "Epoch 63/100\n",
            "187/187 [==============================] - 0s 848us/step - loss: 0.0689 - acc: 0.9305\n",
            "Epoch 64/100\n",
            "187/187 [==============================] - 0s 871us/step - loss: 0.0702 - acc: 0.9198\n",
            "Epoch 65/100\n",
            "187/187 [==============================] - 0s 838us/step - loss: 0.0693 - acc: 0.9358\n",
            "Epoch 66/100\n",
            "187/187 [==============================] - 0s 970us/step - loss: 0.0676 - acc: 0.9305\n",
            "Epoch 67/100\n",
            "187/187 [==============================] - 0s 776us/step - loss: 0.0738 - acc: 0.8930\n",
            "Epoch 68/100\n",
            "187/187 [==============================] - 0s 847us/step - loss: 0.0675 - acc: 0.9305\n",
            "Epoch 69/100\n",
            "187/187 [==============================] - 0s 792us/step - loss: 0.0604 - acc: 0.9465\n",
            "Epoch 70/100\n",
            "187/187 [==============================] - 0s 838us/step - loss: 0.0637 - acc: 0.9251\n",
            "Epoch 71/100\n",
            "187/187 [==============================] - 0s 933us/step - loss: 0.0624 - acc: 0.9465\n",
            "Epoch 72/100\n",
            "187/187 [==============================] - 0s 928us/step - loss: 0.0605 - acc: 0.9305\n",
            "Epoch 73/100\n",
            "187/187 [==============================] - 0s 869us/step - loss: 0.0562 - acc: 0.9626\n",
            "Epoch 74/100\n",
            "187/187 [==============================] - 0s 803us/step - loss: 0.0577 - acc: 0.9358\n",
            "Epoch 75/100\n",
            "187/187 [==============================] - 0s 943us/step - loss: 0.0581 - acc: 0.9465\n",
            "Epoch 76/100\n",
            "187/187 [==============================] - 0s 810us/step - loss: 0.0549 - acc: 0.9412\n",
            "Epoch 77/100\n",
            "187/187 [==============================] - 0s 1ms/step - loss: 0.0527 - acc: 0.9412\n",
            "Epoch 78/100\n",
            "187/187 [==============================] - 0s 873us/step - loss: 0.0518 - acc: 0.9572\n",
            "Epoch 79/100\n",
            "187/187 [==============================] - 0s 916us/step - loss: 0.0524 - acc: 0.9626\n",
            "Epoch 80/100\n",
            "187/187 [==============================] - 0s 942us/step - loss: 0.0504 - acc: 0.9519\n",
            "Epoch 81/100\n",
            "187/187 [==============================] - 0s 918us/step - loss: 0.0509 - acc: 0.9412\n",
            "Epoch 82/100\n",
            "187/187 [==============================] - 0s 964us/step - loss: 0.0487 - acc: 0.9572\n",
            "Epoch 83/100\n",
            "187/187 [==============================] - 0s 907us/step - loss: 0.0473 - acc: 0.9519\n",
            "Epoch 84/100\n",
            "187/187 [==============================] - 0s 895us/step - loss: 0.0476 - acc: 0.9626\n",
            "Epoch 85/100\n",
            "187/187 [==============================] - 0s 980us/step - loss: 0.0514 - acc: 0.9358\n",
            "Epoch 86/100\n",
            "187/187 [==============================] - 0s 816us/step - loss: 0.0450 - acc: 0.9626\n",
            "Epoch 87/100\n",
            "187/187 [==============================] - 0s 943us/step - loss: 0.0463 - acc: 0.9679\n",
            "Epoch 88/100\n",
            "187/187 [==============================] - 0s 847us/step - loss: 0.0498 - acc: 0.9572\n",
            "Epoch 89/100\n",
            "187/187 [==============================] - 0s 798us/step - loss: 0.0422 - acc: 0.9572\n",
            "Epoch 90/100\n",
            "187/187 [==============================] - 0s 985us/step - loss: 0.0437 - acc: 0.9626\n",
            "Epoch 91/100\n",
            "187/187 [==============================] - 0s 908us/step - loss: 0.0442 - acc: 0.9572\n",
            "Epoch 92/100\n",
            "187/187 [==============================] - 0s 780us/step - loss: 0.0484 - acc: 0.9572\n",
            "Epoch 93/100\n",
            "187/187 [==============================] - 0s 815us/step - loss: 0.0412 - acc: 0.9733\n",
            "Epoch 94/100\n",
            "187/187 [==============================] - 0s 934us/step - loss: 0.0409 - acc: 0.9626\n",
            "Epoch 95/100\n",
            "187/187 [==============================] - 0s 853us/step - loss: 0.0374 - acc: 0.9733\n",
            "Epoch 96/100\n",
            "187/187 [==============================] - 0s 808us/step - loss: 0.0402 - acc: 0.9626\n",
            "Epoch 97/100\n",
            "187/187 [==============================] - 0s 926us/step - loss: 0.0348 - acc: 0.9786\n",
            "Epoch 98/100\n",
            "187/187 [==============================] - 0s 786us/step - loss: 0.0379 - acc: 0.9733\n",
            "Epoch 99/100\n",
            "187/187 [==============================] - 0s 828us/step - loss: 0.0374 - acc: 0.9679\n",
            "Epoch 100/100\n",
            "187/187 [==============================] - 0s 828us/step - loss: 0.0374 - acc: 0.9733\n",
            "21/21 [==============================] - 0s 12ms/step\n",
            "Epoch 1/100\n",
            "187/187 [==============================] - 1s 5ms/step - loss: 0.2533 - acc: 0.4920\n",
            "Epoch 2/100\n",
            "187/187 [==============================] - 0s 798us/step - loss: 0.2420 - acc: 0.5668\n",
            "Epoch 3/100\n",
            "187/187 [==============================] - 0s 822us/step - loss: 0.2379 - acc: 0.5455\n",
            "Epoch 4/100\n",
            "187/187 [==============================] - 0s 794us/step - loss: 0.2341 - acc: 0.6417\n",
            "Epoch 5/100\n",
            "187/187 [==============================] - 0s 835us/step - loss: 0.2299 - acc: 0.6524\n",
            "Epoch 6/100\n",
            "187/187 [==============================] - 0s 781us/step - loss: 0.2249 - acc: 0.6631\n",
            "Epoch 7/100\n",
            "187/187 [==============================] - 0s 836us/step - loss: 0.2189 - acc: 0.6845\n",
            "Epoch 8/100\n",
            "187/187 [==============================] - 0s 872us/step - loss: 0.2116 - acc: 0.7166\n",
            "Epoch 9/100\n",
            "187/187 [==============================] - 0s 844us/step - loss: 0.2045 - acc: 0.7754\n",
            "Epoch 10/100\n",
            "187/187 [==============================] - 0s 879us/step - loss: 0.1918 - acc: 0.8021\n",
            "Epoch 11/100\n",
            "187/187 [==============================] - 0s 838us/step - loss: 0.1850 - acc: 0.7861\n",
            "Epoch 12/100\n",
            "187/187 [==============================] - 0s 843us/step - loss: 0.1767 - acc: 0.7754\n",
            "Epoch 13/100\n",
            "187/187 [==============================] - 0s 973us/step - loss: 0.1621 - acc: 0.8128\n",
            "Epoch 14/100\n",
            "187/187 [==============================] - 0s 986us/step - loss: 0.1495 - acc: 0.8289\n",
            "Epoch 15/100\n",
            "187/187 [==============================] - 0s 838us/step - loss: 0.1488 - acc: 0.7914\n",
            "Epoch 16/100\n",
            "187/187 [==============================] - 0s 811us/step - loss: 0.1426 - acc: 0.8235\n",
            "Epoch 17/100\n",
            "187/187 [==============================] - 0s 846us/step - loss: 0.1398 - acc: 0.8075\n",
            "Epoch 18/100\n",
            "187/187 [==============================] - 0s 901us/step - loss: 0.1399 - acc: 0.8075\n",
            "Epoch 19/100\n",
            "187/187 [==============================] - 0s 973us/step - loss: 0.1313 - acc: 0.8289\n",
            "Epoch 20/100\n",
            "187/187 [==============================] - 0s 862us/step - loss: 0.1259 - acc: 0.8235\n",
            "Epoch 21/100\n",
            "187/187 [==============================] - 0s 959us/step - loss: 0.1272 - acc: 0.8289\n",
            "Epoch 22/100\n",
            "187/187 [==============================] - 0s 817us/step - loss: 0.1206 - acc: 0.8128\n",
            "Epoch 23/100\n",
            "187/187 [==============================] - 0s 863us/step - loss: 0.1190 - acc: 0.8396\n",
            "Epoch 24/100\n",
            "187/187 [==============================] - 0s 853us/step - loss: 0.1173 - acc: 0.8342\n",
            "Epoch 25/100\n",
            "187/187 [==============================] - 0s 806us/step - loss: 0.1173 - acc: 0.8556\n",
            "Epoch 26/100\n",
            "187/187 [==============================] - 0s 933us/step - loss: 0.1130 - acc: 0.8503\n",
            "Epoch 27/100\n",
            "187/187 [==============================] - 0s 921us/step - loss: 0.1104 - acc: 0.8449\n",
            "Epoch 28/100\n",
            "187/187 [==============================] - 0s 813us/step - loss: 0.1092 - acc: 0.8556\n",
            "Epoch 29/100\n",
            "187/187 [==============================] - 0s 838us/step - loss: 0.1071 - acc: 0.8449\n",
            "Epoch 30/100\n",
            "187/187 [==============================] - 0s 860us/step - loss: 0.1051 - acc: 0.8449\n",
            "Epoch 31/100\n",
            "187/187 [==============================] - 0s 940us/step - loss: 0.1030 - acc: 0.8663\n",
            "Epoch 32/100\n",
            "187/187 [==============================] - 0s 869us/step - loss: 0.0986 - acc: 0.8717\n",
            "Epoch 33/100\n",
            "187/187 [==============================] - 0s 991us/step - loss: 0.0986 - acc: 0.8663\n",
            "Epoch 34/100\n",
            "187/187 [==============================] - 0s 932us/step - loss: 0.0973 - acc: 0.8824\n",
            "Epoch 35/100\n",
            "187/187 [==============================] - 0s 828us/step - loss: 0.0971 - acc: 0.8610\n",
            "Epoch 36/100\n",
            "187/187 [==============================] - 0s 858us/step - loss: 0.0978 - acc: 0.8770\n",
            "Epoch 37/100\n",
            "187/187 [==============================] - 0s 845us/step - loss: 0.0922 - acc: 0.8824\n",
            "Epoch 38/100\n",
            "187/187 [==============================] - 0s 793us/step - loss: 0.0906 - acc: 0.8930\n",
            "Epoch 39/100\n",
            "187/187 [==============================] - 0s 929us/step - loss: 0.0874 - acc: 0.8824\n",
            "Epoch 40/100\n",
            "187/187 [==============================] - 0s 862us/step - loss: 0.0852 - acc: 0.8824\n",
            "Epoch 41/100\n",
            "187/187 [==============================] - 0s 847us/step - loss: 0.0876 - acc: 0.8663\n",
            "Epoch 42/100\n",
            "187/187 [==============================] - 0s 799us/step - loss: 0.0900 - acc: 0.8824\n",
            "Epoch 43/100\n",
            "187/187 [==============================] - 0s 826us/step - loss: 0.0935 - acc: 0.8556\n",
            "Epoch 44/100\n",
            "187/187 [==============================] - 0s 831us/step - loss: 0.0916 - acc: 0.8877\n",
            "Epoch 45/100\n",
            "187/187 [==============================] - 0s 860us/step - loss: 0.0843 - acc: 0.8984\n",
            "Epoch 46/100\n",
            "187/187 [==============================] - 0s 826us/step - loss: 0.0853 - acc: 0.8770\n",
            "Epoch 47/100\n",
            "187/187 [==============================] - 0s 794us/step - loss: 0.0787 - acc: 0.8930\n",
            "Epoch 48/100\n",
            "187/187 [==============================] - 0s 824us/step - loss: 0.0783 - acc: 0.8984\n",
            "Epoch 49/100\n",
            "187/187 [==============================] - 0s 839us/step - loss: 0.0731 - acc: 0.9091\n",
            "Epoch 50/100\n",
            "187/187 [==============================] - 0s 992us/step - loss: 0.0737 - acc: 0.9251\n",
            "Epoch 51/100\n",
            "187/187 [==============================] - 0s 995us/step - loss: 0.0766 - acc: 0.9091\n",
            "Epoch 52/100\n",
            "187/187 [==============================] - 0s 930us/step - loss: 0.0688 - acc: 0.9251\n",
            "Epoch 53/100\n",
            "187/187 [==============================] - 0s 820us/step - loss: 0.0744 - acc: 0.9198\n",
            "Epoch 54/100\n",
            "187/187 [==============================] - 0s 850us/step - loss: 0.0674 - acc: 0.9251\n",
            "Epoch 55/100\n",
            "187/187 [==============================] - 0s 842us/step - loss: 0.0662 - acc: 0.9305\n",
            "Epoch 56/100\n",
            "187/187 [==============================] - 0s 792us/step - loss: 0.0652 - acc: 0.9251\n",
            "Epoch 57/100\n",
            "187/187 [==============================] - 0s 855us/step - loss: 0.0650 - acc: 0.9144\n",
            "Epoch 58/100\n",
            "187/187 [==============================] - 0s 927us/step - loss: 0.0639 - acc: 0.9358\n",
            "Epoch 59/100\n",
            "187/187 [==============================] - 0s 812us/step - loss: 0.0632 - acc: 0.9358\n",
            "Epoch 60/100\n",
            "187/187 [==============================] - 0s 777us/step - loss: 0.0623 - acc: 0.9358\n",
            "Epoch 61/100\n",
            "187/187 [==============================] - 0s 798us/step - loss: 0.0609 - acc: 0.9305\n",
            "Epoch 62/100\n",
            "187/187 [==============================] - 0s 772us/step - loss: 0.0577 - acc: 0.9305\n",
            "Epoch 63/100\n",
            "187/187 [==============================] - 0s 823us/step - loss: 0.0643 - acc: 0.9144\n",
            "Epoch 64/100\n",
            "187/187 [==============================] - 0s 849us/step - loss: 0.0598 - acc: 0.9358\n",
            "Epoch 65/100\n",
            "187/187 [==============================] - 0s 936us/step - loss: 0.0552 - acc: 0.9465\n",
            "Epoch 66/100\n",
            "187/187 [==============================] - 0s 926us/step - loss: 0.0556 - acc: 0.9358\n",
            "Epoch 67/100\n",
            "187/187 [==============================] - 0s 897us/step - loss: 0.0541 - acc: 0.9358\n",
            "Epoch 68/100\n",
            "187/187 [==============================] - 0s 899us/step - loss: 0.0521 - acc: 0.9465\n",
            "Epoch 69/100\n",
            "187/187 [==============================] - 0s 801us/step - loss: 0.0493 - acc: 0.9519\n",
            "Epoch 70/100\n",
            "187/187 [==============================] - 0s 825us/step - loss: 0.0496 - acc: 0.9572\n",
            "Epoch 71/100\n",
            "187/187 [==============================] - 0s 925us/step - loss: 0.0492 - acc: 0.9358\n",
            "Epoch 72/100\n",
            "187/187 [==============================] - 0s 860us/step - loss: 0.0478 - acc: 0.9572\n",
            "Epoch 73/100\n",
            "187/187 [==============================] - 0s 848us/step - loss: 0.0441 - acc: 0.9572\n",
            "Epoch 74/100\n",
            "187/187 [==============================] - 0s 787us/step - loss: 0.0446 - acc: 0.9626\n",
            "Epoch 75/100\n",
            "187/187 [==============================] - 0s 817us/step - loss: 0.0419 - acc: 0.9679\n",
            "Epoch 76/100\n",
            "187/187 [==============================] - 0s 853us/step - loss: 0.0410 - acc: 0.9679\n",
            "Epoch 77/100\n",
            "187/187 [==============================] - 0s 920us/step - loss: 0.0400 - acc: 0.9626\n",
            "Epoch 78/100\n",
            "187/187 [==============================] - 0s 821us/step - loss: 0.0386 - acc: 0.9733\n",
            "Epoch 79/100\n",
            "187/187 [==============================] - 0s 918us/step - loss: 0.0364 - acc: 0.9679\n",
            "Epoch 80/100\n",
            "187/187 [==============================] - 0s 843us/step - loss: 0.0363 - acc: 0.9733\n",
            "Epoch 81/100\n",
            "187/187 [==============================] - 0s 777us/step - loss: 0.0346 - acc: 0.9786\n",
            "Epoch 82/100\n",
            "187/187 [==============================] - 0s 915us/step - loss: 0.0347 - acc: 0.9786\n",
            "Epoch 83/100\n",
            "187/187 [==============================] - 0s 864us/step - loss: 0.0403 - acc: 0.9840\n",
            "Epoch 84/100\n",
            "187/187 [==============================] - 0s 829us/step - loss: 0.0324 - acc: 0.9840\n",
            "Epoch 85/100\n",
            "187/187 [==============================] - 0s 913us/step - loss: 0.0375 - acc: 0.9679\n",
            "Epoch 86/100\n",
            "187/187 [==============================] - 0s 847us/step - loss: 0.0331 - acc: 0.9786\n",
            "Epoch 87/100\n",
            "187/187 [==============================] - 0s 779us/step - loss: 0.0306 - acc: 0.9840\n",
            "Epoch 88/100\n",
            "187/187 [==============================] - 0s 797us/step - loss: 0.0291 - acc: 0.9840\n",
            "Epoch 89/100\n",
            "187/187 [==============================] - 0s 788us/step - loss: 0.0292 - acc: 0.9786\n",
            "Epoch 90/100\n",
            "187/187 [==============================] - 0s 925us/step - loss: 0.0302 - acc: 0.9840\n",
            "Epoch 91/100\n",
            "187/187 [==============================] - 0s 799us/step - loss: 0.0274 - acc: 0.9893\n",
            "Epoch 92/100\n",
            "187/187 [==============================] - 0s 810us/step - loss: 0.0268 - acc: 0.9786\n",
            "Epoch 93/100\n",
            "187/187 [==============================] - 0s 807us/step - loss: 0.0301 - acc: 0.9786\n",
            "Epoch 94/100\n",
            "187/187 [==============================] - 0s 823us/step - loss: 0.0284 - acc: 0.9893\n",
            "Epoch 95/100\n",
            "187/187 [==============================] - 0s 820us/step - loss: 0.0246 - acc: 0.9893\n",
            "Epoch 96/100\n",
            "187/187 [==============================] - 0s 862us/step - loss: 0.0263 - acc: 0.9893\n",
            "Epoch 97/100\n",
            "187/187 [==============================] - 0s 789us/step - loss: 0.0244 - acc: 0.9840\n",
            "Epoch 98/100\n",
            "187/187 [==============================] - 0s 781us/step - loss: 0.0216 - acc: 0.9893\n",
            "Epoch 99/100\n",
            "187/187 [==============================] - 0s 793us/step - loss: 0.0234 - acc: 0.9840\n",
            "Epoch 100/100\n",
            "187/187 [==============================] - 0s 821us/step - loss: 0.0224 - acc: 0.9947\n",
            "21/21 [==============================] - 0s 12ms/step\n",
            "Epoch 1/100\n",
            "187/187 [==============================] - 1s 5ms/step - loss: 0.2472 - acc: 0.5668\n",
            "Epoch 2/100\n",
            "187/187 [==============================] - 0s 808us/step - loss: 0.2405 - acc: 0.6471\n",
            "Epoch 3/100\n",
            "187/187 [==============================] - 0s 810us/step - loss: 0.2327 - acc: 0.6203\n",
            "Epoch 4/100\n",
            "187/187 [==============================] - 0s 820us/step - loss: 0.2258 - acc: 0.7219\n",
            "Epoch 5/100\n",
            "187/187 [==============================] - 0s 884us/step - loss: 0.2180 - acc: 0.7273\n",
            "Epoch 6/100\n",
            "187/187 [==============================] - 0s 848us/step - loss: 0.2083 - acc: 0.7326\n",
            "Epoch 7/100\n",
            "187/187 [==============================] - 0s 831us/step - loss: 0.1999 - acc: 0.7701\n",
            "Epoch 8/100\n",
            "187/187 [==============================] - 0s 865us/step - loss: 0.1942 - acc: 0.7433\n",
            "Epoch 9/100\n",
            "187/187 [==============================] - 0s 895us/step - loss: 0.1905 - acc: 0.7861\n",
            "Epoch 10/100\n",
            "187/187 [==============================] - 0s 943us/step - loss: 0.1797 - acc: 0.8075\n",
            "Epoch 11/100\n",
            "187/187 [==============================] - 0s 815us/step - loss: 0.1702 - acc: 0.8128\n",
            "Epoch 12/100\n",
            "187/187 [==============================] - 0s 792us/step - loss: 0.1624 - acc: 0.8182\n",
            "Epoch 13/100\n",
            "187/187 [==============================] - 0s 937us/step - loss: 0.1564 - acc: 0.8182\n",
            "Epoch 14/100\n",
            "187/187 [==============================] - 0s 953us/step - loss: 0.1513 - acc: 0.8342\n",
            "Epoch 15/100\n",
            "187/187 [==============================] - 0s 792us/step - loss: 0.1493 - acc: 0.8075\n",
            "Epoch 16/100\n",
            "187/187 [==============================] - 0s 857us/step - loss: 0.1427 - acc: 0.8075\n",
            "Epoch 17/100\n",
            "187/187 [==============================] - 0s 815us/step - loss: 0.1443 - acc: 0.8021\n",
            "Epoch 18/100\n",
            "187/187 [==============================] - 0s 866us/step - loss: 0.1362 - acc: 0.8235\n",
            "Epoch 19/100\n",
            "187/187 [==============================] - 0s 925us/step - loss: 0.1307 - acc: 0.8289\n",
            "Epoch 20/100\n",
            "187/187 [==============================] - 0s 856us/step - loss: 0.1277 - acc: 0.8449\n",
            "Epoch 21/100\n",
            "187/187 [==============================] - 0s 811us/step - loss: 0.1266 - acc: 0.8449\n",
            "Epoch 22/100\n",
            "187/187 [==============================] - 0s 798us/step - loss: 0.1236 - acc: 0.8235\n",
            "Epoch 23/100\n",
            "187/187 [==============================] - 0s 789us/step - loss: 0.1195 - acc: 0.8770\n",
            "Epoch 24/100\n",
            "187/187 [==============================] - 0s 846us/step - loss: 0.1171 - acc: 0.8449\n",
            "Epoch 25/100\n",
            "187/187 [==============================] - 0s 868us/step - loss: 0.1224 - acc: 0.8342\n",
            "Epoch 26/100\n",
            "187/187 [==============================] - 0s 843us/step - loss: 0.1114 - acc: 0.8663\n",
            "Epoch 27/100\n",
            "187/187 [==============================] - 0s 923us/step - loss: 0.1116 - acc: 0.8449\n",
            "Epoch 28/100\n",
            "187/187 [==============================] - 0s 942us/step - loss: 0.1075 - acc: 0.8556\n",
            "Epoch 29/100\n",
            "187/187 [==============================] - 0s 821us/step - loss: 0.1041 - acc: 0.8717\n",
            "Epoch 30/100\n",
            "187/187 [==============================] - 0s 864us/step - loss: 0.1034 - acc: 0.8717\n",
            "Epoch 31/100\n",
            "187/187 [==============================] - 0s 801us/step - loss: 0.1010 - acc: 0.8824\n",
            "Epoch 32/100\n",
            "187/187 [==============================] - 0s 966us/step - loss: 0.0974 - acc: 0.8877\n",
            "Epoch 33/100\n",
            "187/187 [==============================] - 0s 900us/step - loss: 0.0941 - acc: 0.8877\n",
            "Epoch 34/100\n",
            "187/187 [==============================] - 0s 822us/step - loss: 0.0955 - acc: 0.9037\n",
            "Epoch 35/100\n",
            "187/187 [==============================] - 0s 883us/step - loss: 0.0908 - acc: 0.9091\n",
            "Epoch 36/100\n",
            "187/187 [==============================] - 0s 811us/step - loss: 0.0900 - acc: 0.8984\n",
            "Epoch 37/100\n",
            "187/187 [==============================] - 0s 826us/step - loss: 0.0873 - acc: 0.9144\n",
            "Epoch 38/100\n",
            "187/187 [==============================] - 0s 876us/step - loss: 0.0860 - acc: 0.9251\n",
            "Epoch 39/100\n",
            "187/187 [==============================] - 0s 867us/step - loss: 0.0849 - acc: 0.9144\n",
            "Epoch 40/100\n",
            "187/187 [==============================] - 0s 834us/step - loss: 0.0830 - acc: 0.9144\n",
            "Epoch 41/100\n",
            "187/187 [==============================] - 0s 868us/step - loss: 0.0794 - acc: 0.9198\n",
            "Epoch 42/100\n",
            "187/187 [==============================] - 0s 814us/step - loss: 0.0781 - acc: 0.9198\n",
            "Epoch 43/100\n",
            "187/187 [==============================] - 0s 805us/step - loss: 0.0760 - acc: 0.9305\n",
            "Epoch 44/100\n",
            "187/187 [==============================] - 0s 908us/step - loss: 0.0772 - acc: 0.9198\n",
            "Epoch 45/100\n",
            "187/187 [==============================] - 0s 981us/step - loss: 0.0774 - acc: 0.9412\n",
            "Epoch 46/100\n",
            "187/187 [==============================] - 0s 870us/step - loss: 0.0743 - acc: 0.9251\n",
            "Epoch 47/100\n",
            "187/187 [==============================] - 0s 824us/step - loss: 0.0701 - acc: 0.9251\n",
            "Epoch 48/100\n",
            "187/187 [==============================] - 0s 790us/step - loss: 0.0719 - acc: 0.9305\n",
            "Epoch 49/100\n",
            "187/187 [==============================] - 0s 795us/step - loss: 0.0727 - acc: 0.9198\n",
            "Epoch 50/100\n",
            "187/187 [==============================] - 0s 763us/step - loss: 0.0686 - acc: 0.9358\n",
            "Epoch 51/100\n",
            "187/187 [==============================] - 0s 828us/step - loss: 0.0681 - acc: 0.9305\n",
            "Epoch 52/100\n",
            "187/187 [==============================] - 0s 843us/step - loss: 0.0648 - acc: 0.9358\n",
            "Epoch 53/100\n",
            "187/187 [==============================] - 0s 798us/step - loss: 0.0618 - acc: 0.9305\n",
            "Epoch 54/100\n",
            "187/187 [==============================] - 0s 831us/step - loss: 0.0641 - acc: 0.9358\n",
            "Epoch 55/100\n",
            "187/187 [==============================] - 0s 858us/step - loss: 0.0599 - acc: 0.9358\n",
            "Epoch 56/100\n",
            "187/187 [==============================] - 0s 913us/step - loss: 0.0611 - acc: 0.9358\n",
            "Epoch 57/100\n",
            "187/187 [==============================] - 0s 871us/step - loss: 0.0614 - acc: 0.9358\n",
            "Epoch 58/100\n",
            "187/187 [==============================] - 0s 767us/step - loss: 0.0565 - acc: 0.9572\n",
            "Epoch 59/100\n",
            "187/187 [==============================] - 0s 844us/step - loss: 0.0557 - acc: 0.9305\n",
            "Epoch 60/100\n",
            "187/187 [==============================] - 0s 865us/step - loss: 0.0560 - acc: 0.9465\n",
            "Epoch 61/100\n",
            "187/187 [==============================] - 0s 823us/step - loss: 0.0550 - acc: 0.9465\n",
            "Epoch 62/100\n",
            "187/187 [==============================] - 0s 805us/step - loss: 0.0518 - acc: 0.9465\n",
            "Epoch 63/100\n",
            "187/187 [==============================] - 0s 785us/step - loss: 0.0496 - acc: 0.9626\n",
            "Epoch 64/100\n",
            "187/187 [==============================] - 0s 851us/step - loss: 0.0496 - acc: 0.9412\n",
            "Epoch 65/100\n",
            "187/187 [==============================] - 0s 800us/step - loss: 0.0485 - acc: 0.9519\n",
            "Epoch 66/100\n",
            "187/187 [==============================] - 0s 852us/step - loss: 0.0471 - acc: 0.9733\n",
            "Epoch 67/100\n",
            "187/187 [==============================] - 0s 829us/step - loss: 0.0472 - acc: 0.9519\n",
            "Epoch 68/100\n",
            "187/187 [==============================] - 0s 819us/step - loss: 0.0497 - acc: 0.9519\n",
            "Epoch 69/100\n",
            "187/187 [==============================] - 0s 800us/step - loss: 0.0474 - acc: 0.9519\n",
            "Epoch 70/100\n",
            "187/187 [==============================] - 0s 990us/step - loss: 0.0427 - acc: 0.9679\n",
            "Epoch 71/100\n",
            "187/187 [==============================] - 0s 830us/step - loss: 0.0468 - acc: 0.9679\n",
            "Epoch 72/100\n",
            "187/187 [==============================] - 0s 819us/step - loss: 0.0437 - acc: 0.9679\n",
            "Epoch 73/100\n",
            "187/187 [==============================] - 0s 856us/step - loss: 0.0400 - acc: 0.9626\n",
            "Epoch 74/100\n",
            "187/187 [==============================] - 0s 802us/step - loss: 0.0400 - acc: 0.9733\n",
            "Epoch 75/100\n",
            "187/187 [==============================] - 0s 798us/step - loss: 0.0374 - acc: 0.9733\n",
            "Epoch 76/100\n",
            "187/187 [==============================] - 0s 817us/step - loss: 0.0411 - acc: 0.9626\n",
            "Epoch 77/100\n",
            "187/187 [==============================] - 0s 868us/step - loss: 0.0380 - acc: 0.9679\n",
            "Epoch 78/100\n",
            "187/187 [==============================] - 0s 845us/step - loss: 0.0370 - acc: 0.9679\n",
            "Epoch 79/100\n",
            "187/187 [==============================] - 0s 832us/step - loss: 0.0338 - acc: 0.9733\n",
            "Epoch 80/100\n",
            "187/187 [==============================] - 0s 811us/step - loss: 0.0333 - acc: 0.9786\n",
            "Epoch 81/100\n",
            "187/187 [==============================] - 0s 828us/step - loss: 0.0339 - acc: 0.9786\n",
            "Epoch 82/100\n",
            "187/187 [==============================] - 0s 806us/step - loss: 0.0336 - acc: 0.9572\n",
            "Epoch 83/100\n",
            "187/187 [==============================] - 0s 972us/step - loss: 0.0304 - acc: 0.9786\n",
            "Epoch 84/100\n",
            "187/187 [==============================] - 0s 785us/step - loss: 0.0296 - acc: 0.9840\n",
            "Epoch 85/100\n",
            "187/187 [==============================] - 0s 880us/step - loss: 0.0297 - acc: 0.9786\n",
            "Epoch 86/100\n",
            "187/187 [==============================] - 0s 827us/step - loss: 0.0287 - acc: 0.9840\n",
            "Epoch 87/100\n",
            "187/187 [==============================] - 0s 847us/step - loss: 0.0275 - acc: 0.9786\n",
            "Epoch 88/100\n",
            "187/187 [==============================] - 0s 866us/step - loss: 0.0309 - acc: 0.9733\n",
            "Epoch 89/100\n",
            "187/187 [==============================] - 0s 961us/step - loss: 0.0309 - acc: 0.9840\n",
            "Epoch 90/100\n",
            "187/187 [==============================] - 0s 838us/step - loss: 0.0277 - acc: 0.9840\n",
            "Epoch 91/100\n",
            "187/187 [==============================] - 0s 806us/step - loss: 0.0273 - acc: 0.9679\n",
            "Epoch 92/100\n",
            "187/187 [==============================] - 0s 823us/step - loss: 0.0240 - acc: 0.9786\n",
            "Epoch 93/100\n",
            "187/187 [==============================] - 0s 887us/step - loss: 0.0239 - acc: 0.9893\n",
            "Epoch 94/100\n",
            "187/187 [==============================] - 0s 843us/step - loss: 0.0241 - acc: 0.9840\n",
            "Epoch 95/100\n",
            "187/187 [==============================] - 0s 787us/step - loss: 0.0230 - acc: 0.9893\n",
            "Epoch 96/100\n",
            "187/187 [==============================] - 0s 941us/step - loss: 0.0244 - acc: 0.9893\n",
            "Epoch 97/100\n",
            "187/187 [==============================] - 0s 930us/step - loss: 0.0227 - acc: 0.9893\n",
            "Epoch 98/100\n",
            "187/187 [==============================] - 0s 816us/step - loss: 0.0223 - acc: 0.9840\n",
            "Epoch 99/100\n",
            "187/187 [==============================] - 0s 866us/step - loss: 0.0248 - acc: 0.9733\n",
            "Epoch 100/100\n",
            "187/187 [==============================] - 0s 995us/step - loss: 0.0228 - acc: 0.9840\n",
            "21/21 [==============================] - 0s 13ms/step\n",
            "Epoch 1/100\n",
            "188/188 [==============================] - 1s 5ms/step - loss: 0.2539 - acc: 0.4681\n",
            "Epoch 2/100\n",
            "188/188 [==============================] - 0s 834us/step - loss: 0.2486 - acc: 0.5106\n",
            "Epoch 3/100\n",
            "188/188 [==============================] - 0s 943us/step - loss: 0.2469 - acc: 0.5798\n",
            "Epoch 4/100\n",
            "188/188 [==============================] - 0s 931us/step - loss: 0.2451 - acc: 0.5745\n",
            "Epoch 5/100\n",
            "188/188 [==============================] - 0s 909us/step - loss: 0.2417 - acc: 0.7181\n",
            "Epoch 6/100\n",
            "188/188 [==============================] - 0s 927us/step - loss: 0.2386 - acc: 0.6489\n",
            "Epoch 7/100\n",
            "188/188 [==============================] - 0s 782us/step - loss: 0.2357 - acc: 0.6702\n",
            "Epoch 8/100\n",
            "188/188 [==============================] - 0s 777us/step - loss: 0.2271 - acc: 0.7819\n",
            "Epoch 9/100\n",
            "188/188 [==============================] - 0s 828us/step - loss: 0.2200 - acc: 0.7340\n",
            "Epoch 10/100\n",
            "188/188 [==============================] - 0s 851us/step - loss: 0.2065 - acc: 0.7713\n",
            "Epoch 11/100\n",
            "188/188 [==============================] - 0s 890us/step - loss: 0.1942 - acc: 0.7500\n",
            "Epoch 12/100\n",
            "188/188 [==============================] - 0s 942us/step - loss: 0.1870 - acc: 0.7553\n",
            "Epoch 13/100\n",
            "188/188 [==============================] - 0s 851us/step - loss: 0.1775 - acc: 0.7340\n",
            "Epoch 14/100\n",
            "188/188 [==============================] - 0s 854us/step - loss: 0.1676 - acc: 0.7926\n",
            "Epoch 15/100\n",
            "188/188 [==============================] - 0s 807us/step - loss: 0.1608 - acc: 0.8191\n",
            "Epoch 16/100\n",
            "188/188 [==============================] - 0s 875us/step - loss: 0.1553 - acc: 0.8138\n",
            "Epoch 17/100\n",
            "188/188 [==============================] - 0s 907us/step - loss: 0.1570 - acc: 0.7872\n",
            "Epoch 18/100\n",
            "188/188 [==============================] - 0s 902us/step - loss: 0.1453 - acc: 0.8245\n",
            "Epoch 19/100\n",
            "188/188 [==============================] - 0s 797us/step - loss: 0.1412 - acc: 0.8245\n",
            "Epoch 20/100\n",
            "188/188 [==============================] - 0s 811us/step - loss: 0.1386 - acc: 0.8298\n",
            "Epoch 21/100\n",
            "188/188 [==============================] - 0s 810us/step - loss: 0.1352 - acc: 0.8511\n",
            "Epoch 22/100\n",
            "188/188 [==============================] - 0s 815us/step - loss: 0.1375 - acc: 0.8085\n",
            "Epoch 23/100\n",
            "188/188 [==============================] - 0s 846us/step - loss: 0.1320 - acc: 0.8138\n",
            "Epoch 24/100\n",
            "188/188 [==============================] - 0s 941us/step - loss: 0.1310 - acc: 0.8457\n",
            "Epoch 25/100\n",
            "188/188 [==============================] - 0s 928us/step - loss: 0.1249 - acc: 0.8404\n",
            "Epoch 26/100\n",
            "188/188 [==============================] - 0s 906us/step - loss: 0.1270 - acc: 0.8404\n",
            "Epoch 27/100\n",
            "188/188 [==============================] - 0s 937us/step - loss: 0.1225 - acc: 0.8511\n",
            "Epoch 28/100\n",
            "188/188 [==============================] - 0s 885us/step - loss: 0.1203 - acc: 0.8457\n",
            "Epoch 29/100\n",
            "188/188 [==============================] - 0s 811us/step - loss: 0.1187 - acc: 0.8457\n",
            "Epoch 30/100\n",
            "188/188 [==============================] - 0s 796us/step - loss: 0.1136 - acc: 0.8511\n",
            "Epoch 31/100\n",
            "188/188 [==============================] - 0s 988us/step - loss: 0.1124 - acc: 0.8457\n",
            "Epoch 32/100\n",
            "188/188 [==============================] - 0s 899us/step - loss: 0.1086 - acc: 0.8723\n",
            "Epoch 33/100\n",
            "188/188 [==============================] - 0s 780us/step - loss: 0.1109 - acc: 0.8511\n",
            "Epoch 34/100\n",
            "188/188 [==============================] - 0s 815us/step - loss: 0.1130 - acc: 0.8245\n",
            "Epoch 35/100\n",
            "188/188 [==============================] - 0s 803us/step - loss: 0.1048 - acc: 0.8723\n",
            "Epoch 36/100\n",
            "188/188 [==============================] - 0s 838us/step - loss: 0.1028 - acc: 0.8617\n",
            "Epoch 37/100\n",
            "188/188 [==============================] - 0s 960us/step - loss: 0.1060 - acc: 0.8404\n",
            "Epoch 38/100\n",
            "188/188 [==============================] - 0s 821us/step - loss: 0.0983 - acc: 0.8670\n",
            "Epoch 39/100\n",
            "188/188 [==============================] - 0s 788us/step - loss: 0.0964 - acc: 0.8883\n",
            "Epoch 40/100\n",
            "188/188 [==============================] - 0s 819us/step - loss: 0.0940 - acc: 0.8723\n",
            "Epoch 41/100\n",
            "188/188 [==============================] - 0s 958us/step - loss: 0.0948 - acc: 0.8989\n",
            "Epoch 42/100\n",
            "188/188 [==============================] - 0s 857us/step - loss: 0.0917 - acc: 0.8883\n",
            "Epoch 43/100\n",
            "188/188 [==============================] - 0s 1ms/step - loss: 0.0896 - acc: 0.8989\n",
            "Epoch 44/100\n",
            "188/188 [==============================] - 0s 916us/step - loss: 0.0911 - acc: 0.8989\n",
            "Epoch 45/100\n",
            "188/188 [==============================] - 0s 848us/step - loss: 0.0854 - acc: 0.8936\n",
            "Epoch 46/100\n",
            "188/188 [==============================] - 0s 805us/step - loss: 0.0869 - acc: 0.9043\n",
            "Epoch 47/100\n",
            "188/188 [==============================] - 0s 781us/step - loss: 0.0821 - acc: 0.9096\n",
            "Epoch 48/100\n",
            "188/188 [==============================] - 0s 808us/step - loss: 0.0870 - acc: 0.8989\n",
            "Epoch 49/100\n",
            "188/188 [==============================] - 0s 837us/step - loss: 0.0832 - acc: 0.9043\n",
            "Epoch 50/100\n",
            "188/188 [==============================] - 0s 830us/step - loss: 0.0831 - acc: 0.9096\n",
            "Epoch 51/100\n",
            "188/188 [==============================] - 0s 807us/step - loss: 0.0767 - acc: 0.9255\n",
            "Epoch 52/100\n",
            "188/188 [==============================] - 0s 783us/step - loss: 0.0740 - acc: 0.9255\n",
            "Epoch 53/100\n",
            "188/188 [==============================] - 0s 817us/step - loss: 0.0730 - acc: 0.9096\n",
            "Epoch 54/100\n",
            "188/188 [==============================] - 0s 806us/step - loss: 0.0726 - acc: 0.9202\n",
            "Epoch 55/100\n",
            "188/188 [==============================] - 0s 866us/step - loss: 0.0732 - acc: 0.9255\n",
            "Epoch 56/100\n",
            "188/188 [==============================] - 0s 1ms/step - loss: 0.0716 - acc: 0.9255\n",
            "Epoch 57/100\n",
            "188/188 [==============================] - 0s 795us/step - loss: 0.0687 - acc: 0.9309\n",
            "Epoch 58/100\n",
            "188/188 [==============================] - 0s 802us/step - loss: 0.0698 - acc: 0.9202\n",
            "Epoch 59/100\n",
            "188/188 [==============================] - 0s 893us/step - loss: 0.0668 - acc: 0.9309\n",
            "Epoch 60/100\n",
            "188/188 [==============================] - 0s 790us/step - loss: 0.0674 - acc: 0.9255\n",
            "Epoch 61/100\n",
            "188/188 [==============================] - 0s 797us/step - loss: 0.0734 - acc: 0.8989\n",
            "Epoch 62/100\n",
            "188/188 [==============================] - 0s 1ms/step - loss: 0.0648 - acc: 0.9415\n",
            "Epoch 63/100\n",
            "188/188 [==============================] - 0s 848us/step - loss: 0.0638 - acc: 0.9309\n",
            "Epoch 64/100\n",
            "188/188 [==============================] - 0s 801us/step - loss: 0.0607 - acc: 0.9362\n",
            "Epoch 65/100\n",
            "188/188 [==============================] - 0s 922us/step - loss: 0.0625 - acc: 0.9362\n",
            "Epoch 66/100\n",
            "188/188 [==============================] - 0s 851us/step - loss: 0.0603 - acc: 0.9362\n",
            "Epoch 67/100\n",
            "188/188 [==============================] - 0s 799us/step - loss: 0.0614 - acc: 0.9362\n",
            "Epoch 68/100\n",
            "188/188 [==============================] - 0s 818us/step - loss: 0.0573 - acc: 0.9362\n",
            "Epoch 69/100\n",
            "188/188 [==============================] - 0s 930us/step - loss: 0.0581 - acc: 0.9309\n",
            "Epoch 70/100\n",
            "188/188 [==============================] - 0s 764us/step - loss: 0.0543 - acc: 0.9468\n",
            "Epoch 71/100\n",
            "188/188 [==============================] - 0s 931us/step - loss: 0.0530 - acc: 0.9468\n",
            "Epoch 72/100\n",
            "188/188 [==============================] - 0s 888us/step - loss: 0.0551 - acc: 0.9415\n",
            "Epoch 73/100\n",
            "188/188 [==============================] - 0s 949us/step - loss: 0.0546 - acc: 0.9415\n",
            "Epoch 74/100\n",
            "188/188 [==============================] - 0s 825us/step - loss: 0.0506 - acc: 0.9468\n",
            "Epoch 75/100\n",
            "188/188 [==============================] - 0s 892us/step - loss: 0.0503 - acc: 0.9574\n",
            "Epoch 76/100\n",
            "188/188 [==============================] - 0s 825us/step - loss: 0.0511 - acc: 0.9574\n",
            "Epoch 77/100\n",
            "188/188 [==============================] - 0s 851us/step - loss: 0.0492 - acc: 0.9521\n",
            "Epoch 78/100\n",
            "188/188 [==============================] - 0s 828us/step - loss: 0.0463 - acc: 0.9628\n",
            "Epoch 79/100\n",
            "188/188 [==============================] - 0s 805us/step - loss: 0.0463 - acc: 0.9574\n",
            "Epoch 80/100\n",
            "188/188 [==============================] - 0s 799us/step - loss: 0.0458 - acc: 0.9628\n",
            "Epoch 81/100\n",
            "188/188 [==============================] - 0s 849us/step - loss: 0.0446 - acc: 0.9628\n",
            "Epoch 82/100\n",
            "188/188 [==============================] - 0s 951us/step - loss: 0.0471 - acc: 0.9521\n",
            "Epoch 83/100\n",
            "188/188 [==============================] - 0s 932us/step - loss: 0.0424 - acc: 0.9734\n",
            "Epoch 84/100\n",
            "188/188 [==============================] - 0s 919us/step - loss: 0.0421 - acc: 0.9574\n",
            "Epoch 85/100\n",
            "188/188 [==============================] - 0s 924us/step - loss: 0.0417 - acc: 0.9681\n",
            "Epoch 86/100\n",
            "188/188 [==============================] - 0s 784us/step - loss: 0.0424 - acc: 0.9521\n",
            "Epoch 87/100\n",
            "188/188 [==============================] - 0s 810us/step - loss: 0.0397 - acc: 0.9628\n",
            "Epoch 88/100\n",
            "188/188 [==============================] - 0s 999us/step - loss: 0.0381 - acc: 0.9628\n",
            "Epoch 89/100\n",
            "188/188 [==============================] - 0s 857us/step - loss: 0.0409 - acc: 0.9574\n",
            "Epoch 90/100\n",
            "188/188 [==============================] - 0s 934us/step - loss: 0.0396 - acc: 0.9628\n",
            "Epoch 91/100\n",
            "188/188 [==============================] - 0s 854us/step - loss: 0.0358 - acc: 0.9734\n",
            "Epoch 92/100\n",
            "188/188 [==============================] - 0s 950us/step - loss: 0.0377 - acc: 0.9681\n",
            "Epoch 93/100\n",
            "188/188 [==============================] - 0s 855us/step - loss: 0.0340 - acc: 0.9681\n",
            "Epoch 94/100\n",
            "188/188 [==============================] - 0s 977us/step - loss: 0.0351 - acc: 0.9734\n",
            "Epoch 95/100\n",
            "188/188 [==============================] - 0s 946us/step - loss: 0.0329 - acc: 0.9787\n",
            "Epoch 96/100\n",
            "188/188 [==============================] - 0s 954us/step - loss: 0.0328 - acc: 0.9787\n",
            "Epoch 97/100\n",
            "188/188 [==============================] - 0s 835us/step - loss: 0.0316 - acc: 0.9734\n",
            "Epoch 98/100\n",
            "188/188 [==============================] - 0s 810us/step - loss: 0.0308 - acc: 0.9734\n",
            "Epoch 99/100\n",
            "188/188 [==============================] - 0s 817us/step - loss: 0.0303 - acc: 0.9787\n",
            "Epoch 100/100\n",
            "188/188 [==============================] - 0s 957us/step - loss: 0.0308 - acc: 0.9840\n",
            "20/20 [==============================] - 0s 15ms/step\n",
            "Epoch 1/100\n",
            "188/188 [==============================] - 1s 6ms/step - loss: 0.2551 - acc: 0.4840\n",
            "Epoch 2/100\n",
            "188/188 [==============================] - 0s 924us/step - loss: 0.2396 - acc: 0.5798\n",
            "Epoch 3/100\n",
            "188/188 [==============================] - 0s 946us/step - loss: 0.2295 - acc: 0.5426\n",
            "Epoch 4/100\n",
            "188/188 [==============================] - 0s 937us/step - loss: 0.2229 - acc: 0.6596\n",
            "Epoch 5/100\n",
            "188/188 [==============================] - 0s 975us/step - loss: 0.2165 - acc: 0.6649\n",
            "Epoch 6/100\n",
            "188/188 [==============================] - 0s 925us/step - loss: 0.2093 - acc: 0.7021\n",
            "Epoch 7/100\n",
            "188/188 [==============================] - 0s 978us/step - loss: 0.2030 - acc: 0.7074\n",
            "Epoch 8/100\n",
            "188/188 [==============================] - 0s 943us/step - loss: 0.1954 - acc: 0.7287\n",
            "Epoch 9/100\n",
            "188/188 [==============================] - 0s 975us/step - loss: 0.1954 - acc: 0.7340\n",
            "Epoch 10/100\n",
            "188/188 [==============================] - 0s 928us/step - loss: 0.1817 - acc: 0.7766\n",
            "Epoch 11/100\n",
            "188/188 [==============================] - 0s 908us/step - loss: 0.1766 - acc: 0.7606\n",
            "Epoch 12/100\n",
            "188/188 [==============================] - 0s 1ms/step - loss: 0.1712 - acc: 0.7713\n",
            "Epoch 13/100\n",
            "188/188 [==============================] - 0s 895us/step - loss: 0.1658 - acc: 0.8032\n",
            "Epoch 14/100\n",
            "188/188 [==============================] - 0s 1ms/step - loss: 0.1537 - acc: 0.8298\n",
            "Epoch 15/100\n",
            "188/188 [==============================] - 0s 855us/step - loss: 0.1519 - acc: 0.7979\n",
            "Epoch 16/100\n",
            "188/188 [==============================] - 0s 822us/step - loss: 0.1490 - acc: 0.7979\n",
            "Epoch 17/100\n",
            "188/188 [==============================] - 0s 906us/step - loss: 0.1415 - acc: 0.8351\n",
            "Epoch 18/100\n",
            "188/188 [==============================] - 0s 826us/step - loss: 0.1383 - acc: 0.8191\n",
            "Epoch 19/100\n",
            "188/188 [==============================] - 0s 838us/step - loss: 0.1309 - acc: 0.8245\n",
            "Epoch 20/100\n",
            "188/188 [==============================] - 0s 1ms/step - loss: 0.1367 - acc: 0.8032\n",
            "Epoch 21/100\n",
            "188/188 [==============================] - 0s 910us/step - loss: 0.1300 - acc: 0.8138\n",
            "Epoch 22/100\n",
            "188/188 [==============================] - 0s 896us/step - loss: 0.1263 - acc: 0.8404\n",
            "Epoch 23/100\n",
            "188/188 [==============================] - 0s 922us/step - loss: 0.1228 - acc: 0.8298\n",
            "Epoch 24/100\n",
            "188/188 [==============================] - 0s 912us/step - loss: 0.1290 - acc: 0.8191\n",
            "Epoch 25/100\n",
            "188/188 [==============================] - 0s 812us/step - loss: 0.1184 - acc: 0.8457\n",
            "Epoch 26/100\n",
            "188/188 [==============================] - 0s 962us/step - loss: 0.1174 - acc: 0.8298\n",
            "Epoch 27/100\n",
            "188/188 [==============================] - 0s 833us/step - loss: 0.1099 - acc: 0.8457\n",
            "Epoch 28/100\n",
            "188/188 [==============================] - 0s 857us/step - loss: 0.1103 - acc: 0.8564\n",
            "Epoch 29/100\n",
            "188/188 [==============================] - 0s 833us/step - loss: 0.1154 - acc: 0.8245\n",
            "Epoch 30/100\n",
            "188/188 [==============================] - 0s 865us/step - loss: 0.1100 - acc: 0.8617\n",
            "Epoch 31/100\n",
            "188/188 [==============================] - 0s 830us/step - loss: 0.1067 - acc: 0.8404\n",
            "Epoch 32/100\n",
            "188/188 [==============================] - 0s 913us/step - loss: 0.1031 - acc: 0.8670\n",
            "Epoch 33/100\n",
            "188/188 [==============================] - 0s 852us/step - loss: 0.1021 - acc: 0.8670\n",
            "Epoch 34/100\n",
            "188/188 [==============================] - 0s 953us/step - loss: 0.1001 - acc: 0.8723\n",
            "Epoch 35/100\n",
            "188/188 [==============================] - 0s 849us/step - loss: 0.0952 - acc: 0.8883\n",
            "Epoch 36/100\n",
            "188/188 [==============================] - 0s 923us/step - loss: 0.0916 - acc: 0.8989\n",
            "Epoch 37/100\n",
            "188/188 [==============================] - 0s 895us/step - loss: 0.0902 - acc: 0.9096\n",
            "Epoch 38/100\n",
            "188/188 [==============================] - 0s 925us/step - loss: 0.0920 - acc: 0.9043\n",
            "Epoch 39/100\n",
            "188/188 [==============================] - 0s 874us/step - loss: 0.0861 - acc: 0.8989\n",
            "Epoch 40/100\n",
            "188/188 [==============================] - 0s 858us/step - loss: 0.0872 - acc: 0.9096\n",
            "Epoch 41/100\n",
            "188/188 [==============================] - 0s 846us/step - loss: 0.0917 - acc: 0.8670\n",
            "Epoch 42/100\n",
            "188/188 [==============================] - 0s 846us/step - loss: 0.0828 - acc: 0.8989\n",
            "Epoch 43/100\n",
            "188/188 [==============================] - 0s 865us/step - loss: 0.0799 - acc: 0.9202\n",
            "Epoch 44/100\n",
            "188/188 [==============================] - 0s 939us/step - loss: 0.0746 - acc: 0.9309\n",
            "Epoch 45/100\n",
            "188/188 [==============================] - 0s 902us/step - loss: 0.0777 - acc: 0.9096\n",
            "Epoch 46/100\n",
            "188/188 [==============================] - 0s 949us/step - loss: 0.0778 - acc: 0.9043\n",
            "Epoch 47/100\n",
            "188/188 [==============================] - 0s 878us/step - loss: 0.0751 - acc: 0.9202\n",
            "Epoch 48/100\n",
            "188/188 [==============================] - 0s 918us/step - loss: 0.0701 - acc: 0.9149\n",
            "Epoch 49/100\n",
            "188/188 [==============================] - 0s 890us/step - loss: 0.0738 - acc: 0.9202\n",
            "Epoch 50/100\n",
            "188/188 [==============================] - 0s 1ms/step - loss: 0.0682 - acc: 0.9255\n",
            "Epoch 51/100\n",
            "188/188 [==============================] - 0s 854us/step - loss: 0.0662 - acc: 0.9309\n",
            "Epoch 52/100\n",
            "188/188 [==============================] - 0s 909us/step - loss: 0.0662 - acc: 0.9255\n",
            "Epoch 53/100\n",
            "188/188 [==============================] - 0s 867us/step - loss: 0.0651 - acc: 0.9255\n",
            "Epoch 54/100\n",
            "188/188 [==============================] - 0s 918us/step - loss: 0.0658 - acc: 0.9202\n",
            "Epoch 55/100\n",
            "188/188 [==============================] - 0s 851us/step - loss: 0.0642 - acc: 0.9202\n",
            "Epoch 56/100\n",
            "188/188 [==============================] - 0s 985us/step - loss: 0.0590 - acc: 0.9415\n",
            "Epoch 57/100\n",
            "188/188 [==============================] - 0s 911us/step - loss: 0.0608 - acc: 0.9468\n",
            "Epoch 58/100\n",
            "188/188 [==============================] - 0s 847us/step - loss: 0.0597 - acc: 0.9362\n",
            "Epoch 59/100\n",
            "188/188 [==============================] - 0s 805us/step - loss: 0.0572 - acc: 0.9362\n",
            "Epoch 60/100\n",
            "188/188 [==============================] - 0s 836us/step - loss: 0.0552 - acc: 0.9362\n",
            "Epoch 61/100\n",
            "188/188 [==============================] - 0s 830us/step - loss: 0.0584 - acc: 0.9309\n",
            "Epoch 62/100\n",
            "188/188 [==============================] - 0s 836us/step - loss: 0.0527 - acc: 0.9521\n",
            "Epoch 63/100\n",
            "188/188 [==============================] - 0s 887us/step - loss: 0.0525 - acc: 0.9521\n",
            "Epoch 64/100\n",
            "188/188 [==============================] - 0s 802us/step - loss: 0.0547 - acc: 0.9468\n",
            "Epoch 65/100\n",
            "188/188 [==============================] - 0s 793us/step - loss: 0.0527 - acc: 0.9468\n",
            "Epoch 66/100\n",
            "188/188 [==============================] - 0s 842us/step - loss: 0.0507 - acc: 0.9521\n",
            "Epoch 67/100\n",
            "188/188 [==============================] - 0s 838us/step - loss: 0.0520 - acc: 0.9362\n",
            "Epoch 68/100\n",
            "188/188 [==============================] - 0s 786us/step - loss: 0.0490 - acc: 0.9628\n",
            "Epoch 69/100\n",
            "188/188 [==============================] - 0s 936us/step - loss: 0.0467 - acc: 0.9468\n",
            "Epoch 70/100\n",
            "188/188 [==============================] - 0s 821us/step - loss: 0.0475 - acc: 0.9574\n",
            "Epoch 71/100\n",
            "188/188 [==============================] - 0s 787us/step - loss: 0.0455 - acc: 0.9628\n",
            "Epoch 72/100\n",
            "188/188 [==============================] - 0s 816us/step - loss: 0.0499 - acc: 0.9521\n",
            "Epoch 73/100\n",
            "188/188 [==============================] - 0s 947us/step - loss: 0.0463 - acc: 0.9628\n",
            "Epoch 74/100\n",
            "188/188 [==============================] - 0s 815us/step - loss: 0.0426 - acc: 0.9681\n",
            "Epoch 75/100\n",
            "188/188 [==============================] - 0s 876us/step - loss: 0.0445 - acc: 0.9734\n",
            "Epoch 76/100\n",
            "188/188 [==============================] - 0s 799us/step - loss: 0.0433 - acc: 0.9521\n",
            "Epoch 77/100\n",
            "188/188 [==============================] - 0s 954us/step - loss: 0.0392 - acc: 0.9787\n",
            "Epoch 78/100\n",
            "188/188 [==============================] - 0s 867us/step - loss: 0.0435 - acc: 0.9521\n",
            "Epoch 79/100\n",
            "188/188 [==============================] - 0s 930us/step - loss: 0.0398 - acc: 0.9681\n",
            "Epoch 80/100\n",
            "188/188 [==============================] - 0s 806us/step - loss: 0.0383 - acc: 0.9787\n",
            "Epoch 81/100\n",
            "188/188 [==============================] - 0s 794us/step - loss: 0.0384 - acc: 0.9734\n",
            "Epoch 82/100\n",
            "188/188 [==============================] - 0s 954us/step - loss: 0.0355 - acc: 0.9681\n",
            "Epoch 83/100\n",
            "188/188 [==============================] - 0s 835us/step - loss: 0.0347 - acc: 0.9787\n",
            "Epoch 84/100\n",
            "188/188 [==============================] - 0s 877us/step - loss: 0.0406 - acc: 0.9628\n",
            "Epoch 85/100\n",
            "188/188 [==============================] - 0s 906us/step - loss: 0.0395 - acc: 0.9734\n",
            "Epoch 86/100\n",
            "188/188 [==============================] - 0s 832us/step - loss: 0.0369 - acc: 0.9734\n",
            "Epoch 87/100\n",
            "188/188 [==============================] - 0s 836us/step - loss: 0.0327 - acc: 0.9734\n",
            "Epoch 88/100\n",
            "188/188 [==============================] - 0s 945us/step - loss: 0.0316 - acc: 0.9787\n",
            "Epoch 89/100\n",
            "188/188 [==============================] - 0s 970us/step - loss: 0.0303 - acc: 0.9787\n",
            "Epoch 90/100\n",
            "188/188 [==============================] - 0s 943us/step - loss: 0.0306 - acc: 0.9734\n",
            "Epoch 91/100\n",
            "188/188 [==============================] - 0s 953us/step - loss: 0.0296 - acc: 0.9787\n",
            "Epoch 92/100\n",
            "188/188 [==============================] - 0s 862us/step - loss: 0.0307 - acc: 0.9734\n",
            "Epoch 93/100\n",
            "188/188 [==============================] - 0s 932us/step - loss: 0.0330 - acc: 0.9787\n",
            "Epoch 94/100\n",
            "188/188 [==============================] - 0s 909us/step - loss: 0.0314 - acc: 0.9681\n",
            "Epoch 95/100\n",
            "188/188 [==============================] - 0s 886us/step - loss: 0.0271 - acc: 0.9840\n",
            "Epoch 96/100\n",
            "188/188 [==============================] - 0s 857us/step - loss: 0.0258 - acc: 0.9787\n",
            "Epoch 97/100\n",
            "188/188 [==============================] - 0s 831us/step - loss: 0.0261 - acc: 0.9787\n",
            "Epoch 98/100\n",
            "188/188 [==============================] - 0s 790us/step - loss: 0.0255 - acc: 0.9894\n",
            "Epoch 99/100\n",
            "188/188 [==============================] - 0s 809us/step - loss: 0.0262 - acc: 0.9840\n",
            "Epoch 100/100\n",
            "188/188 [==============================] - 0s 899us/step - loss: 0.0384 - acc: 0.9681\n",
            "20/20 [==============================] - 0s 15ms/step\n",
            "\n",
            " 10 fold accuracy: ['0.6667', '0.8095', '0.7619', '0.9048', '0.8571', '0.8095', '0.7619', '0.8095', '0.9500', '0.8000']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gIm_qQ1WmCef",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "1c6e0dd7-a9ab-4f6d-f4e6-ee72937333c3"
      },
      "source": [
        "'''\n",
        "와인의 종류 예측 분석\n",
        " 총 6497개의 샘플\n",
        " 13개의 속성\n",
        "'''\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense\n",
        "from keras.callbacks import ModelCheckpoint, EarlyStopping\n",
        "import pandas as pd\n",
        "import numpy\n",
        "import tensorflow as tf\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "seed = 0\n",
        "numpy.random.seed(seed) # seed 값 설정\n",
        "tf.set_random_seed(seed)\n",
        "\n",
        "df_pre = pd.read_csv('/content/drive/My Drive/Colab Notebooks/싸이킷런 머신러닝(박태정T)/data/wine.csv', header=None)\n",
        "df = df_pre.sample(frac=1) #rac = 1 지정은 원본 데이터의 100%를 불러오라는 의미\n",
        "dataset = df.values\n",
        "X = dataset[:,0:12]\n",
        "Y = dataset[:,12]\n",
        "\n",
        "model = Sequential() # 모델 설정(4개의 은닉층을 만들어 각각 30, 12, 8, 1개의 노드를 주었습니다)\n",
        "model.add(Dense(30, input_dim=12, activation='relu'))\n",
        "model.add(Dense(12, activation='relu'))\n",
        "model.add(Dense(8, activation='relu'))\n",
        "model.add(Dense(1, activation='sigmoid'))\n",
        "\n",
        "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy']) # 모델 컴파일\n",
        "model.fit(X, Y, epochs=200, batch_size=200) # 모델 실행\n",
        "print(\"\\n Accuracy: %.4f\" % (model.evaluate(X, Y)[1])) # 결과 출력"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/200\n",
            "6497/6497 [==============================] - 1s 178us/step - loss: 0.3857 - acc: 0.8038\n",
            "Epoch 2/200\n",
            "6497/6497 [==============================] - 0s 27us/step - loss: 0.2715 - acc: 0.9024\n",
            "Epoch 3/200\n",
            "6497/6497 [==============================] - 0s 26us/step - loss: 0.2205 - acc: 0.9324\n",
            "Epoch 4/200\n",
            "6497/6497 [==============================] - 0s 28us/step - loss: 0.2039 - acc: 0.9355\n",
            "Epoch 5/200\n",
            "6497/6497 [==============================] - 0s 25us/step - loss: 0.1946 - acc: 0.9378\n",
            "Epoch 6/200\n",
            "6497/6497 [==============================] - 0s 28us/step - loss: 0.1863 - acc: 0.9381\n",
            "Epoch 7/200\n",
            "6497/6497 [==============================] - 0s 24us/step - loss: 0.1838 - acc: 0.9401\n",
            "Epoch 8/200\n",
            "6497/6497 [==============================] - 0s 24us/step - loss: 0.1768 - acc: 0.9407\n",
            "Epoch 9/200\n",
            "6497/6497 [==============================] - 0s 24us/step - loss: 0.1742 - acc: 0.9403\n",
            "Epoch 10/200\n",
            "6497/6497 [==============================] - 0s 27us/step - loss: 0.1680 - acc: 0.9427\n",
            "Epoch 11/200\n",
            "6497/6497 [==============================] - 0s 26us/step - loss: 0.1663 - acc: 0.9452\n",
            "Epoch 12/200\n",
            "6497/6497 [==============================] - 0s 26us/step - loss: 0.1621 - acc: 0.9458\n",
            "Epoch 13/200\n",
            "6497/6497 [==============================] - 0s 28us/step - loss: 0.1567 - acc: 0.9475\n",
            "Epoch 14/200\n",
            "6497/6497 [==============================] - 0s 25us/step - loss: 0.1530 - acc: 0.9467\n",
            "Epoch 15/200\n",
            "6497/6497 [==============================] - 0s 25us/step - loss: 0.1530 - acc: 0.9474\n",
            "Epoch 16/200\n",
            "6497/6497 [==============================] - 0s 27us/step - loss: 0.1437 - acc: 0.9491\n",
            "Epoch 17/200\n",
            "6497/6497 [==============================] - 0s 24us/step - loss: 0.1413 - acc: 0.9489\n",
            "Epoch 18/200\n",
            "6497/6497 [==============================] - 0s 25us/step - loss: 0.1351 - acc: 0.9511\n",
            "Epoch 19/200\n",
            "6497/6497 [==============================] - 0s 25us/step - loss: 0.1332 - acc: 0.9517\n",
            "Epoch 20/200\n",
            "6497/6497 [==============================] - 0s 27us/step - loss: 0.1315 - acc: 0.9520\n",
            "Epoch 21/200\n",
            "6497/6497 [==============================] - 0s 26us/step - loss: 0.1292 - acc: 0.9547\n",
            "Epoch 22/200\n",
            "6497/6497 [==============================] - 0s 27us/step - loss: 0.1231 - acc: 0.9549\n",
            "Epoch 23/200\n",
            "6497/6497 [==============================] - 0s 29us/step - loss: 0.1226 - acc: 0.9552\n",
            "Epoch 24/200\n",
            "6497/6497 [==============================] - 0s 27us/step - loss: 0.1220 - acc: 0.9561\n",
            "Epoch 25/200\n",
            "6497/6497 [==============================] - 0s 27us/step - loss: 0.1187 - acc: 0.9581\n",
            "Epoch 26/200\n",
            "6497/6497 [==============================] - 0s 25us/step - loss: 0.1110 - acc: 0.9606\n",
            "Epoch 27/200\n",
            "6497/6497 [==============================] - 0s 24us/step - loss: 0.1111 - acc: 0.9611\n",
            "Epoch 28/200\n",
            "6497/6497 [==============================] - 0s 26us/step - loss: 0.1117 - acc: 0.9617\n",
            "Epoch 29/200\n",
            "6497/6497 [==============================] - 0s 25us/step - loss: 0.1029 - acc: 0.9654\n",
            "Epoch 30/200\n",
            "6497/6497 [==============================] - 0s 25us/step - loss: 0.1020 - acc: 0.9652\n",
            "Epoch 31/200\n",
            "6497/6497 [==============================] - 0s 25us/step - loss: 0.1015 - acc: 0.9655\n",
            "Epoch 32/200\n",
            "6497/6497 [==============================] - 0s 25us/step - loss: 0.0982 - acc: 0.9680\n",
            "Epoch 33/200\n",
            "6497/6497 [==============================] - 0s 26us/step - loss: 0.0914 - acc: 0.9709\n",
            "Epoch 34/200\n",
            "6497/6497 [==============================] - 0s 28us/step - loss: 0.0916 - acc: 0.9701\n",
            "Epoch 35/200\n",
            "6497/6497 [==============================] - 0s 27us/step - loss: 0.0908 - acc: 0.9715\n",
            "Epoch 36/200\n",
            "6497/6497 [==============================] - 0s 26us/step - loss: 0.0896 - acc: 0.9708\n",
            "Epoch 37/200\n",
            "6497/6497 [==============================] - 0s 27us/step - loss: 0.0861 - acc: 0.9737\n",
            "Epoch 38/200\n",
            "6497/6497 [==============================] - 0s 27us/step - loss: 0.0842 - acc: 0.9741\n",
            "Epoch 39/200\n",
            "6497/6497 [==============================] - 0s 27us/step - loss: 0.0869 - acc: 0.9720\n",
            "Epoch 40/200\n",
            "6497/6497 [==============================] - 0s 27us/step - loss: 0.0817 - acc: 0.9755\n",
            "Epoch 41/200\n",
            "6497/6497 [==============================] - 0s 26us/step - loss: 0.0871 - acc: 0.9720\n",
            "Epoch 42/200\n",
            "6497/6497 [==============================] - 0s 27us/step - loss: 0.0864 - acc: 0.9731\n",
            "Epoch 43/200\n",
            "6497/6497 [==============================] - 0s 28us/step - loss: 0.0875 - acc: 0.9706\n",
            "Epoch 44/200\n",
            "6497/6497 [==============================] - 0s 27us/step - loss: 0.0800 - acc: 0.9743\n",
            "Epoch 45/200\n",
            "6497/6497 [==============================] - 0s 29us/step - loss: 0.0765 - acc: 0.9755\n",
            "Epoch 46/200\n",
            "6497/6497 [==============================] - 0s 27us/step - loss: 0.0762 - acc: 0.9766\n",
            "Epoch 47/200\n",
            "6497/6497 [==============================] - 0s 25us/step - loss: 0.0729 - acc: 0.9781\n",
            "Epoch 48/200\n",
            "6497/6497 [==============================] - 0s 25us/step - loss: 0.0716 - acc: 0.9777\n",
            "Epoch 49/200\n",
            "6497/6497 [==============================] - 0s 25us/step - loss: 0.0757 - acc: 0.9749\n",
            "Epoch 50/200\n",
            "6497/6497 [==============================] - 0s 25us/step - loss: 0.0750 - acc: 0.9765\n",
            "Epoch 51/200\n",
            "6497/6497 [==============================] - 0s 25us/step - loss: 0.0699 - acc: 0.9781\n",
            "Epoch 52/200\n",
            "6497/6497 [==============================] - 0s 29us/step - loss: 0.0685 - acc: 0.9791\n",
            "Epoch 53/200\n",
            "6497/6497 [==============================] - 0s 26us/step - loss: 0.0756 - acc: 0.9780\n",
            "Epoch 54/200\n",
            "6497/6497 [==============================] - 0s 25us/step - loss: 0.0700 - acc: 0.9783\n",
            "Epoch 55/200\n",
            "6497/6497 [==============================] - 0s 25us/step - loss: 0.0748 - acc: 0.9765\n",
            "Epoch 56/200\n",
            "6497/6497 [==============================] - 0s 26us/step - loss: 0.0738 - acc: 0.9755\n",
            "Epoch 57/200\n",
            "6497/6497 [==============================] - 0s 24us/step - loss: 0.0654 - acc: 0.9794\n",
            "Epoch 58/200\n",
            "6497/6497 [==============================] - 0s 29us/step - loss: 0.0652 - acc: 0.9803\n",
            "Epoch 59/200\n",
            "6497/6497 [==============================] - 0s 29us/step - loss: 0.0673 - acc: 0.9795\n",
            "Epoch 60/200\n",
            "6497/6497 [==============================] - 0s 24us/step - loss: 0.0672 - acc: 0.9801\n",
            "Epoch 61/200\n",
            "6497/6497 [==============================] - 0s 24us/step - loss: 0.0637 - acc: 0.9811\n",
            "Epoch 62/200\n",
            "6497/6497 [==============================] - 0s 24us/step - loss: 0.0633 - acc: 0.9801\n",
            "Epoch 63/200\n",
            "6497/6497 [==============================] - 0s 24us/step - loss: 0.0676 - acc: 0.9811\n",
            "Epoch 64/200\n",
            "6497/6497 [==============================] - 0s 27us/step - loss: 0.0618 - acc: 0.9818\n",
            "Epoch 65/200\n",
            "6497/6497 [==============================] - 0s 27us/step - loss: 0.0631 - acc: 0.9811\n",
            "Epoch 66/200\n",
            "6497/6497 [==============================] - 0s 25us/step - loss: 0.0608 - acc: 0.9812\n",
            "Epoch 67/200\n",
            "6497/6497 [==============================] - 0s 28us/step - loss: 0.0613 - acc: 0.9826\n",
            "Epoch 68/200\n",
            "6497/6497 [==============================] - 0s 25us/step - loss: 0.0611 - acc: 0.9823\n",
            "Epoch 69/200\n",
            "6497/6497 [==============================] - 0s 25us/step - loss: 0.0645 - acc: 0.9795\n",
            "Epoch 70/200\n",
            "6497/6497 [==============================] - 0s 29us/step - loss: 0.0630 - acc: 0.9814\n",
            "Epoch 71/200\n",
            "6497/6497 [==============================] - 0s 24us/step - loss: 0.0623 - acc: 0.9814\n",
            "Epoch 72/200\n",
            "6497/6497 [==============================] - 0s 24us/step - loss: 0.0597 - acc: 0.9823\n",
            "Epoch 73/200\n",
            "6497/6497 [==============================] - 0s 27us/step - loss: 0.0636 - acc: 0.9809\n",
            "Epoch 74/200\n",
            "6497/6497 [==============================] - 0s 24us/step - loss: 0.0618 - acc: 0.9826\n",
            "Epoch 75/200\n",
            "6497/6497 [==============================] - 0s 25us/step - loss: 0.0592 - acc: 0.9826\n",
            "Epoch 76/200\n",
            "6497/6497 [==============================] - 0s 28us/step - loss: 0.0581 - acc: 0.9837\n",
            "Epoch 77/200\n",
            "6497/6497 [==============================] - 0s 29us/step - loss: 0.0578 - acc: 0.9835\n",
            "Epoch 78/200\n",
            "6497/6497 [==============================] - 0s 32us/step - loss: 0.0584 - acc: 0.9829\n",
            "Epoch 79/200\n",
            "6497/6497 [==============================] - 0s 30us/step - loss: 0.0585 - acc: 0.9835\n",
            "Epoch 80/200\n",
            "6497/6497 [==============================] - 0s 31us/step - loss: 0.0565 - acc: 0.9841\n",
            "Epoch 81/200\n",
            "6497/6497 [==============================] - 0s 32us/step - loss: 0.0572 - acc: 0.9837\n",
            "Epoch 82/200\n",
            "6497/6497 [==============================] - 0s 30us/step - loss: 0.0592 - acc: 0.9828\n",
            "Epoch 83/200\n",
            "6497/6497 [==============================] - 0s 28us/step - loss: 0.0567 - acc: 0.9832\n",
            "Epoch 84/200\n",
            "6497/6497 [==============================] - 0s 25us/step - loss: 0.0539 - acc: 0.9849\n",
            "Epoch 85/200\n",
            "6497/6497 [==============================] - 0s 24us/step - loss: 0.0545 - acc: 0.9845\n",
            "Epoch 86/200\n",
            "6497/6497 [==============================] - 0s 25us/step - loss: 0.0559 - acc: 0.9838\n",
            "Epoch 87/200\n",
            "6497/6497 [==============================] - 0s 27us/step - loss: 0.0640 - acc: 0.9821\n",
            "Epoch 88/200\n",
            "6497/6497 [==============================] - 0s 24us/step - loss: 0.0710 - acc: 0.9801\n",
            "Epoch 89/200\n",
            "6497/6497 [==============================] - 0s 23us/step - loss: 0.0576 - acc: 0.9841\n",
            "Epoch 90/200\n",
            "6497/6497 [==============================] - 0s 25us/step - loss: 0.0536 - acc: 0.9846\n",
            "Epoch 91/200\n",
            "6497/6497 [==============================] - 0s 25us/step - loss: 0.0546 - acc: 0.9845\n",
            "Epoch 92/200\n",
            "6497/6497 [==============================] - 0s 28us/step - loss: 0.0609 - acc: 0.9825\n",
            "Epoch 93/200\n",
            "6497/6497 [==============================] - 0s 28us/step - loss: 0.0552 - acc: 0.9837\n",
            "Epoch 94/200\n",
            "6497/6497 [==============================] - 0s 26us/step - loss: 0.0536 - acc: 0.9852\n",
            "Epoch 95/200\n",
            "6497/6497 [==============================] - 0s 25us/step - loss: 0.0563 - acc: 0.9841\n",
            "Epoch 96/200\n",
            "6497/6497 [==============================] - 0s 26us/step - loss: 0.0518 - acc: 0.9860\n",
            "Epoch 97/200\n",
            "6497/6497 [==============================] - 0s 26us/step - loss: 0.0532 - acc: 0.9848\n",
            "Epoch 98/200\n",
            "6497/6497 [==============================] - 0s 28us/step - loss: 0.0536 - acc: 0.9851\n",
            "Epoch 99/200\n",
            "6497/6497 [==============================] - 0s 27us/step - loss: 0.0535 - acc: 0.9843\n",
            "Epoch 100/200\n",
            "6497/6497 [==============================] - 0s 25us/step - loss: 0.0553 - acc: 0.9849\n",
            "Epoch 101/200\n",
            "6497/6497 [==============================] - 0s 25us/step - loss: 0.0560 - acc: 0.9848\n",
            "Epoch 102/200\n",
            "6497/6497 [==============================] - 0s 25us/step - loss: 0.0503 - acc: 0.9851\n",
            "Epoch 103/200\n",
            "6497/6497 [==============================] - 0s 26us/step - loss: 0.0530 - acc: 0.9848\n",
            "Epoch 104/200\n",
            "6497/6497 [==============================] - 0s 25us/step - loss: 0.0532 - acc: 0.9854\n",
            "Epoch 105/200\n",
            "6497/6497 [==============================] - 0s 28us/step - loss: 0.0507 - acc: 0.9860\n",
            "Epoch 106/200\n",
            "6497/6497 [==============================] - 0s 23us/step - loss: 0.0532 - acc: 0.9846\n",
            "Epoch 107/200\n",
            "6497/6497 [==============================] - 0s 24us/step - loss: 0.0509 - acc: 0.9852\n",
            "Epoch 108/200\n",
            "6497/6497 [==============================] - 0s 27us/step - loss: 0.0636 - acc: 0.9806\n",
            "Epoch 109/200\n",
            "6497/6497 [==============================] - 0s 27us/step - loss: 0.0525 - acc: 0.9852\n",
            "Epoch 110/200\n",
            "6497/6497 [==============================] - 0s 28us/step - loss: 0.0505 - acc: 0.9854\n",
            "Epoch 111/200\n",
            "6497/6497 [==============================] - 0s 27us/step - loss: 0.0504 - acc: 0.9852\n",
            "Epoch 112/200\n",
            "6497/6497 [==============================] - 0s 26us/step - loss: 0.0509 - acc: 0.9860\n",
            "Epoch 113/200\n",
            "6497/6497 [==============================] - 0s 25us/step - loss: 0.0540 - acc: 0.9843\n",
            "Epoch 114/200\n",
            "6497/6497 [==============================] - 0s 25us/step - loss: 0.0553 - acc: 0.9851\n",
            "Epoch 115/200\n",
            "6497/6497 [==============================] - 0s 25us/step - loss: 0.0518 - acc: 0.9858\n",
            "Epoch 116/200\n",
            "6497/6497 [==============================] - 0s 26us/step - loss: 0.0499 - acc: 0.9854\n",
            "Epoch 117/200\n",
            "6497/6497 [==============================] - 0s 27us/step - loss: 0.0499 - acc: 0.9863\n",
            "Epoch 118/200\n",
            "6497/6497 [==============================] - 0s 25us/step - loss: 0.0509 - acc: 0.9861\n",
            "Epoch 119/200\n",
            "6497/6497 [==============================] - 0s 25us/step - loss: 0.0539 - acc: 0.9868\n",
            "Epoch 120/200\n",
            "6497/6497 [==============================] - 0s 25us/step - loss: 0.0537 - acc: 0.9846\n",
            "Epoch 121/200\n",
            "6497/6497 [==============================] - 0s 25us/step - loss: 0.0516 - acc: 0.9857\n",
            "Epoch 122/200\n",
            "6497/6497 [==============================] - 0s 25us/step - loss: 0.0478 - acc: 0.9872\n",
            "Epoch 123/200\n",
            "6497/6497 [==============================] - 0s 27us/step - loss: 0.0508 - acc: 0.9861\n",
            "Epoch 124/200\n",
            "6497/6497 [==============================] - 0s 25us/step - loss: 0.0527 - acc: 0.9860\n",
            "Epoch 125/200\n",
            "6497/6497 [==============================] - 0s 27us/step - loss: 0.0538 - acc: 0.9849\n",
            "Epoch 126/200\n",
            "6497/6497 [==============================] - 0s 25us/step - loss: 0.0484 - acc: 0.9863\n",
            "Epoch 127/200\n",
            "6497/6497 [==============================] - 0s 28us/step - loss: 0.0510 - acc: 0.9857\n",
            "Epoch 128/200\n",
            "6497/6497 [==============================] - 0s 25us/step - loss: 0.0564 - acc: 0.9829\n",
            "Epoch 129/200\n",
            "6497/6497 [==============================] - 0s 27us/step - loss: 0.0472 - acc: 0.9874\n",
            "Epoch 130/200\n",
            "6497/6497 [==============================] - 0s 27us/step - loss: 0.0468 - acc: 0.9868\n",
            "Epoch 131/200\n",
            "6497/6497 [==============================] - 0s 26us/step - loss: 0.0498 - acc: 0.9854\n",
            "Epoch 132/200\n",
            "6497/6497 [==============================] - 0s 27us/step - loss: 0.0475 - acc: 0.9868\n",
            "Epoch 133/200\n",
            "6497/6497 [==============================] - 0s 26us/step - loss: 0.0489 - acc: 0.9868\n",
            "Epoch 134/200\n",
            "6497/6497 [==============================] - 0s 27us/step - loss: 0.0489 - acc: 0.9868\n",
            "Epoch 135/200\n",
            "6497/6497 [==============================] - 0s 30us/step - loss: 0.0471 - acc: 0.9874\n",
            "Epoch 136/200\n",
            "6497/6497 [==============================] - 0s 28us/step - loss: 0.0471 - acc: 0.9871\n",
            "Epoch 137/200\n",
            "6497/6497 [==============================] - 0s 25us/step - loss: 0.0469 - acc: 0.9874\n",
            "Epoch 138/200\n",
            "6497/6497 [==============================] - 0s 26us/step - loss: 0.0493 - acc: 0.9858\n",
            "Epoch 139/200\n",
            "6497/6497 [==============================] - 0s 25us/step - loss: 0.0476 - acc: 0.9866\n",
            "Epoch 140/200\n",
            "6497/6497 [==============================] - 0s 26us/step - loss: 0.0479 - acc: 0.9860\n",
            "Epoch 141/200\n",
            "6497/6497 [==============================] - 0s 28us/step - loss: 0.0474 - acc: 0.9868\n",
            "Epoch 142/200\n",
            "6497/6497 [==============================] - 0s 26us/step - loss: 0.0454 - acc: 0.9875\n",
            "Epoch 143/200\n",
            "6497/6497 [==============================] - 0s 26us/step - loss: 0.0502 - acc: 0.9875\n",
            "Epoch 144/200\n",
            "6497/6497 [==============================] - 0s 32us/step - loss: 0.0478 - acc: 0.9868\n",
            "Epoch 145/200\n",
            "6497/6497 [==============================] - 0s 29us/step - loss: 0.0527 - acc: 0.9858\n",
            "Epoch 146/200\n",
            "6497/6497 [==============================] - 0s 28us/step - loss: 0.0557 - acc: 0.9818\n",
            "Epoch 147/200\n",
            "6497/6497 [==============================] - 0s 27us/step - loss: 0.0505 - acc: 0.9861\n",
            "Epoch 148/200\n",
            "6497/6497 [==============================] - 0s 28us/step - loss: 0.0465 - acc: 0.9866\n",
            "Epoch 149/200\n",
            "6497/6497 [==============================] - 0s 24us/step - loss: 0.0487 - acc: 0.9868\n",
            "Epoch 150/200\n",
            "6497/6497 [==============================] - 0s 27us/step - loss: 0.0500 - acc: 0.9866\n",
            "Epoch 151/200\n",
            "6497/6497 [==============================] - 0s 26us/step - loss: 0.0465 - acc: 0.9871\n",
            "Epoch 152/200\n",
            "6497/6497 [==============================] - 0s 27us/step - loss: 0.0473 - acc: 0.9875\n",
            "Epoch 153/200\n",
            "6497/6497 [==============================] - 0s 28us/step - loss: 0.0473 - acc: 0.9866\n",
            "Epoch 154/200\n",
            "6497/6497 [==============================] - 0s 27us/step - loss: 0.0449 - acc: 0.9872\n",
            "Epoch 155/200\n",
            "6497/6497 [==============================] - 0s 27us/step - loss: 0.0458 - acc: 0.9877\n",
            "Epoch 156/200\n",
            "6497/6497 [==============================] - 0s 27us/step - loss: 0.0449 - acc: 0.9874\n",
            "Epoch 157/200\n",
            "6497/6497 [==============================] - 0s 26us/step - loss: 0.0551 - acc: 0.9858\n",
            "Epoch 158/200\n",
            "6497/6497 [==============================] - 0s 28us/step - loss: 0.0517 - acc: 0.9854\n",
            "Epoch 159/200\n",
            "6497/6497 [==============================] - 0s 27us/step - loss: 0.0447 - acc: 0.9874\n",
            "Epoch 160/200\n",
            "6497/6497 [==============================] - 0s 27us/step - loss: 0.0596 - acc: 0.9815\n",
            "Epoch 161/200\n",
            "6497/6497 [==============================] - 0s 28us/step - loss: 0.0452 - acc: 0.9874\n",
            "Epoch 162/200\n",
            "6497/6497 [==============================] - 0s 28us/step - loss: 0.0467 - acc: 0.9878\n",
            "Epoch 163/200\n",
            "6497/6497 [==============================] - 0s 26us/step - loss: 0.0463 - acc: 0.9881\n",
            "Epoch 164/200\n",
            "6497/6497 [==============================] - 0s 28us/step - loss: 0.0438 - acc: 0.9880\n",
            "Epoch 165/200\n",
            "6497/6497 [==============================] - 0s 24us/step - loss: 0.0470 - acc: 0.9868\n",
            "Epoch 166/200\n",
            "6497/6497 [==============================] - 0s 25us/step - loss: 0.0547 - acc: 0.9849\n",
            "Epoch 167/200\n",
            "6497/6497 [==============================] - 0s 26us/step - loss: 0.0482 - acc: 0.9868\n",
            "Epoch 168/200\n",
            "6497/6497 [==============================] - 0s 27us/step - loss: 0.0442 - acc: 0.9877\n",
            "Epoch 169/200\n",
            "6497/6497 [==============================] - 0s 26us/step - loss: 0.0471 - acc: 0.9875\n",
            "Epoch 170/200\n",
            "6497/6497 [==============================] - 0s 30us/step - loss: 0.0532 - acc: 0.9849\n",
            "Epoch 171/200\n",
            "6497/6497 [==============================] - 0s 24us/step - loss: 0.0480 - acc: 0.9868\n",
            "Epoch 172/200\n",
            "6497/6497 [==============================] - 0s 28us/step - loss: 0.0444 - acc: 0.9875\n",
            "Epoch 173/200\n",
            "6497/6497 [==============================] - 0s 27us/step - loss: 0.0500 - acc: 0.9869\n",
            "Epoch 174/200\n",
            "6497/6497 [==============================] - 0s 28us/step - loss: 0.0470 - acc: 0.9871\n",
            "Epoch 175/200\n",
            "6497/6497 [==============================] - 0s 25us/step - loss: 0.0434 - acc: 0.9888\n",
            "Epoch 176/200\n",
            "6497/6497 [==============================] - 0s 36us/step - loss: 0.0453 - acc: 0.9886\n",
            "Epoch 177/200\n",
            "6497/6497 [==============================] - 0s 28us/step - loss: 0.0440 - acc: 0.9885\n",
            "Epoch 178/200\n",
            "6497/6497 [==============================] - 0s 26us/step - loss: 0.0444 - acc: 0.9886\n",
            "Epoch 179/200\n",
            "6497/6497 [==============================] - 0s 24us/step - loss: 0.0447 - acc: 0.9880\n",
            "Epoch 180/200\n",
            "6497/6497 [==============================] - 0s 28us/step - loss: 0.0506 - acc: 0.9852\n",
            "Epoch 181/200\n",
            "6497/6497 [==============================] - 0s 25us/step - loss: 0.0465 - acc: 0.9877\n",
            "Epoch 182/200\n",
            "6497/6497 [==============================] - 0s 29us/step - loss: 0.0446 - acc: 0.9889\n",
            "Epoch 183/200\n",
            "6497/6497 [==============================] - 0s 28us/step - loss: 0.0480 - acc: 0.9868\n",
            "Epoch 184/200\n",
            "6497/6497 [==============================] - 0s 28us/step - loss: 0.0456 - acc: 0.9874\n",
            "Epoch 185/200\n",
            "6497/6497 [==============================] - 0s 25us/step - loss: 0.0438 - acc: 0.9877\n",
            "Epoch 186/200\n",
            "6497/6497 [==============================] - 0s 25us/step - loss: 0.0428 - acc: 0.9888\n",
            "Epoch 187/200\n",
            "6497/6497 [==============================] - 0s 25us/step - loss: 0.0438 - acc: 0.9883\n",
            "Epoch 188/200\n",
            "6497/6497 [==============================] - 0s 28us/step - loss: 0.0433 - acc: 0.9883\n",
            "Epoch 189/200\n",
            "6497/6497 [==============================] - 0s 25us/step - loss: 0.0461 - acc: 0.9875\n",
            "Epoch 190/200\n",
            "6497/6497 [==============================] - 0s 27us/step - loss: 0.0421 - acc: 0.9889\n",
            "Epoch 191/200\n",
            "6497/6497 [==============================] - 0s 27us/step - loss: 0.0479 - acc: 0.9872\n",
            "Epoch 192/200\n",
            "6497/6497 [==============================] - 0s 28us/step - loss: 0.0442 - acc: 0.9877\n",
            "Epoch 193/200\n",
            "6497/6497 [==============================] - 0s 27us/step - loss: 0.0435 - acc: 0.9891\n",
            "Epoch 194/200\n",
            "6497/6497 [==============================] - 0s 29us/step - loss: 0.0427 - acc: 0.9891\n",
            "Epoch 195/200\n",
            "6497/6497 [==============================] - 0s 27us/step - loss: 0.0434 - acc: 0.9892\n",
            "Epoch 196/200\n",
            "6497/6497 [==============================] - 0s 27us/step - loss: 0.0472 - acc: 0.9885\n",
            "Epoch 197/200\n",
            "6497/6497 [==============================] - 0s 25us/step - loss: 0.0435 - acc: 0.9886\n",
            "Epoch 198/200\n",
            "6497/6497 [==============================] - 0s 25us/step - loss: 0.0472 - acc: 0.9881\n",
            "Epoch 199/200\n",
            "6497/6497 [==============================] - 0s 28us/step - loss: 0.0473 - acc: 0.9871\n",
            "Epoch 200/200\n",
            "6497/6497 [==============================] - 0s 25us/step - loss: 0.0464 - acc: 0.9880\n",
            "6497/6497 [==============================] - 1s 111us/step\n",
            "\n",
            " Accuracy: 0.9877\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GzHSbvDPme1r",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "7882518c-b5c4-4cfe-d69b-fe427394c5dc"
      },
      "source": [
        "## 와인의 종류 예측 분석 – 모델 업데이트\n",
        "\n",
        "# 에포크(epoch)마다 모델의 정확도를 함께 기록하면서 저장\n",
        "from keras.callbacks import ModelCheckpoint\n",
        "import os \n",
        "\n",
        "# 모델 컴파일\n",
        "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "# 모델 저장 폴더 설정\n",
        "MODEL_DIR = '/content/drive/My Drive/Colab Notebooks/싸이킷런 머신러닝(박태정T)/data/output/model(wine)'\n",
        "if not os.path.exists(MODEL_DIR):\n",
        "    os.mkdir(MODEL_DIR)\n",
        "\n",
        "# 테스트 오차는 케라스 내부에서 val_loss, 학습 정확도는 acc, \n",
        "# 테스트셋 정확도는 val_acc, 학습셋 오차는 loss로 각각 기록됩니다\n",
        "\n",
        "# 모델 저장 조건 설정\n",
        "modelpath=\"/content/drive/My Drive/Colab Notebooks/싸이킷런 머신러닝(박태정T)/data/output/model(wine)/{epoch:02d}-{val_loss:.4f}.hdf5\"\n",
        "checkpointer = ModelCheckpoint(filepath=modelpath, monitor='val_loss', verbose=1, save_best_only=True)\n",
        "# 모델 실행 및 저장\n",
        "model.fit(X, Y, validation_split=0.2, epochs=200, batch_size=200,verbose=0, callbacks=[checkpointer])"
      ],
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Epoch 00001: val_loss improved from inf to 0.04092, saving model to /content/drive/My Drive/Colab Notebooks/싸이킷런 머신러닝(박태정T)/data/output/model(wine)/01-0.0409.hdf5\n",
            "\n",
            "Epoch 00002: val_loss did not improve from 0.04092\n",
            "\n",
            "Epoch 00003: val_loss did not improve from 0.04092\n",
            "\n",
            "Epoch 00004: val_loss improved from 0.04092 to 0.04081, saving model to /content/drive/My Drive/Colab Notebooks/싸이킷런 머신러닝(박태정T)/data/output/model(wine)/04-0.0408.hdf5\n",
            "\n",
            "Epoch 00005: val_loss did not improve from 0.04081\n",
            "\n",
            "Epoch 00006: val_loss improved from 0.04081 to 0.04062, saving model to /content/drive/My Drive/Colab Notebooks/싸이킷런 머신러닝(박태정T)/data/output/model(wine)/06-0.0406.hdf5\n",
            "\n",
            "Epoch 00007: val_loss did not improve from 0.04062\n",
            "\n",
            "Epoch 00008: val_loss did not improve from 0.04062\n",
            "\n",
            "Epoch 00009: val_loss did not improve from 0.04062\n",
            "\n",
            "Epoch 00010: val_loss did not improve from 0.04062\n",
            "\n",
            "Epoch 00011: val_loss did not improve from 0.04062\n",
            "\n",
            "Epoch 00012: val_loss did not improve from 0.04062\n",
            "\n",
            "Epoch 00013: val_loss did not improve from 0.04062\n",
            "\n",
            "Epoch 00014: val_loss did not improve from 0.04062\n",
            "\n",
            "Epoch 00015: val_loss did not improve from 0.04062\n",
            "\n",
            "Epoch 00016: val_loss did not improve from 0.04062\n",
            "\n",
            "Epoch 00017: val_loss did not improve from 0.04062\n",
            "\n",
            "Epoch 00018: val_loss did not improve from 0.04062\n",
            "\n",
            "Epoch 00019: val_loss did not improve from 0.04062\n",
            "\n",
            "Epoch 00020: val_loss did not improve from 0.04062\n",
            "\n",
            "Epoch 00021: val_loss did not improve from 0.04062\n",
            "\n",
            "Epoch 00022: val_loss did not improve from 0.04062\n",
            "\n",
            "Epoch 00023: val_loss did not improve from 0.04062\n",
            "\n",
            "Epoch 00024: val_loss did not improve from 0.04062\n",
            "\n",
            "Epoch 00025: val_loss did not improve from 0.04062\n",
            "\n",
            "Epoch 00026: val_loss did not improve from 0.04062\n",
            "\n",
            "Epoch 00027: val_loss did not improve from 0.04062\n",
            "\n",
            "Epoch 00028: val_loss did not improve from 0.04062\n",
            "\n",
            "Epoch 00029: val_loss did not improve from 0.04062\n",
            "\n",
            "Epoch 00030: val_loss did not improve from 0.04062\n",
            "\n",
            "Epoch 00031: val_loss did not improve from 0.04062\n",
            "\n",
            "Epoch 00032: val_loss did not improve from 0.04062\n",
            "\n",
            "Epoch 00033: val_loss did not improve from 0.04062\n",
            "\n",
            "Epoch 00034: val_loss did not improve from 0.04062\n",
            "\n",
            "Epoch 00035: val_loss did not improve from 0.04062\n",
            "\n",
            "Epoch 00036: val_loss did not improve from 0.04062\n",
            "\n",
            "Epoch 00037: val_loss did not improve from 0.04062\n",
            "\n",
            "Epoch 00038: val_loss did not improve from 0.04062\n",
            "\n",
            "Epoch 00039: val_loss did not improve from 0.04062\n",
            "\n",
            "Epoch 00040: val_loss did not improve from 0.04062\n",
            "\n",
            "Epoch 00041: val_loss did not improve from 0.04062\n",
            "\n",
            "Epoch 00042: val_loss did not improve from 0.04062\n",
            "\n",
            "Epoch 00043: val_loss did not improve from 0.04062\n",
            "\n",
            "Epoch 00044: val_loss did not improve from 0.04062\n",
            "\n",
            "Epoch 00045: val_loss did not improve from 0.04062\n",
            "\n",
            "Epoch 00046: val_loss did not improve from 0.04062\n",
            "\n",
            "Epoch 00047: val_loss did not improve from 0.04062\n",
            "\n",
            "Epoch 00048: val_loss did not improve from 0.04062\n",
            "\n",
            "Epoch 00049: val_loss did not improve from 0.04062\n",
            "\n",
            "Epoch 00050: val_loss did not improve from 0.04062\n",
            "\n",
            "Epoch 00051: val_loss did not improve from 0.04062\n",
            "\n",
            "Epoch 00052: val_loss did not improve from 0.04062\n",
            "\n",
            "Epoch 00053: val_loss did not improve from 0.04062\n",
            "\n",
            "Epoch 00054: val_loss did not improve from 0.04062\n",
            "\n",
            "Epoch 00055: val_loss did not improve from 0.04062\n",
            "\n",
            "Epoch 00056: val_loss did not improve from 0.04062\n",
            "\n",
            "Epoch 00057: val_loss did not improve from 0.04062\n",
            "\n",
            "Epoch 00058: val_loss did not improve from 0.04062\n",
            "\n",
            "Epoch 00059: val_loss did not improve from 0.04062\n",
            "\n",
            "Epoch 00060: val_loss did not improve from 0.04062\n",
            "\n",
            "Epoch 00061: val_loss did not improve from 0.04062\n",
            "\n",
            "Epoch 00062: val_loss did not improve from 0.04062\n",
            "\n",
            "Epoch 00063: val_loss did not improve from 0.04062\n",
            "\n",
            "Epoch 00064: val_loss did not improve from 0.04062\n",
            "\n",
            "Epoch 00065: val_loss did not improve from 0.04062\n",
            "\n",
            "Epoch 00066: val_loss did not improve from 0.04062\n",
            "\n",
            "Epoch 00067: val_loss did not improve from 0.04062\n",
            "\n",
            "Epoch 00068: val_loss did not improve from 0.04062\n",
            "\n",
            "Epoch 00069: val_loss did not improve from 0.04062\n",
            "\n",
            "Epoch 00070: val_loss did not improve from 0.04062\n",
            "\n",
            "Epoch 00071: val_loss did not improve from 0.04062\n",
            "\n",
            "Epoch 00072: val_loss did not improve from 0.04062\n",
            "\n",
            "Epoch 00073: val_loss did not improve from 0.04062\n",
            "\n",
            "Epoch 00074: val_loss did not improve from 0.04062\n",
            "\n",
            "Epoch 00075: val_loss did not improve from 0.04062\n",
            "\n",
            "Epoch 00076: val_loss did not improve from 0.04062\n",
            "\n",
            "Epoch 00077: val_loss did not improve from 0.04062\n",
            "\n",
            "Epoch 00078: val_loss did not improve from 0.04062\n",
            "\n",
            "Epoch 00079: val_loss did not improve from 0.04062\n",
            "\n",
            "Epoch 00080: val_loss did not improve from 0.04062\n",
            "\n",
            "Epoch 00081: val_loss did not improve from 0.04062\n",
            "\n",
            "Epoch 00082: val_loss did not improve from 0.04062\n",
            "\n",
            "Epoch 00083: val_loss did not improve from 0.04062\n",
            "\n",
            "Epoch 00084: val_loss did not improve from 0.04062\n",
            "\n",
            "Epoch 00085: val_loss did not improve from 0.04062\n",
            "\n",
            "Epoch 00086: val_loss did not improve from 0.04062\n",
            "\n",
            "Epoch 00087: val_loss did not improve from 0.04062\n",
            "\n",
            "Epoch 00088: val_loss did not improve from 0.04062\n",
            "\n",
            "Epoch 00089: val_loss did not improve from 0.04062\n",
            "\n",
            "Epoch 00090: val_loss did not improve from 0.04062\n",
            "\n",
            "Epoch 00091: val_loss did not improve from 0.04062\n",
            "\n",
            "Epoch 00092: val_loss did not improve from 0.04062\n",
            "\n",
            "Epoch 00093: val_loss did not improve from 0.04062\n",
            "\n",
            "Epoch 00094: val_loss did not improve from 0.04062\n",
            "\n",
            "Epoch 00095: val_loss did not improve from 0.04062\n",
            "\n",
            "Epoch 00096: val_loss did not improve from 0.04062\n",
            "\n",
            "Epoch 00097: val_loss did not improve from 0.04062\n",
            "\n",
            "Epoch 00098: val_loss did not improve from 0.04062\n",
            "\n",
            "Epoch 00099: val_loss did not improve from 0.04062\n",
            "\n",
            "Epoch 00100: val_loss did not improve from 0.04062\n",
            "\n",
            "Epoch 00101: val_loss did not improve from 0.04062\n",
            "\n",
            "Epoch 00102: val_loss did not improve from 0.04062\n",
            "\n",
            "Epoch 00103: val_loss did not improve from 0.04062\n",
            "\n",
            "Epoch 00104: val_loss did not improve from 0.04062\n",
            "\n",
            "Epoch 00105: val_loss did not improve from 0.04062\n",
            "\n",
            "Epoch 00106: val_loss did not improve from 0.04062\n",
            "\n",
            "Epoch 00107: val_loss did not improve from 0.04062\n",
            "\n",
            "Epoch 00108: val_loss did not improve from 0.04062\n",
            "\n",
            "Epoch 00109: val_loss did not improve from 0.04062\n",
            "\n",
            "Epoch 00110: val_loss did not improve from 0.04062\n",
            "\n",
            "Epoch 00111: val_loss did not improve from 0.04062\n",
            "\n",
            "Epoch 00112: val_loss did not improve from 0.04062\n",
            "\n",
            "Epoch 00113: val_loss did not improve from 0.04062\n",
            "\n",
            "Epoch 00114: val_loss did not improve from 0.04062\n",
            "\n",
            "Epoch 00115: val_loss did not improve from 0.04062\n",
            "\n",
            "Epoch 00116: val_loss did not improve from 0.04062\n",
            "\n",
            "Epoch 00117: val_loss did not improve from 0.04062\n",
            "\n",
            "Epoch 00118: val_loss did not improve from 0.04062\n",
            "\n",
            "Epoch 00119: val_loss did not improve from 0.04062\n",
            "\n",
            "Epoch 00120: val_loss did not improve from 0.04062\n",
            "\n",
            "Epoch 00121: val_loss did not improve from 0.04062\n",
            "\n",
            "Epoch 00122: val_loss did not improve from 0.04062\n",
            "\n",
            "Epoch 00123: val_loss did not improve from 0.04062\n",
            "\n",
            "Epoch 00124: val_loss did not improve from 0.04062\n",
            "\n",
            "Epoch 00125: val_loss did not improve from 0.04062\n",
            "\n",
            "Epoch 00126: val_loss did not improve from 0.04062\n",
            "\n",
            "Epoch 00127: val_loss did not improve from 0.04062\n",
            "\n",
            "Epoch 00128: val_loss did not improve from 0.04062\n",
            "\n",
            "Epoch 00129: val_loss did not improve from 0.04062\n",
            "\n",
            "Epoch 00130: val_loss did not improve from 0.04062\n",
            "\n",
            "Epoch 00131: val_loss did not improve from 0.04062\n",
            "\n",
            "Epoch 00132: val_loss did not improve from 0.04062\n",
            "\n",
            "Epoch 00133: val_loss did not improve from 0.04062\n",
            "\n",
            "Epoch 00134: val_loss did not improve from 0.04062\n",
            "\n",
            "Epoch 00135: val_loss did not improve from 0.04062\n",
            "\n",
            "Epoch 00136: val_loss did not improve from 0.04062\n",
            "\n",
            "Epoch 00137: val_loss did not improve from 0.04062\n",
            "\n",
            "Epoch 00138: val_loss did not improve from 0.04062\n",
            "\n",
            "Epoch 00139: val_loss did not improve from 0.04062\n",
            "\n",
            "Epoch 00140: val_loss did not improve from 0.04062\n",
            "\n",
            "Epoch 00141: val_loss did not improve from 0.04062\n",
            "\n",
            "Epoch 00142: val_loss did not improve from 0.04062\n",
            "\n",
            "Epoch 00143: val_loss did not improve from 0.04062\n",
            "\n",
            "Epoch 00144: val_loss did not improve from 0.04062\n",
            "\n",
            "Epoch 00145: val_loss did not improve from 0.04062\n",
            "\n",
            "Epoch 00146: val_loss did not improve from 0.04062\n",
            "\n",
            "Epoch 00147: val_loss did not improve from 0.04062\n",
            "\n",
            "Epoch 00148: val_loss did not improve from 0.04062\n",
            "\n",
            "Epoch 00149: val_loss did not improve from 0.04062\n",
            "\n",
            "Epoch 00150: val_loss did not improve from 0.04062\n",
            "\n",
            "Epoch 00151: val_loss did not improve from 0.04062\n",
            "\n",
            "Epoch 00152: val_loss did not improve from 0.04062\n",
            "\n",
            "Epoch 00153: val_loss did not improve from 0.04062\n",
            "\n",
            "Epoch 00154: val_loss did not improve from 0.04062\n",
            "\n",
            "Epoch 00155: val_loss did not improve from 0.04062\n",
            "\n",
            "Epoch 00156: val_loss did not improve from 0.04062\n",
            "\n",
            "Epoch 00157: val_loss did not improve from 0.04062\n",
            "\n",
            "Epoch 00158: val_loss did not improve from 0.04062\n",
            "\n",
            "Epoch 00159: val_loss did not improve from 0.04062\n",
            "\n",
            "Epoch 00160: val_loss did not improve from 0.04062\n",
            "\n",
            "Epoch 00161: val_loss did not improve from 0.04062\n",
            "\n",
            "Epoch 00162: val_loss improved from 0.04062 to 0.03990, saving model to /content/drive/My Drive/Colab Notebooks/싸이킷런 머신러닝(박태정T)/data/output/model(wine)/162-0.0399.hdf5\n",
            "\n",
            "Epoch 00163: val_loss did not improve from 0.03990\n",
            "\n",
            "Epoch 00164: val_loss did not improve from 0.03990\n",
            "\n",
            "Epoch 00165: val_loss did not improve from 0.03990\n",
            "\n",
            "Epoch 00166: val_loss did not improve from 0.03990\n",
            "\n",
            "Epoch 00167: val_loss did not improve from 0.03990\n",
            "\n",
            "Epoch 00168: val_loss did not improve from 0.03990\n",
            "\n",
            "Epoch 00169: val_loss did not improve from 0.03990\n",
            "\n",
            "Epoch 00170: val_loss did not improve from 0.03990\n",
            "\n",
            "Epoch 00171: val_loss did not improve from 0.03990\n",
            "\n",
            "Epoch 00172: val_loss did not improve from 0.03990\n",
            "\n",
            "Epoch 00173: val_loss did not improve from 0.03990\n",
            "\n",
            "Epoch 00174: val_loss did not improve from 0.03990\n",
            "\n",
            "Epoch 00175: val_loss did not improve from 0.03990\n",
            "\n",
            "Epoch 00176: val_loss did not improve from 0.03990\n",
            "\n",
            "Epoch 00177: val_loss did not improve from 0.03990\n",
            "\n",
            "Epoch 00178: val_loss did not improve from 0.03990\n",
            "\n",
            "Epoch 00179: val_loss did not improve from 0.03990\n",
            "\n",
            "Epoch 00180: val_loss did not improve from 0.03990\n",
            "\n",
            "Epoch 00181: val_loss did not improve from 0.03990\n",
            "\n",
            "Epoch 00182: val_loss did not improve from 0.03990\n",
            "\n",
            "Epoch 00183: val_loss did not improve from 0.03990\n",
            "\n",
            "Epoch 00184: val_loss did not improve from 0.03990\n",
            "\n",
            "Epoch 00185: val_loss did not improve from 0.03990\n",
            "\n",
            "Epoch 00186: val_loss did not improve from 0.03990\n",
            "\n",
            "Epoch 00187: val_loss did not improve from 0.03990\n",
            "\n",
            "Epoch 00188: val_loss did not improve from 0.03990\n",
            "\n",
            "Epoch 00189: val_loss did not improve from 0.03990\n",
            "\n",
            "Epoch 00190: val_loss did not improve from 0.03990\n",
            "\n",
            "Epoch 00191: val_loss did not improve from 0.03990\n",
            "\n",
            "Epoch 00192: val_loss did not improve from 0.03990\n",
            "\n",
            "Epoch 00193: val_loss did not improve from 0.03990\n",
            "\n",
            "Epoch 00194: val_loss did not improve from 0.03990\n",
            "\n",
            "Epoch 00195: val_loss did not improve from 0.03990\n",
            "\n",
            "Epoch 00196: val_loss did not improve from 0.03990\n",
            "\n",
            "Epoch 00197: val_loss did not improve from 0.03990\n",
            "\n",
            "Epoch 00198: val_loss did not improve from 0.03990\n",
            "\n",
            "Epoch 00199: val_loss did not improve from 0.03990\n",
            "\n",
            "Epoch 00200: val_loss did not improve from 0.03990\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7fd8f64310f0>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 32
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QYQaGmTbmwn6",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "4043dea5-6f0e-41f3-f9ab-79035515f369"
      },
      "source": [
        "# 그래프 표현\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# 모델 실행 및 저장\n",
        "# sample() 함수를 이용하여 전체 샘플 중 15%만 불러오게 하고, 배치 크기는 500으로 늘려 한 번 딥러닝을 가동할\n",
        "# 때 더 많이 입력되게끔 했습니다. 불러온 샘플 중 33%는 분리하여 테스트셋으로 사용하였습니다.\n",
        "history = model.fit(X, Y, validation_split=0.33, epochs=3500, batch_size=500)\n",
        "\n",
        "# y_vloss에 테스트셋(33%)으로 실험 결과의 오차 값을 저장\n",
        "y_vloss=history.history['val_loss']\n",
        "\n",
        "# y_acc에 학습셋(67%)으로 측정한 정확도의 값을 저장\n",
        "y_acc=history.history['acc']\n",
        "\n",
        "# x 값을 지정하고 정확도를 파란색으로, 오차를 빨간색으로 표시\n",
        "x_len = numpy.arange(len(y_acc))\n",
        "plt.plot(x_len, y_vloss, \"o\", c=\"red\", markersize=3)\n",
        "plt.plot(x_len, y_acc, \"o\", c=\"blue\", markersize=3)\n",
        "plt.show()"
      ],
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\u001b[1;30;43m스트리밍 출력 내용이 길어서 마지막 5000줄이 삭제되었습니다.\u001b[0m\n",
            "4352/4352 [==============================] - 0s 13us/step - loss: 0.0232 - acc: 0.9952 - val_loss: 0.0382 - val_acc: 0.9921\n",
            "Epoch 1002/3500\n",
            "4352/4352 [==============================] - 0s 13us/step - loss: 0.0237 - acc: 0.9938 - val_loss: 0.0410 - val_acc: 0.9911\n",
            "Epoch 1003/3500\n",
            "4352/4352 [==============================] - 0s 13us/step - loss: 0.0256 - acc: 0.9938 - val_loss: 0.0398 - val_acc: 0.9921\n",
            "Epoch 1004/3500\n",
            "4352/4352 [==============================] - 0s 14us/step - loss: 0.0250 - acc: 0.9938 - val_loss: 0.0385 - val_acc: 0.9916\n",
            "Epoch 1005/3500\n",
            "4352/4352 [==============================] - 0s 13us/step - loss: 0.0261 - acc: 0.9922 - val_loss: 0.0375 - val_acc: 0.9935\n",
            "Epoch 1006/3500\n",
            "4352/4352 [==============================] - 0s 14us/step - loss: 0.0248 - acc: 0.9940 - val_loss: 0.0371 - val_acc: 0.9916\n",
            "Epoch 1007/3500\n",
            "4352/4352 [==============================] - 0s 15us/step - loss: 0.0254 - acc: 0.9936 - val_loss: 0.0373 - val_acc: 0.9925\n",
            "Epoch 1008/3500\n",
            "4352/4352 [==============================] - 0s 14us/step - loss: 0.0273 - acc: 0.9933 - val_loss: 0.0404 - val_acc: 0.9916\n",
            "Epoch 1009/3500\n",
            "4352/4352 [==============================] - 0s 13us/step - loss: 0.0287 - acc: 0.9917 - val_loss: 0.0631 - val_acc: 0.9869\n",
            "Epoch 1010/3500\n",
            "4352/4352 [==============================] - 0s 16us/step - loss: 0.0279 - acc: 0.9933 - val_loss: 0.0467 - val_acc: 0.9902\n",
            "Epoch 1011/3500\n",
            "4352/4352 [==============================] - 0s 14us/step - loss: 0.0269 - acc: 0.9931 - val_loss: 0.0538 - val_acc: 0.9888\n",
            "Epoch 1012/3500\n",
            "4352/4352 [==============================] - 0s 15us/step - loss: 0.0287 - acc: 0.9922 - val_loss: 0.0583 - val_acc: 0.9879\n",
            "Epoch 1013/3500\n",
            "4352/4352 [==============================] - 0s 13us/step - loss: 0.0304 - acc: 0.9917 - val_loss: 0.0433 - val_acc: 0.9911\n",
            "Epoch 1014/3500\n",
            "4352/4352 [==============================] - 0s 13us/step - loss: 0.0262 - acc: 0.9936 - val_loss: 0.0383 - val_acc: 0.9930\n",
            "Epoch 1015/3500\n",
            "4352/4352 [==============================] - 0s 13us/step - loss: 0.0253 - acc: 0.9936 - val_loss: 0.0375 - val_acc: 0.9925\n",
            "Epoch 1016/3500\n",
            "4352/4352 [==============================] - 0s 13us/step - loss: 0.0240 - acc: 0.9943 - val_loss: 0.0372 - val_acc: 0.9925\n",
            "Epoch 1017/3500\n",
            "4352/4352 [==============================] - 0s 13us/step - loss: 0.0247 - acc: 0.9936 - val_loss: 0.0373 - val_acc: 0.9930\n",
            "Epoch 1018/3500\n",
            "4352/4352 [==============================] - 0s 14us/step - loss: 0.0245 - acc: 0.9933 - val_loss: 0.0374 - val_acc: 0.9911\n",
            "Epoch 1019/3500\n",
            "4352/4352 [==============================] - 0s 14us/step - loss: 0.0244 - acc: 0.9940 - val_loss: 0.0380 - val_acc: 0.9930\n",
            "Epoch 1020/3500\n",
            "4352/4352 [==============================] - 0s 14us/step - loss: 0.0259 - acc: 0.9933 - val_loss: 0.0373 - val_acc: 0.9930\n",
            "Epoch 1021/3500\n",
            "4352/4352 [==============================] - 0s 14us/step - loss: 0.0248 - acc: 0.9940 - val_loss: 0.0385 - val_acc: 0.9911\n",
            "Epoch 1022/3500\n",
            "4352/4352 [==============================] - 0s 14us/step - loss: 0.0243 - acc: 0.9938 - val_loss: 0.0389 - val_acc: 0.9916\n",
            "Epoch 1023/3500\n",
            "4352/4352 [==============================] - 0s 15us/step - loss: 0.0236 - acc: 0.9940 - val_loss: 0.0432 - val_acc: 0.9902\n",
            "Epoch 1024/3500\n",
            "4352/4352 [==============================] - 0s 14us/step - loss: 0.0229 - acc: 0.9947 - val_loss: 0.0374 - val_acc: 0.9930\n",
            "Epoch 1025/3500\n",
            "4352/4352 [==============================] - 0s 14us/step - loss: 0.0251 - acc: 0.9943 - val_loss: 0.0377 - val_acc: 0.9921\n",
            "Epoch 1026/3500\n",
            "4352/4352 [==============================] - 0s 17us/step - loss: 0.0264 - acc: 0.9926 - val_loss: 0.0372 - val_acc: 0.9921\n",
            "Epoch 1027/3500\n",
            "4352/4352 [==============================] - 0s 17us/step - loss: 0.0251 - acc: 0.9929 - val_loss: 0.0404 - val_acc: 0.9921\n",
            "Epoch 1028/3500\n",
            "4352/4352 [==============================] - 0s 13us/step - loss: 0.0232 - acc: 0.9947 - val_loss: 0.0399 - val_acc: 0.9916\n",
            "Epoch 1029/3500\n",
            "4352/4352 [==============================] - 0s 14us/step - loss: 0.0241 - acc: 0.9938 - val_loss: 0.0396 - val_acc: 0.9921\n",
            "Epoch 1030/3500\n",
            "4352/4352 [==============================] - 0s 15us/step - loss: 0.0241 - acc: 0.9940 - val_loss: 0.0412 - val_acc: 0.9907\n",
            "Epoch 1031/3500\n",
            "4352/4352 [==============================] - 0s 15us/step - loss: 0.0237 - acc: 0.9936 - val_loss: 0.0372 - val_acc: 0.9925\n",
            "Epoch 1032/3500\n",
            "4352/4352 [==============================] - 0s 14us/step - loss: 0.0255 - acc: 0.9936 - val_loss: 0.0370 - val_acc: 0.9925\n",
            "Epoch 1033/3500\n",
            "4352/4352 [==============================] - 0s 13us/step - loss: 0.0247 - acc: 0.9938 - val_loss: 0.0522 - val_acc: 0.9883\n",
            "Epoch 1034/3500\n",
            "4352/4352 [==============================] - 0s 13us/step - loss: 0.0277 - acc: 0.9929 - val_loss: 0.0462 - val_acc: 0.9911\n",
            "Epoch 1035/3500\n",
            "4352/4352 [==============================] - 0s 13us/step - loss: 0.0247 - acc: 0.9938 - val_loss: 0.0439 - val_acc: 0.9902\n",
            "Epoch 1036/3500\n",
            "4352/4352 [==============================] - 0s 13us/step - loss: 0.0246 - acc: 0.9936 - val_loss: 0.0422 - val_acc: 0.9911\n",
            "Epoch 1037/3500\n",
            "4352/4352 [==============================] - 0s 13us/step - loss: 0.0245 - acc: 0.9933 - val_loss: 0.0406 - val_acc: 0.9907\n",
            "Epoch 1038/3500\n",
            "4352/4352 [==============================] - 0s 13us/step - loss: 0.0249 - acc: 0.9936 - val_loss: 0.0378 - val_acc: 0.9925\n",
            "Epoch 1039/3500\n",
            "4352/4352 [==============================] - 0s 14us/step - loss: 0.0245 - acc: 0.9940 - val_loss: 0.0371 - val_acc: 0.9925\n",
            "Epoch 1040/3500\n",
            "4352/4352 [==============================] - 0s 13us/step - loss: 0.0272 - acc: 0.9926 - val_loss: 0.0387 - val_acc: 0.9911\n",
            "Epoch 1041/3500\n",
            "4352/4352 [==============================] - 0s 14us/step - loss: 0.0235 - acc: 0.9940 - val_loss: 0.0377 - val_acc: 0.9916\n",
            "Epoch 1042/3500\n",
            "4352/4352 [==============================] - 0s 16us/step - loss: 0.0246 - acc: 0.9931 - val_loss: 0.0384 - val_acc: 0.9930\n",
            "Epoch 1043/3500\n",
            "4352/4352 [==============================] - 0s 17us/step - loss: 0.0243 - acc: 0.9936 - val_loss: 0.0397 - val_acc: 0.9921\n",
            "Epoch 1044/3500\n",
            "4352/4352 [==============================] - 0s 14us/step - loss: 0.0237 - acc: 0.9940 - val_loss: 0.0417 - val_acc: 0.9902\n",
            "Epoch 1045/3500\n",
            "4352/4352 [==============================] - 0s 15us/step - loss: 0.0233 - acc: 0.9947 - val_loss: 0.0385 - val_acc: 0.9925\n",
            "Epoch 1046/3500\n",
            "4352/4352 [==============================] - 0s 16us/step - loss: 0.0256 - acc: 0.9931 - val_loss: 0.0377 - val_acc: 0.9925\n",
            "Epoch 1047/3500\n",
            "4352/4352 [==============================] - 0s 14us/step - loss: 0.0256 - acc: 0.9933 - val_loss: 0.0377 - val_acc: 0.9921\n",
            "Epoch 1048/3500\n",
            "4352/4352 [==============================] - 0s 15us/step - loss: 0.0229 - acc: 0.9954 - val_loss: 0.0409 - val_acc: 0.9911\n",
            "Epoch 1049/3500\n",
            "4352/4352 [==============================] - 0s 15us/step - loss: 0.0230 - acc: 0.9945 - val_loss: 0.0383 - val_acc: 0.9916\n",
            "Epoch 1050/3500\n",
            "4352/4352 [==============================] - 0s 16us/step - loss: 0.0229 - acc: 0.9949 - val_loss: 0.0399 - val_acc: 0.9911\n",
            "Epoch 1051/3500\n",
            "4352/4352 [==============================] - 0s 15us/step - loss: 0.0236 - acc: 0.9945 - val_loss: 0.0421 - val_acc: 0.9902\n",
            "Epoch 1052/3500\n",
            "4352/4352 [==============================] - 0s 15us/step - loss: 0.0236 - acc: 0.9938 - val_loss: 0.0384 - val_acc: 0.9921\n",
            "Epoch 1053/3500\n",
            "4352/4352 [==============================] - 0s 15us/step - loss: 0.0244 - acc: 0.9933 - val_loss: 0.0420 - val_acc: 0.9902\n",
            "Epoch 1054/3500\n",
            "4352/4352 [==============================] - 0s 16us/step - loss: 0.0236 - acc: 0.9940 - val_loss: 0.0430 - val_acc: 0.9902\n",
            "Epoch 1055/3500\n",
            "4352/4352 [==============================] - 0s 14us/step - loss: 0.0238 - acc: 0.9947 - val_loss: 0.0379 - val_acc: 0.9925\n",
            "Epoch 1056/3500\n",
            "4352/4352 [==============================] - 0s 13us/step - loss: 0.0241 - acc: 0.9929 - val_loss: 0.0385 - val_acc: 0.9925\n",
            "Epoch 1057/3500\n",
            "4352/4352 [==============================] - 0s 15us/step - loss: 0.0252 - acc: 0.9936 - val_loss: 0.0374 - val_acc: 0.9930\n",
            "Epoch 1058/3500\n",
            "4352/4352 [==============================] - 0s 18us/step - loss: 0.0257 - acc: 0.9929 - val_loss: 0.0407 - val_acc: 0.9921\n",
            "Epoch 1059/3500\n",
            "4352/4352 [==============================] - 0s 14us/step - loss: 0.0237 - acc: 0.9936 - val_loss: 0.0477 - val_acc: 0.9897\n",
            "Epoch 1060/3500\n",
            "4352/4352 [==============================] - 0s 14us/step - loss: 0.0241 - acc: 0.9938 - val_loss: 0.0380 - val_acc: 0.9907\n",
            "Epoch 1061/3500\n",
            "4352/4352 [==============================] - 0s 14us/step - loss: 0.0246 - acc: 0.9945 - val_loss: 0.0381 - val_acc: 0.9921\n",
            "Epoch 1062/3500\n",
            "4352/4352 [==============================] - 0s 15us/step - loss: 0.0238 - acc: 0.9938 - val_loss: 0.0387 - val_acc: 0.9925\n",
            "Epoch 1063/3500\n",
            "4352/4352 [==============================] - 0s 14us/step - loss: 0.0251 - acc: 0.9945 - val_loss: 0.0371 - val_acc: 0.9930\n",
            "Epoch 1064/3500\n",
            "4352/4352 [==============================] - 0s 14us/step - loss: 0.0236 - acc: 0.9943 - val_loss: 0.0375 - val_acc: 0.9925\n",
            "Epoch 1065/3500\n",
            "4352/4352 [==============================] - 0s 14us/step - loss: 0.0237 - acc: 0.9943 - val_loss: 0.0391 - val_acc: 0.9921\n",
            "Epoch 1066/3500\n",
            "4352/4352 [==============================] - 0s 14us/step - loss: 0.0231 - acc: 0.9943 - val_loss: 0.0380 - val_acc: 0.9921\n",
            "Epoch 1067/3500\n",
            "4352/4352 [==============================] - 0s 14us/step - loss: 0.0232 - acc: 0.9943 - val_loss: 0.0391 - val_acc: 0.9925\n",
            "Epoch 1068/3500\n",
            "4352/4352 [==============================] - 0s 15us/step - loss: 0.0231 - acc: 0.9943 - val_loss: 0.0390 - val_acc: 0.9916\n",
            "Epoch 1069/3500\n",
            "4352/4352 [==============================] - 0s 14us/step - loss: 0.0234 - acc: 0.9938 - val_loss: 0.0407 - val_acc: 0.9916\n",
            "Epoch 1070/3500\n",
            "4352/4352 [==============================] - 0s 14us/step - loss: 0.0259 - acc: 0.9931 - val_loss: 0.0447 - val_acc: 0.9911\n",
            "Epoch 1071/3500\n",
            "4352/4352 [==============================] - 0s 14us/step - loss: 0.0261 - acc: 0.9920 - val_loss: 0.0389 - val_acc: 0.9921\n",
            "Epoch 1072/3500\n",
            "4352/4352 [==============================] - 0s 15us/step - loss: 0.0272 - acc: 0.9924 - val_loss: 0.0387 - val_acc: 0.9925\n",
            "Epoch 1073/3500\n",
            "4352/4352 [==============================] - 0s 15us/step - loss: 0.0240 - acc: 0.9938 - val_loss: 0.0374 - val_acc: 0.9921\n",
            "Epoch 1074/3500\n",
            "4352/4352 [==============================] - 0s 17us/step - loss: 0.0247 - acc: 0.9926 - val_loss: 0.0391 - val_acc: 0.9925\n",
            "Epoch 1075/3500\n",
            "4352/4352 [==============================] - 0s 14us/step - loss: 0.0248 - acc: 0.9945 - val_loss: 0.0379 - val_acc: 0.9930\n",
            "Epoch 1076/3500\n",
            "4352/4352 [==============================] - 0s 14us/step - loss: 0.0245 - acc: 0.9943 - val_loss: 0.0412 - val_acc: 0.9921\n",
            "Epoch 1077/3500\n",
            "4352/4352 [==============================] - 0s 14us/step - loss: 0.0241 - acc: 0.9940 - val_loss: 0.0420 - val_acc: 0.9911\n",
            "Epoch 1078/3500\n",
            "4352/4352 [==============================] - 0s 14us/step - loss: 0.0252 - acc: 0.9933 - val_loss: 0.0422 - val_acc: 0.9921\n",
            "Epoch 1079/3500\n",
            "4352/4352 [==============================] - 0s 14us/step - loss: 0.0245 - acc: 0.9936 - val_loss: 0.0444 - val_acc: 0.9902\n",
            "Epoch 1080/3500\n",
            "4352/4352 [==============================] - 0s 13us/step - loss: 0.0236 - acc: 0.9945 - val_loss: 0.0374 - val_acc: 0.9921\n",
            "Epoch 1081/3500\n",
            "4352/4352 [==============================] - 0s 14us/step - loss: 0.0243 - acc: 0.9943 - val_loss: 0.0376 - val_acc: 0.9921\n",
            "Epoch 1082/3500\n",
            "4352/4352 [==============================] - 0s 13us/step - loss: 0.0237 - acc: 0.9940 - val_loss: 0.0379 - val_acc: 0.9930\n",
            "Epoch 1083/3500\n",
            "4352/4352 [==============================] - 0s 14us/step - loss: 0.0246 - acc: 0.9936 - val_loss: 0.0406 - val_acc: 0.9916\n",
            "Epoch 1084/3500\n",
            "4352/4352 [==============================] - 0s 14us/step - loss: 0.0259 - acc: 0.9926 - val_loss: 0.0382 - val_acc: 0.9925\n",
            "Epoch 1085/3500\n",
            "4352/4352 [==============================] - 0s 13us/step - loss: 0.0266 - acc: 0.9922 - val_loss: 0.0496 - val_acc: 0.9888\n",
            "Epoch 1086/3500\n",
            "4352/4352 [==============================] - 0s 13us/step - loss: 0.0250 - acc: 0.9940 - val_loss: 0.0451 - val_acc: 0.9902\n",
            "Epoch 1087/3500\n",
            "4352/4352 [==============================] - 0s 14us/step - loss: 0.0238 - acc: 0.9938 - val_loss: 0.0392 - val_acc: 0.9921\n",
            "Epoch 1088/3500\n",
            "4352/4352 [==============================] - 0s 14us/step - loss: 0.0236 - acc: 0.9938 - val_loss: 0.0416 - val_acc: 0.9907\n",
            "Epoch 1089/3500\n",
            "4352/4352 [==============================] - 0s 15us/step - loss: 0.0243 - acc: 0.9931 - val_loss: 0.0434 - val_acc: 0.9916\n",
            "Epoch 1090/3500\n",
            "4352/4352 [==============================] - 0s 18us/step - loss: 0.0239 - acc: 0.9947 - val_loss: 0.0377 - val_acc: 0.9930\n",
            "Epoch 1091/3500\n",
            "4352/4352 [==============================] - 0s 13us/step - loss: 0.0239 - acc: 0.9945 - val_loss: 0.0382 - val_acc: 0.9925\n",
            "Epoch 1092/3500\n",
            "4352/4352 [==============================] - 0s 14us/step - loss: 0.0242 - acc: 0.9940 - val_loss: 0.0377 - val_acc: 0.9921\n",
            "Epoch 1093/3500\n",
            "4352/4352 [==============================] - 0s 13us/step - loss: 0.0259 - acc: 0.9929 - val_loss: 0.0391 - val_acc: 0.9925\n",
            "Epoch 1094/3500\n",
            "4352/4352 [==============================] - 0s 12us/step - loss: 0.0239 - acc: 0.9936 - val_loss: 0.0429 - val_acc: 0.9907\n",
            "Epoch 1095/3500\n",
            "4352/4352 [==============================] - 0s 13us/step - loss: 0.0229 - acc: 0.9949 - val_loss: 0.0415 - val_acc: 0.9902\n",
            "Epoch 1096/3500\n",
            "4352/4352 [==============================] - 0s 14us/step - loss: 0.0228 - acc: 0.9949 - val_loss: 0.0407 - val_acc: 0.9921\n",
            "Epoch 1097/3500\n",
            "4352/4352 [==============================] - 0s 13us/step - loss: 0.0226 - acc: 0.9952 - val_loss: 0.0381 - val_acc: 0.9925\n",
            "Epoch 1098/3500\n",
            "4352/4352 [==============================] - 0s 14us/step - loss: 0.0241 - acc: 0.9938 - val_loss: 0.0386 - val_acc: 0.9916\n",
            "Epoch 1099/3500\n",
            "4352/4352 [==============================] - 0s 14us/step - loss: 0.0236 - acc: 0.9940 - val_loss: 0.0379 - val_acc: 0.9916\n",
            "Epoch 1100/3500\n",
            "4352/4352 [==============================] - 0s 16us/step - loss: 0.0235 - acc: 0.9945 - val_loss: 0.0563 - val_acc: 0.9879\n",
            "Epoch 1101/3500\n",
            "4352/4352 [==============================] - 0s 16us/step - loss: 0.0254 - acc: 0.9940 - val_loss: 0.0394 - val_acc: 0.9921\n",
            "Epoch 1102/3500\n",
            "4352/4352 [==============================] - 0s 14us/step - loss: 0.0248 - acc: 0.9940 - val_loss: 0.0382 - val_acc: 0.9921\n",
            "Epoch 1103/3500\n",
            "4352/4352 [==============================] - 0s 13us/step - loss: 0.0231 - acc: 0.9945 - val_loss: 0.0385 - val_acc: 0.9921\n",
            "Epoch 1104/3500\n",
            "4352/4352 [==============================] - 0s 15us/step - loss: 0.0244 - acc: 0.9936 - val_loss: 0.0387 - val_acc: 0.9921\n",
            "Epoch 1105/3500\n",
            "4352/4352 [==============================] - 0s 14us/step - loss: 0.0242 - acc: 0.9940 - val_loss: 0.0384 - val_acc: 0.9930\n",
            "Epoch 1106/3500\n",
            "4352/4352 [==============================] - 0s 16us/step - loss: 0.0257 - acc: 0.9931 - val_loss: 0.0378 - val_acc: 0.9925\n",
            "Epoch 1107/3500\n",
            "4352/4352 [==============================] - 0s 14us/step - loss: 0.0258 - acc: 0.9936 - val_loss: 0.0392 - val_acc: 0.9925\n",
            "Epoch 1108/3500\n",
            "4352/4352 [==============================] - 0s 14us/step - loss: 0.0235 - acc: 0.9943 - val_loss: 0.0428 - val_acc: 0.9911\n",
            "Epoch 1109/3500\n",
            "4352/4352 [==============================] - 0s 14us/step - loss: 0.0243 - acc: 0.9936 - val_loss: 0.0424 - val_acc: 0.9902\n",
            "Epoch 1110/3500\n",
            "4352/4352 [==============================] - 0s 12us/step - loss: 0.0235 - acc: 0.9943 - val_loss: 0.0490 - val_acc: 0.9893\n",
            "Epoch 1111/3500\n",
            "4352/4352 [==============================] - 0s 15us/step - loss: 0.0251 - acc: 0.9931 - val_loss: 0.0429 - val_acc: 0.9911\n",
            "Epoch 1112/3500\n",
            "4352/4352 [==============================] - 0s 14us/step - loss: 0.0284 - acc: 0.9926 - val_loss: 0.0399 - val_acc: 0.9921\n",
            "Epoch 1113/3500\n",
            "4352/4352 [==============================] - 0s 13us/step - loss: 0.0365 - acc: 0.9897 - val_loss: 0.0388 - val_acc: 0.9921\n",
            "Epoch 1114/3500\n",
            "4352/4352 [==============================] - 0s 13us/step - loss: 0.0285 - acc: 0.9926 - val_loss: 0.0388 - val_acc: 0.9921\n",
            "Epoch 1115/3500\n",
            "4352/4352 [==============================] - 0s 15us/step - loss: 0.0256 - acc: 0.9933 - val_loss: 0.0415 - val_acc: 0.9911\n",
            "Epoch 1116/3500\n",
            "4352/4352 [==============================] - 0s 14us/step - loss: 0.0243 - acc: 0.9931 - val_loss: 0.0384 - val_acc: 0.9921\n",
            "Epoch 1117/3500\n",
            "4352/4352 [==============================] - 0s 13us/step - loss: 0.0229 - acc: 0.9940 - val_loss: 0.0395 - val_acc: 0.9921\n",
            "Epoch 1118/3500\n",
            "4352/4352 [==============================] - 0s 14us/step - loss: 0.0230 - acc: 0.9949 - val_loss: 0.0386 - val_acc: 0.9916\n",
            "Epoch 1119/3500\n",
            "4352/4352 [==============================] - 0s 13us/step - loss: 0.0239 - acc: 0.9931 - val_loss: 0.0406 - val_acc: 0.9921\n",
            "Epoch 1120/3500\n",
            "4352/4352 [==============================] - 0s 15us/step - loss: 0.0230 - acc: 0.9945 - val_loss: 0.0377 - val_acc: 0.9930\n",
            "Epoch 1121/3500\n",
            "4352/4352 [==============================] - 0s 15us/step - loss: 0.0264 - acc: 0.9917 - val_loss: 0.0399 - val_acc: 0.9925\n",
            "Epoch 1122/3500\n",
            "4352/4352 [==============================] - 0s 17us/step - loss: 0.0230 - acc: 0.9943 - val_loss: 0.0377 - val_acc: 0.9921\n",
            "Epoch 1123/3500\n",
            "4352/4352 [==============================] - 0s 13us/step - loss: 0.0243 - acc: 0.9924 - val_loss: 0.0381 - val_acc: 0.9916\n",
            "Epoch 1124/3500\n",
            "4352/4352 [==============================] - 0s 13us/step - loss: 0.0249 - acc: 0.9933 - val_loss: 0.0388 - val_acc: 0.9925\n",
            "Epoch 1125/3500\n",
            "4352/4352 [==============================] - 0s 14us/step - loss: 0.0308 - acc: 0.9906 - val_loss: 0.0376 - val_acc: 0.9930\n",
            "Epoch 1126/3500\n",
            "4352/4352 [==============================] - 0s 14us/step - loss: 0.0381 - acc: 0.9878 - val_loss: 0.0407 - val_acc: 0.9921\n",
            "Epoch 1127/3500\n",
            "4352/4352 [==============================] - 0s 14us/step - loss: 0.0371 - acc: 0.9897 - val_loss: 0.0389 - val_acc: 0.9916\n",
            "Epoch 1128/3500\n",
            "4352/4352 [==============================] - 0s 14us/step - loss: 0.0277 - acc: 0.9915 - val_loss: 0.0425 - val_acc: 0.9916\n",
            "Epoch 1129/3500\n",
            "4352/4352 [==============================] - 0s 14us/step - loss: 0.0236 - acc: 0.9938 - val_loss: 0.0469 - val_acc: 0.9907\n",
            "Epoch 1130/3500\n",
            "4352/4352 [==============================] - 0s 14us/step - loss: 0.0232 - acc: 0.9945 - val_loss: 0.0384 - val_acc: 0.9925\n",
            "Epoch 1131/3500\n",
            "4352/4352 [==============================] - 0s 15us/step - loss: 0.0232 - acc: 0.9945 - val_loss: 0.0395 - val_acc: 0.9921\n",
            "Epoch 1132/3500\n",
            "4352/4352 [==============================] - 0s 13us/step - loss: 0.0230 - acc: 0.9940 - val_loss: 0.0385 - val_acc: 0.9930\n",
            "Epoch 1133/3500\n",
            "4352/4352 [==============================] - 0s 13us/step - loss: 0.0261 - acc: 0.9938 - val_loss: 0.0390 - val_acc: 0.9925\n",
            "Epoch 1134/3500\n",
            "4352/4352 [==============================] - 0s 13us/step - loss: 0.0241 - acc: 0.9940 - val_loss: 0.0440 - val_acc: 0.9907\n",
            "Epoch 1135/3500\n",
            "4352/4352 [==============================] - 0s 13us/step - loss: 0.0243 - acc: 0.9943 - val_loss: 0.0530 - val_acc: 0.9888\n",
            "Epoch 1136/3500\n",
            "4352/4352 [==============================] - 0s 14us/step - loss: 0.0277 - acc: 0.9917 - val_loss: 0.0469 - val_acc: 0.9902\n",
            "Epoch 1137/3500\n",
            "4352/4352 [==============================] - 0s 13us/step - loss: 0.0310 - acc: 0.9910 - val_loss: 0.0378 - val_acc: 0.9930\n",
            "Epoch 1138/3500\n",
            "4352/4352 [==============================] - 0s 17us/step - loss: 0.0254 - acc: 0.9929 - val_loss: 0.0401 - val_acc: 0.9916\n",
            "Epoch 1139/3500\n",
            "4352/4352 [==============================] - 0s 16us/step - loss: 0.0233 - acc: 0.9943 - val_loss: 0.0408 - val_acc: 0.9916\n",
            "Epoch 1140/3500\n",
            "4352/4352 [==============================] - 0s 14us/step - loss: 0.0229 - acc: 0.9938 - val_loss: 0.0421 - val_acc: 0.9911\n",
            "Epoch 1141/3500\n",
            "4352/4352 [==============================] - 0s 14us/step - loss: 0.0228 - acc: 0.9945 - val_loss: 0.0408 - val_acc: 0.9925\n",
            "Epoch 1142/3500\n",
            "4352/4352 [==============================] - 0s 14us/step - loss: 0.0234 - acc: 0.9940 - val_loss: 0.0428 - val_acc: 0.9911\n",
            "Epoch 1143/3500\n",
            "4352/4352 [==============================] - 0s 13us/step - loss: 0.0230 - acc: 0.9949 - val_loss: 0.0389 - val_acc: 0.9916\n",
            "Epoch 1144/3500\n",
            "4352/4352 [==============================] - 0s 14us/step - loss: 0.0232 - acc: 0.9947 - val_loss: 0.0422 - val_acc: 0.9921\n",
            "Epoch 1145/3500\n",
            "4352/4352 [==============================] - 0s 13us/step - loss: 0.0240 - acc: 0.9938 - val_loss: 0.0419 - val_acc: 0.9916\n",
            "Epoch 1146/3500\n",
            "4352/4352 [==============================] - 0s 14us/step - loss: 0.0234 - acc: 0.9933 - val_loss: 0.0532 - val_acc: 0.9893\n",
            "Epoch 1147/3500\n",
            "4352/4352 [==============================] - 0s 14us/step - loss: 0.0250 - acc: 0.9936 - val_loss: 0.0386 - val_acc: 0.9925\n",
            "Epoch 1148/3500\n",
            "4352/4352 [==============================] - 0s 15us/step - loss: 0.0241 - acc: 0.9931 - val_loss: 0.0387 - val_acc: 0.9916\n",
            "Epoch 1149/3500\n",
            "4352/4352 [==============================] - 0s 14us/step - loss: 0.0239 - acc: 0.9943 - val_loss: 0.0392 - val_acc: 0.9916\n",
            "Epoch 1150/3500\n",
            "4352/4352 [==============================] - 0s 13us/step - loss: 0.0241 - acc: 0.9936 - val_loss: 0.0380 - val_acc: 0.9930\n",
            "Epoch 1151/3500\n",
            "4352/4352 [==============================] - 0s 13us/step - loss: 0.0254 - acc: 0.9938 - val_loss: 0.0382 - val_acc: 0.9925\n",
            "Epoch 1152/3500\n",
            "4352/4352 [==============================] - 0s 14us/step - loss: 0.0270 - acc: 0.9922 - val_loss: 0.0380 - val_acc: 0.9935\n",
            "Epoch 1153/3500\n",
            "4352/4352 [==============================] - 0s 13us/step - loss: 0.0255 - acc: 0.9933 - val_loss: 0.0408 - val_acc: 0.9907\n",
            "Epoch 1154/3500\n",
            "4352/4352 [==============================] - 0s 13us/step - loss: 0.0230 - acc: 0.9945 - val_loss: 0.0424 - val_acc: 0.9907\n",
            "Epoch 1155/3500\n",
            "4352/4352 [==============================] - 0s 15us/step - loss: 0.0238 - acc: 0.9940 - val_loss: 0.0447 - val_acc: 0.9911\n",
            "Epoch 1156/3500\n",
            "4352/4352 [==============================] - 0s 14us/step - loss: 0.0245 - acc: 0.9945 - val_loss: 0.0525 - val_acc: 0.9888\n",
            "Epoch 1157/3500\n",
            "4352/4352 [==============================] - 0s 14us/step - loss: 0.0285 - acc: 0.9933 - val_loss: 0.0509 - val_acc: 0.9888\n",
            "Epoch 1158/3500\n",
            "4352/4352 [==============================] - 0s 15us/step - loss: 0.0278 - acc: 0.9929 - val_loss: 0.0381 - val_acc: 0.9925\n",
            "Epoch 1159/3500\n",
            "4352/4352 [==============================] - 0s 15us/step - loss: 0.0289 - acc: 0.9926 - val_loss: 0.0390 - val_acc: 0.9925\n",
            "Epoch 1160/3500\n",
            "4352/4352 [==============================] - 0s 16us/step - loss: 0.0234 - acc: 0.9940 - val_loss: 0.0383 - val_acc: 0.9916\n",
            "Epoch 1161/3500\n",
            "4352/4352 [==============================] - 0s 14us/step - loss: 0.0245 - acc: 0.9940 - val_loss: 0.0416 - val_acc: 0.9921\n",
            "Epoch 1162/3500\n",
            "4352/4352 [==============================] - 0s 14us/step - loss: 0.0231 - acc: 0.9936 - val_loss: 0.0390 - val_acc: 0.9916\n",
            "Epoch 1163/3500\n",
            "4352/4352 [==============================] - 0s 14us/step - loss: 0.0232 - acc: 0.9940 - val_loss: 0.0388 - val_acc: 0.9916\n",
            "Epoch 1164/3500\n",
            "4352/4352 [==============================] - 0s 15us/step - loss: 0.0232 - acc: 0.9949 - val_loss: 0.0409 - val_acc: 0.9925\n",
            "Epoch 1165/3500\n",
            "4352/4352 [==============================] - 0s 14us/step - loss: 0.0240 - acc: 0.9936 - val_loss: 0.0395 - val_acc: 0.9925\n",
            "Epoch 1166/3500\n",
            "4352/4352 [==============================] - 0s 14us/step - loss: 0.0235 - acc: 0.9938 - val_loss: 0.0421 - val_acc: 0.9902\n",
            "Epoch 1167/3500\n",
            "4352/4352 [==============================] - 0s 13us/step - loss: 0.0231 - acc: 0.9940 - val_loss: 0.0471 - val_acc: 0.9907\n",
            "Epoch 1168/3500\n",
            "4352/4352 [==============================] - 0s 15us/step - loss: 0.0244 - acc: 0.9938 - val_loss: 0.0475 - val_acc: 0.9902\n",
            "Epoch 1169/3500\n",
            "4352/4352 [==============================] - 0s 14us/step - loss: 0.0235 - acc: 0.9945 - val_loss: 0.0389 - val_acc: 0.9921\n",
            "Epoch 1170/3500\n",
            "4352/4352 [==============================] - 0s 13us/step - loss: 0.0226 - acc: 0.9952 - val_loss: 0.0407 - val_acc: 0.9921\n",
            "Epoch 1171/3500\n",
            "4352/4352 [==============================] - 0s 16us/step - loss: 0.0227 - acc: 0.9943 - val_loss: 0.0489 - val_acc: 0.9897\n",
            "Epoch 1172/3500\n",
            "4352/4352 [==============================] - 0s 15us/step - loss: 0.0280 - acc: 0.9922 - val_loss: 0.0446 - val_acc: 0.9911\n",
            "Epoch 1173/3500\n",
            "4352/4352 [==============================] - 0s 14us/step - loss: 0.0265 - acc: 0.9924 - val_loss: 0.0423 - val_acc: 0.9916\n",
            "Epoch 1174/3500\n",
            "4352/4352 [==============================] - 0s 15us/step - loss: 0.0269 - acc: 0.9936 - val_loss: 0.0390 - val_acc: 0.9925\n",
            "Epoch 1175/3500\n",
            "4352/4352 [==============================] - 0s 16us/step - loss: 0.0280 - acc: 0.9913 - val_loss: 0.0395 - val_acc: 0.9921\n",
            "Epoch 1176/3500\n",
            "4352/4352 [==============================] - 0s 15us/step - loss: 0.0252 - acc: 0.9936 - val_loss: 0.0385 - val_acc: 0.9925\n",
            "Epoch 1177/3500\n",
            "4352/4352 [==============================] - 0s 14us/step - loss: 0.0248 - acc: 0.9943 - val_loss: 0.0397 - val_acc: 0.9916\n",
            "Epoch 1178/3500\n",
            "4352/4352 [==============================] - 0s 16us/step - loss: 0.0239 - acc: 0.9938 - val_loss: 0.0416 - val_acc: 0.9911\n",
            "Epoch 1179/3500\n",
            "4352/4352 [==============================] - 0s 15us/step - loss: 0.0226 - acc: 0.9938 - val_loss: 0.0404 - val_acc: 0.9916\n",
            "Epoch 1180/3500\n",
            "4352/4352 [==============================] - 0s 15us/step - loss: 0.0232 - acc: 0.9943 - val_loss: 0.0385 - val_acc: 0.9921\n",
            "Epoch 1181/3500\n",
            "4352/4352 [==============================] - 0s 15us/step - loss: 0.0245 - acc: 0.9938 - val_loss: 0.0400 - val_acc: 0.9921\n",
            "Epoch 1182/3500\n",
            "4352/4352 [==============================] - 0s 15us/step - loss: 0.0238 - acc: 0.9938 - val_loss: 0.0406 - val_acc: 0.9925\n",
            "Epoch 1183/3500\n",
            "4352/4352 [==============================] - 0s 14us/step - loss: 0.0243 - acc: 0.9931 - val_loss: 0.0409 - val_acc: 0.9911\n",
            "Epoch 1184/3500\n",
            "4352/4352 [==============================] - 0s 13us/step - loss: 0.0236 - acc: 0.9945 - val_loss: 0.0467 - val_acc: 0.9907\n",
            "Epoch 1185/3500\n",
            "4352/4352 [==============================] - 0s 13us/step - loss: 0.0232 - acc: 0.9943 - val_loss: 0.0438 - val_acc: 0.9907\n",
            "Epoch 1186/3500\n",
            "4352/4352 [==============================] - 0s 13us/step - loss: 0.0241 - acc: 0.9936 - val_loss: 0.0402 - val_acc: 0.9925\n",
            "Epoch 1187/3500\n",
            "4352/4352 [==============================] - 0s 18us/step - loss: 0.0255 - acc: 0.9933 - val_loss: 0.0386 - val_acc: 0.9916\n",
            "Epoch 1188/3500\n",
            "4352/4352 [==============================] - 0s 15us/step - loss: 0.0245 - acc: 0.9949 - val_loss: 0.0406 - val_acc: 0.9925\n",
            "Epoch 1189/3500\n",
            "4352/4352 [==============================] - 0s 14us/step - loss: 0.0259 - acc: 0.9943 - val_loss: 0.0400 - val_acc: 0.9930\n",
            "Epoch 1190/3500\n",
            "4352/4352 [==============================] - 0s 15us/step - loss: 0.0258 - acc: 0.9926 - val_loss: 0.0402 - val_acc: 0.9916\n",
            "Epoch 1191/3500\n",
            "4352/4352 [==============================] - 0s 14us/step - loss: 0.0239 - acc: 0.9943 - val_loss: 0.0385 - val_acc: 0.9925\n",
            "Epoch 1192/3500\n",
            "4352/4352 [==============================] - 0s 14us/step - loss: 0.0233 - acc: 0.9947 - val_loss: 0.0453 - val_acc: 0.9907\n",
            "Epoch 1193/3500\n",
            "4352/4352 [==============================] - 0s 14us/step - loss: 0.0232 - acc: 0.9947 - val_loss: 0.0457 - val_acc: 0.9902\n",
            "Epoch 1194/3500\n",
            "4352/4352 [==============================] - 0s 14us/step - loss: 0.0233 - acc: 0.9949 - val_loss: 0.0404 - val_acc: 0.9911\n",
            "Epoch 1195/3500\n",
            "4352/4352 [==============================] - 0s 15us/step - loss: 0.0232 - acc: 0.9940 - val_loss: 0.0434 - val_acc: 0.9911\n",
            "Epoch 1196/3500\n",
            "4352/4352 [==============================] - 0s 14us/step - loss: 0.0233 - acc: 0.9940 - val_loss: 0.0413 - val_acc: 0.9916\n",
            "Epoch 1197/3500\n",
            "4352/4352 [==============================] - 0s 14us/step - loss: 0.0278 - acc: 0.9926 - val_loss: 0.0394 - val_acc: 0.9921\n",
            "Epoch 1198/3500\n",
            "4352/4352 [==============================] - 0s 14us/step - loss: 0.0279 - acc: 0.9924 - val_loss: 0.0396 - val_acc: 0.9930\n",
            "Epoch 1199/3500\n",
            "4352/4352 [==============================] - 0s 14us/step - loss: 0.0268 - acc: 0.9917 - val_loss: 0.0386 - val_acc: 0.9925\n",
            "Epoch 1200/3500\n",
            "4352/4352 [==============================] - 0s 13us/step - loss: 0.0235 - acc: 0.9943 - val_loss: 0.0385 - val_acc: 0.9925\n",
            "Epoch 1201/3500\n",
            "4352/4352 [==============================] - 0s 13us/step - loss: 0.0232 - acc: 0.9943 - val_loss: 0.0384 - val_acc: 0.9916\n",
            "Epoch 1202/3500\n",
            "4352/4352 [==============================] - 0s 14us/step - loss: 0.0235 - acc: 0.9945 - val_loss: 0.0438 - val_acc: 0.9916\n",
            "Epoch 1203/3500\n",
            "4352/4352 [==============================] - 0s 16us/step - loss: 0.0229 - acc: 0.9945 - val_loss: 0.0418 - val_acc: 0.9916\n",
            "Epoch 1204/3500\n",
            "4352/4352 [==============================] - 0s 15us/step - loss: 0.0232 - acc: 0.9936 - val_loss: 0.0384 - val_acc: 0.9925\n",
            "Epoch 1205/3500\n",
            "4352/4352 [==============================] - 0s 14us/step - loss: 0.0258 - acc: 0.9933 - val_loss: 0.0390 - val_acc: 0.9925\n",
            "Epoch 1206/3500\n",
            "4352/4352 [==============================] - 0s 14us/step - loss: 0.0242 - acc: 0.9940 - val_loss: 0.0452 - val_acc: 0.9902\n",
            "Epoch 1207/3500\n",
            "4352/4352 [==============================] - 0s 14us/step - loss: 0.0233 - acc: 0.9947 - val_loss: 0.0407 - val_acc: 0.9921\n",
            "Epoch 1208/3500\n",
            "4352/4352 [==============================] - 0s 14us/step - loss: 0.0235 - acc: 0.9933 - val_loss: 0.0405 - val_acc: 0.9911\n",
            "Epoch 1209/3500\n",
            "4352/4352 [==============================] - 0s 14us/step - loss: 0.0230 - acc: 0.9938 - val_loss: 0.0391 - val_acc: 0.9921\n",
            "Epoch 1210/3500\n",
            "4352/4352 [==============================] - 0s 15us/step - loss: 0.0229 - acc: 0.9947 - val_loss: 0.0431 - val_acc: 0.9911\n",
            "Epoch 1211/3500\n",
            "4352/4352 [==============================] - 0s 15us/step - loss: 0.0231 - acc: 0.9943 - val_loss: 0.0414 - val_acc: 0.9911\n",
            "Epoch 1212/3500\n",
            "4352/4352 [==============================] - 0s 13us/step - loss: 0.0223 - acc: 0.9949 - val_loss: 0.0390 - val_acc: 0.9921\n",
            "Epoch 1213/3500\n",
            "4352/4352 [==============================] - 0s 15us/step - loss: 0.0226 - acc: 0.9947 - val_loss: 0.0384 - val_acc: 0.9930\n",
            "Epoch 1214/3500\n",
            "4352/4352 [==============================] - 0s 15us/step - loss: 0.0271 - acc: 0.9913 - val_loss: 0.0382 - val_acc: 0.9930\n",
            "Epoch 1215/3500\n",
            "4352/4352 [==============================] - 0s 13us/step - loss: 0.0273 - acc: 0.9922 - val_loss: 0.0418 - val_acc: 0.9921\n",
            "Epoch 1216/3500\n",
            "4352/4352 [==============================] - 0s 14us/step - loss: 0.0241 - acc: 0.9945 - val_loss: 0.0469 - val_acc: 0.9897\n",
            "Epoch 1217/3500\n",
            "4352/4352 [==============================] - 0s 16us/step - loss: 0.0236 - acc: 0.9943 - val_loss: 0.0401 - val_acc: 0.9921\n",
            "Epoch 1218/3500\n",
            "4352/4352 [==============================] - 0s 16us/step - loss: 0.0237 - acc: 0.9943 - val_loss: 0.0395 - val_acc: 0.9925\n",
            "Epoch 1219/3500\n",
            "4352/4352 [==============================] - 0s 16us/step - loss: 0.0228 - acc: 0.9940 - val_loss: 0.0406 - val_acc: 0.9921\n",
            "Epoch 1220/3500\n",
            "4352/4352 [==============================] - 0s 14us/step - loss: 0.0234 - acc: 0.9936 - val_loss: 0.0463 - val_acc: 0.9897\n",
            "Epoch 1221/3500\n",
            "4352/4352 [==============================] - 0s 15us/step - loss: 0.0226 - acc: 0.9947 - val_loss: 0.0392 - val_acc: 0.9916\n",
            "Epoch 1222/3500\n",
            "4352/4352 [==============================] - 0s 13us/step - loss: 0.0228 - acc: 0.9943 - val_loss: 0.0391 - val_acc: 0.9921\n",
            "Epoch 1223/3500\n",
            "4352/4352 [==============================] - 0s 15us/step - loss: 0.0257 - acc: 0.9936 - val_loss: 0.0382 - val_acc: 0.9930\n",
            "Epoch 1224/3500\n",
            "4352/4352 [==============================] - 0s 16us/step - loss: 0.0256 - acc: 0.9931 - val_loss: 0.0403 - val_acc: 0.9911\n",
            "Epoch 1225/3500\n",
            "4352/4352 [==============================] - 0s 14us/step - loss: 0.0230 - acc: 0.9945 - val_loss: 0.0386 - val_acc: 0.9925\n",
            "Epoch 1226/3500\n",
            "4352/4352 [==============================] - 0s 13us/step - loss: 0.0220 - acc: 0.9940 - val_loss: 0.0536 - val_acc: 0.9897\n",
            "Epoch 1227/3500\n",
            "4352/4352 [==============================] - 0s 14us/step - loss: 0.0262 - acc: 0.9933 - val_loss: 0.0497 - val_acc: 0.9897\n",
            "Epoch 1228/3500\n",
            "4352/4352 [==============================] - 0s 15us/step - loss: 0.0258 - acc: 0.9926 - val_loss: 0.0431 - val_acc: 0.9921\n",
            "Epoch 1229/3500\n",
            "4352/4352 [==============================] - 0s 13us/step - loss: 0.0242 - acc: 0.9938 - val_loss: 0.0378 - val_acc: 0.9925\n",
            "Epoch 1230/3500\n",
            "4352/4352 [==============================] - 0s 13us/step - loss: 0.0233 - acc: 0.9945 - val_loss: 0.0389 - val_acc: 0.9921\n",
            "Epoch 1231/3500\n",
            "4352/4352 [==============================] - 0s 13us/step - loss: 0.0273 - acc: 0.9926 - val_loss: 0.0418 - val_acc: 0.9921\n",
            "Epoch 1232/3500\n",
            "4352/4352 [==============================] - 0s 14us/step - loss: 0.0258 - acc: 0.9936 - val_loss: 0.0473 - val_acc: 0.9911\n",
            "Epoch 1233/3500\n",
            "4352/4352 [==============================] - 0s 14us/step - loss: 0.0235 - acc: 0.9936 - val_loss: 0.0588 - val_acc: 0.9883\n",
            "Epoch 1234/3500\n",
            "4352/4352 [==============================] - 0s 14us/step - loss: 0.0273 - acc: 0.9929 - val_loss: 0.0487 - val_acc: 0.9902\n",
            "Epoch 1235/3500\n",
            "4352/4352 [==============================] - 0s 15us/step - loss: 0.0263 - acc: 0.9926 - val_loss: 0.0462 - val_acc: 0.9897\n",
            "Epoch 1236/3500\n",
            "4352/4352 [==============================] - 0s 14us/step - loss: 0.0240 - acc: 0.9945 - val_loss: 0.0397 - val_acc: 0.9916\n",
            "Epoch 1237/3500\n",
            "4352/4352 [==============================] - 0s 15us/step - loss: 0.0229 - acc: 0.9947 - val_loss: 0.0393 - val_acc: 0.9921\n",
            "Epoch 1238/3500\n",
            "4352/4352 [==============================] - 0s 15us/step - loss: 0.0251 - acc: 0.9936 - val_loss: 0.0400 - val_acc: 0.9925\n",
            "Epoch 1239/3500\n",
            "4352/4352 [==============================] - 0s 14us/step - loss: 0.0256 - acc: 0.9933 - val_loss: 0.0431 - val_acc: 0.9911\n",
            "Epoch 1240/3500\n",
            "4352/4352 [==============================] - 0s 15us/step - loss: 0.0221 - acc: 0.9947 - val_loss: 0.0412 - val_acc: 0.9921\n",
            "Epoch 1241/3500\n",
            "4352/4352 [==============================] - 0s 14us/step - loss: 0.0222 - acc: 0.9949 - val_loss: 0.0420 - val_acc: 0.9907\n",
            "Epoch 1242/3500\n",
            "4352/4352 [==============================] - 0s 14us/step - loss: 0.0224 - acc: 0.9945 - val_loss: 0.0390 - val_acc: 0.9916\n",
            "Epoch 1243/3500\n",
            "4352/4352 [==============================] - 0s 14us/step - loss: 0.0228 - acc: 0.9933 - val_loss: 0.0456 - val_acc: 0.9902\n",
            "Epoch 1244/3500\n",
            "4352/4352 [==============================] - 0s 13us/step - loss: 0.0225 - acc: 0.9947 - val_loss: 0.0410 - val_acc: 0.9911\n",
            "Epoch 1245/3500\n",
            "4352/4352 [==============================] - 0s 13us/step - loss: 0.0229 - acc: 0.9947 - val_loss: 0.0386 - val_acc: 0.9921\n",
            "Epoch 1246/3500\n",
            "4352/4352 [==============================] - 0s 14us/step - loss: 0.0239 - acc: 0.9936 - val_loss: 0.0395 - val_acc: 0.9925\n",
            "Epoch 1247/3500\n",
            "4352/4352 [==============================] - 0s 15us/step - loss: 0.0225 - acc: 0.9945 - val_loss: 0.0380 - val_acc: 0.9921\n",
            "Epoch 1248/3500\n",
            "4352/4352 [==============================] - 0s 14us/step - loss: 0.0229 - acc: 0.9936 - val_loss: 0.0434 - val_acc: 0.9907\n",
            "Epoch 1249/3500\n",
            "4352/4352 [==============================] - 0s 14us/step - loss: 0.0223 - acc: 0.9949 - val_loss: 0.0409 - val_acc: 0.9911\n",
            "Epoch 1250/3500\n",
            "4352/4352 [==============================] - 0s 15us/step - loss: 0.0224 - acc: 0.9945 - val_loss: 0.0426 - val_acc: 0.9916\n",
            "Epoch 1251/3500\n",
            "4352/4352 [==============================] - 0s 15us/step - loss: 0.0236 - acc: 0.9938 - val_loss: 0.0409 - val_acc: 0.9925\n",
            "Epoch 1252/3500\n",
            "4352/4352 [==============================] - 0s 15us/step - loss: 0.0272 - acc: 0.9924 - val_loss: 0.0393 - val_acc: 0.9921\n",
            "Epoch 1253/3500\n",
            "4352/4352 [==============================] - 0s 14us/step - loss: 0.0242 - acc: 0.9933 - val_loss: 0.0429 - val_acc: 0.9921\n",
            "Epoch 1254/3500\n",
            "4352/4352 [==============================] - 0s 17us/step - loss: 0.0231 - acc: 0.9943 - val_loss: 0.0456 - val_acc: 0.9907\n",
            "Epoch 1255/3500\n",
            "4352/4352 [==============================] - 0s 15us/step - loss: 0.0224 - acc: 0.9947 - val_loss: 0.0419 - val_acc: 0.9916\n",
            "Epoch 1256/3500\n",
            "4352/4352 [==============================] - 0s 14us/step - loss: 0.0227 - acc: 0.9952 - val_loss: 0.0403 - val_acc: 0.9925\n",
            "Epoch 1257/3500\n",
            "4352/4352 [==============================] - 0s 14us/step - loss: 0.0233 - acc: 0.9943 - val_loss: 0.0416 - val_acc: 0.9911\n",
            "Epoch 1258/3500\n",
            "4352/4352 [==============================] - 0s 14us/step - loss: 0.0242 - acc: 0.9943 - val_loss: 0.0464 - val_acc: 0.9907\n",
            "Epoch 1259/3500\n",
            "4352/4352 [==============================] - 0s 14us/step - loss: 0.0225 - acc: 0.9940 - val_loss: 0.0390 - val_acc: 0.9925\n",
            "Epoch 1260/3500\n",
            "4352/4352 [==============================] - 0s 14us/step - loss: 0.0229 - acc: 0.9949 - val_loss: 0.0415 - val_acc: 0.9911\n",
            "Epoch 1261/3500\n",
            "4352/4352 [==============================] - 0s 13us/step - loss: 0.0295 - acc: 0.9910 - val_loss: 0.0404 - val_acc: 0.9921\n",
            "Epoch 1262/3500\n",
            "4352/4352 [==============================] - 0s 14us/step - loss: 0.0278 - acc: 0.9915 - val_loss: 0.0411 - val_acc: 0.9925\n",
            "Epoch 1263/3500\n",
            "4352/4352 [==============================] - 0s 15us/step - loss: 0.0281 - acc: 0.9917 - val_loss: 0.0446 - val_acc: 0.9907\n",
            "Epoch 1264/3500\n",
            "4352/4352 [==============================] - 0s 13us/step - loss: 0.0307 - acc: 0.9913 - val_loss: 0.0516 - val_acc: 0.9888\n",
            "Epoch 1265/3500\n",
            "4352/4352 [==============================] - 0s 14us/step - loss: 0.0267 - acc: 0.9933 - val_loss: 0.0545 - val_acc: 0.9888\n",
            "Epoch 1266/3500\n",
            "4352/4352 [==============================] - 0s 14us/step - loss: 0.0251 - acc: 0.9936 - val_loss: 0.0455 - val_acc: 0.9907\n",
            "Epoch 1267/3500\n",
            "4352/4352 [==============================] - 0s 16us/step - loss: 0.0232 - acc: 0.9943 - val_loss: 0.0426 - val_acc: 0.9916\n",
            "Epoch 1268/3500\n",
            "4352/4352 [==============================] - 0s 14us/step - loss: 0.0228 - acc: 0.9938 - val_loss: 0.0409 - val_acc: 0.9907\n",
            "Epoch 1269/3500\n",
            "4352/4352 [==============================] - 0s 15us/step - loss: 0.0234 - acc: 0.9947 - val_loss: 0.0425 - val_acc: 0.9921\n",
            "Epoch 1270/3500\n",
            "4352/4352 [==============================] - 0s 14us/step - loss: 0.0233 - acc: 0.9945 - val_loss: 0.0430 - val_acc: 0.9907\n",
            "Epoch 1271/3500\n",
            "4352/4352 [==============================] - 0s 14us/step - loss: 0.0231 - acc: 0.9945 - val_loss: 0.0416 - val_acc: 0.9921\n",
            "Epoch 1272/3500\n",
            "4352/4352 [==============================] - 0s 14us/step - loss: 0.0224 - acc: 0.9949 - val_loss: 0.0406 - val_acc: 0.9921\n",
            "Epoch 1273/3500\n",
            "4352/4352 [==============================] - 0s 15us/step - loss: 0.0222 - acc: 0.9947 - val_loss: 0.0415 - val_acc: 0.9925\n",
            "Epoch 1274/3500\n",
            "4352/4352 [==============================] - 0s 14us/step - loss: 0.0236 - acc: 0.9940 - val_loss: 0.0459 - val_acc: 0.9897\n",
            "Epoch 1275/3500\n",
            "4352/4352 [==============================] - 0s 13us/step - loss: 0.0311 - acc: 0.9924 - val_loss: 0.0419 - val_acc: 0.9921\n",
            "Epoch 1276/3500\n",
            "4352/4352 [==============================] - 0s 14us/step - loss: 0.0247 - acc: 0.9933 - val_loss: 0.0477 - val_acc: 0.9893\n",
            "Epoch 1277/3500\n",
            "4352/4352 [==============================] - 0s 15us/step - loss: 0.0256 - acc: 0.9940 - val_loss: 0.0485 - val_acc: 0.9902\n",
            "Epoch 1278/3500\n",
            "4352/4352 [==============================] - 0s 13us/step - loss: 0.0265 - acc: 0.9924 - val_loss: 0.0394 - val_acc: 0.9916\n",
            "Epoch 1279/3500\n",
            "4352/4352 [==============================] - 0s 14us/step - loss: 0.0241 - acc: 0.9940 - val_loss: 0.0421 - val_acc: 0.9925\n",
            "Epoch 1280/3500\n",
            "4352/4352 [==============================] - 0s 15us/step - loss: 0.0263 - acc: 0.9924 - val_loss: 0.0398 - val_acc: 0.9921\n",
            "Epoch 1281/3500\n",
            "4352/4352 [==============================] - 0s 14us/step - loss: 0.0237 - acc: 0.9936 - val_loss: 0.0443 - val_acc: 0.9921\n",
            "Epoch 1282/3500\n",
            "4352/4352 [==============================] - 0s 14us/step - loss: 0.0241 - acc: 0.9933 - val_loss: 0.0399 - val_acc: 0.9925\n",
            "Epoch 1283/3500\n",
            "4352/4352 [==============================] - 0s 15us/step - loss: 0.0259 - acc: 0.9924 - val_loss: 0.0415 - val_acc: 0.9921\n",
            "Epoch 1284/3500\n",
            "4352/4352 [==============================] - 0s 14us/step - loss: 0.0229 - acc: 0.9940 - val_loss: 0.0517 - val_acc: 0.9893\n",
            "Epoch 1285/3500\n",
            "4352/4352 [==============================] - 0s 15us/step - loss: 0.0242 - acc: 0.9933 - val_loss: 0.0423 - val_acc: 0.9907\n",
            "Epoch 1286/3500\n",
            "4352/4352 [==============================] - 0s 13us/step - loss: 0.0232 - acc: 0.9938 - val_loss: 0.0400 - val_acc: 0.9925\n",
            "Epoch 1287/3500\n",
            "4352/4352 [==============================] - 0s 15us/step - loss: 0.0227 - acc: 0.9940 - val_loss: 0.0417 - val_acc: 0.9925\n",
            "Epoch 1288/3500\n",
            "4352/4352 [==============================] - 0s 14us/step - loss: 0.0220 - acc: 0.9949 - val_loss: 0.0441 - val_acc: 0.9911\n",
            "Epoch 1289/3500\n",
            "4352/4352 [==============================] - 0s 15us/step - loss: 0.0220 - acc: 0.9949 - val_loss: 0.0431 - val_acc: 0.9916\n",
            "Epoch 1290/3500\n",
            "4352/4352 [==============================] - 0s 14us/step - loss: 0.0219 - acc: 0.9947 - val_loss: 0.0412 - val_acc: 0.9916\n",
            "Epoch 1291/3500\n",
            "4352/4352 [==============================] - 0s 15us/step - loss: 0.0223 - acc: 0.9947 - val_loss: 0.0414 - val_acc: 0.9925\n",
            "Epoch 1292/3500\n",
            "4352/4352 [==============================] - 0s 16us/step - loss: 0.0224 - acc: 0.9952 - val_loss: 0.0408 - val_acc: 0.9921\n",
            "Epoch 1293/3500\n",
            "4352/4352 [==============================] - 0s 14us/step - loss: 0.0244 - acc: 0.9945 - val_loss: 0.0396 - val_acc: 0.9925\n",
            "Epoch 1294/3500\n",
            "4352/4352 [==============================] - 0s 13us/step - loss: 0.0232 - acc: 0.9940 - val_loss: 0.0394 - val_acc: 0.9921\n",
            "Epoch 1295/3500\n",
            "4352/4352 [==============================] - 0s 14us/step - loss: 0.0236 - acc: 0.9933 - val_loss: 0.0400 - val_acc: 0.9925\n",
            "Epoch 1296/3500\n",
            "4352/4352 [==============================] - 0s 15us/step - loss: 0.0236 - acc: 0.9943 - val_loss: 0.0475 - val_acc: 0.9902\n",
            "Epoch 1297/3500\n",
            "4352/4352 [==============================] - 0s 13us/step - loss: 0.0222 - acc: 0.9943 - val_loss: 0.0477 - val_acc: 0.9907\n",
            "Epoch 1298/3500\n",
            "4352/4352 [==============================] - 0s 13us/step - loss: 0.0232 - acc: 0.9938 - val_loss: 0.0396 - val_acc: 0.9916\n",
            "Epoch 1299/3500\n",
            "4352/4352 [==============================] - 0s 16us/step - loss: 0.0222 - acc: 0.9952 - val_loss: 0.0391 - val_acc: 0.9921\n",
            "Epoch 1300/3500\n",
            "4352/4352 [==============================] - 0s 21us/step - loss: 0.0241 - acc: 0.9936 - val_loss: 0.0392 - val_acc: 0.9921\n",
            "Epoch 1301/3500\n",
            "4352/4352 [==============================] - 0s 15us/step - loss: 0.0244 - acc: 0.9933 - val_loss: 0.0431 - val_acc: 0.9916\n",
            "Epoch 1302/3500\n",
            "4352/4352 [==============================] - 0s 14us/step - loss: 0.0225 - acc: 0.9945 - val_loss: 0.0461 - val_acc: 0.9907\n",
            "Epoch 1303/3500\n",
            "4352/4352 [==============================] - 0s 14us/step - loss: 0.0227 - acc: 0.9952 - val_loss: 0.0426 - val_acc: 0.9911\n",
            "Epoch 1304/3500\n",
            "4352/4352 [==============================] - 0s 13us/step - loss: 0.0229 - acc: 0.9943 - val_loss: 0.0415 - val_acc: 0.9925\n",
            "Epoch 1305/3500\n",
            "4352/4352 [==============================] - 0s 14us/step - loss: 0.0226 - acc: 0.9945 - val_loss: 0.0406 - val_acc: 0.9921\n",
            "Epoch 1306/3500\n",
            "4352/4352 [==============================] - 0s 15us/step - loss: 0.0218 - acc: 0.9949 - val_loss: 0.0416 - val_acc: 0.9921\n",
            "Epoch 1307/3500\n",
            "4352/4352 [==============================] - 0s 15us/step - loss: 0.0225 - acc: 0.9949 - val_loss: 0.0457 - val_acc: 0.9911\n",
            "Epoch 1308/3500\n",
            "4352/4352 [==============================] - 0s 15us/step - loss: 0.0226 - acc: 0.9940 - val_loss: 0.0412 - val_acc: 0.9925\n",
            "Epoch 1309/3500\n",
            "4352/4352 [==============================] - 0s 14us/step - loss: 0.0220 - acc: 0.9954 - val_loss: 0.0424 - val_acc: 0.9911\n",
            "Epoch 1310/3500\n",
            "4352/4352 [==============================] - 0s 14us/step - loss: 0.0226 - acc: 0.9943 - val_loss: 0.0460 - val_acc: 0.9907\n",
            "Epoch 1311/3500\n",
            "4352/4352 [==============================] - 0s 13us/step - loss: 0.0234 - acc: 0.9940 - val_loss: 0.0402 - val_acc: 0.9911\n",
            "Epoch 1312/3500\n",
            "4352/4352 [==============================] - 0s 15us/step - loss: 0.0224 - acc: 0.9952 - val_loss: 0.0398 - val_acc: 0.9921\n",
            "Epoch 1313/3500\n",
            "4352/4352 [==============================] - 0s 14us/step - loss: 0.0239 - acc: 0.9940 - val_loss: 0.0410 - val_acc: 0.9921\n",
            "Epoch 1314/3500\n",
            "4352/4352 [==============================] - 0s 16us/step - loss: 0.0229 - acc: 0.9943 - val_loss: 0.0487 - val_acc: 0.9902\n",
            "Epoch 1315/3500\n",
            "4352/4352 [==============================] - 0s 14us/step - loss: 0.0227 - acc: 0.9943 - val_loss: 0.0533 - val_acc: 0.9888\n",
            "Epoch 1316/3500\n",
            "4352/4352 [==============================] - 0s 13us/step - loss: 0.0238 - acc: 0.9943 - val_loss: 0.0427 - val_acc: 0.9921\n",
            "Epoch 1317/3500\n",
            "4352/4352 [==============================] - 0s 14us/step - loss: 0.0246 - acc: 0.9945 - val_loss: 0.0394 - val_acc: 0.9911\n",
            "Epoch 1318/3500\n",
            "4352/4352 [==============================] - 0s 15us/step - loss: 0.0228 - acc: 0.9945 - val_loss: 0.0403 - val_acc: 0.9921\n",
            "Epoch 1319/3500\n",
            "4352/4352 [==============================] - 0s 14us/step - loss: 0.0229 - acc: 0.9933 - val_loss: 0.0412 - val_acc: 0.9925\n",
            "Epoch 1320/3500\n",
            "4352/4352 [==============================] - 0s 14us/step - loss: 0.0243 - acc: 0.9933 - val_loss: 0.0421 - val_acc: 0.9916\n",
            "Epoch 1321/3500\n",
            "4352/4352 [==============================] - 0s 14us/step - loss: 0.0261 - acc: 0.9933 - val_loss: 0.0429 - val_acc: 0.9921\n",
            "Epoch 1322/3500\n",
            "4352/4352 [==============================] - 0s 14us/step - loss: 0.0272 - acc: 0.9926 - val_loss: 0.0427 - val_acc: 0.9925\n",
            "Epoch 1323/3500\n",
            "4352/4352 [==============================] - 0s 14us/step - loss: 0.0237 - acc: 0.9949 - val_loss: 0.0424 - val_acc: 0.9916\n",
            "Epoch 1324/3500\n",
            "4352/4352 [==============================] - 0s 14us/step - loss: 0.0260 - acc: 0.9922 - val_loss: 0.0401 - val_acc: 0.9916\n",
            "Epoch 1325/3500\n",
            "4352/4352 [==============================] - 0s 14us/step - loss: 0.0266 - acc: 0.9926 - val_loss: 0.0396 - val_acc: 0.9930\n",
            "Epoch 1326/3500\n",
            "4352/4352 [==============================] - 0s 14us/step - loss: 0.0277 - acc: 0.9933 - val_loss: 0.0602 - val_acc: 0.9893\n",
            "Epoch 1327/3500\n",
            "4352/4352 [==============================] - 0s 15us/step - loss: 0.0266 - acc: 0.9931 - val_loss: 0.0543 - val_acc: 0.9883\n",
            "Epoch 1328/3500\n",
            "4352/4352 [==============================] - 0s 14us/step - loss: 0.0238 - acc: 0.9943 - val_loss: 0.0427 - val_acc: 0.9911\n",
            "Epoch 1329/3500\n",
            "4352/4352 [==============================] - 0s 14us/step - loss: 0.0248 - acc: 0.9938 - val_loss: 0.0433 - val_acc: 0.9916\n",
            "Epoch 1330/3500\n",
            "4352/4352 [==============================] - 0s 18us/step - loss: 0.0218 - acc: 0.9945 - val_loss: 0.0394 - val_acc: 0.9921\n",
            "Epoch 1331/3500\n",
            "4352/4352 [==============================] - 0s 14us/step - loss: 0.0248 - acc: 0.9940 - val_loss: 0.0403 - val_acc: 0.9921\n",
            "Epoch 1332/3500\n",
            "4352/4352 [==============================] - 0s 14us/step - loss: 0.0237 - acc: 0.9936 - val_loss: 0.0399 - val_acc: 0.9916\n",
            "Epoch 1333/3500\n",
            "4352/4352 [==============================] - 0s 15us/step - loss: 0.0228 - acc: 0.9945 - val_loss: 0.0438 - val_acc: 0.9907\n",
            "Epoch 1334/3500\n",
            "4352/4352 [==============================] - 0s 14us/step - loss: 0.0223 - acc: 0.9945 - val_loss: 0.0397 - val_acc: 0.9916\n",
            "Epoch 1335/3500\n",
            "4352/4352 [==============================] - 0s 14us/step - loss: 0.0227 - acc: 0.9949 - val_loss: 0.0406 - val_acc: 0.9925\n",
            "Epoch 1336/3500\n",
            "4352/4352 [==============================] - 0s 15us/step - loss: 0.0223 - acc: 0.9947 - val_loss: 0.0420 - val_acc: 0.9925\n",
            "Epoch 1337/3500\n",
            "4352/4352 [==============================] - 0s 14us/step - loss: 0.0227 - acc: 0.9945 - val_loss: 0.0402 - val_acc: 0.9930\n",
            "Epoch 1338/3500\n",
            "4352/4352 [==============================] - 0s 14us/step - loss: 0.0223 - acc: 0.9949 - val_loss: 0.0530 - val_acc: 0.9897\n",
            "Epoch 1339/3500\n",
            "4352/4352 [==============================] - 0s 14us/step - loss: 0.0218 - acc: 0.9947 - val_loss: 0.0395 - val_acc: 0.9916\n",
            "Epoch 1340/3500\n",
            "4352/4352 [==============================] - 0s 14us/step - loss: 0.0225 - acc: 0.9947 - val_loss: 0.0393 - val_acc: 0.9925\n",
            "Epoch 1341/3500\n",
            "4352/4352 [==============================] - 0s 15us/step - loss: 0.0225 - acc: 0.9945 - val_loss: 0.0396 - val_acc: 0.9916\n",
            "Epoch 1342/3500\n",
            "4352/4352 [==============================] - 0s 15us/step - loss: 0.0219 - acc: 0.9949 - val_loss: 0.0429 - val_acc: 0.9921\n",
            "Epoch 1343/3500\n",
            "4352/4352 [==============================] - 0s 14us/step - loss: 0.0221 - acc: 0.9947 - val_loss: 0.0411 - val_acc: 0.9921\n",
            "Epoch 1344/3500\n",
            "4352/4352 [==============================] - 0s 16us/step - loss: 0.0221 - acc: 0.9940 - val_loss: 0.0403 - val_acc: 0.9925\n",
            "Epoch 1345/3500\n",
            "4352/4352 [==============================] - 0s 15us/step - loss: 0.0227 - acc: 0.9945 - val_loss: 0.0397 - val_acc: 0.9921\n",
            "Epoch 1346/3500\n",
            "4352/4352 [==============================] - 0s 17us/step - loss: 0.0246 - acc: 0.9936 - val_loss: 0.0398 - val_acc: 0.9925\n",
            "Epoch 1347/3500\n",
            "4352/4352 [==============================] - 0s 14us/step - loss: 0.0235 - acc: 0.9940 - val_loss: 0.0400 - val_acc: 0.9925\n",
            "Epoch 1348/3500\n",
            "4352/4352 [==============================] - 0s 14us/step - loss: 0.0231 - acc: 0.9943 - val_loss: 0.0402 - val_acc: 0.9925\n",
            "Epoch 1349/3500\n",
            "4352/4352 [==============================] - 0s 13us/step - loss: 0.0218 - acc: 0.9947 - val_loss: 0.0476 - val_acc: 0.9907\n",
            "Epoch 1350/3500\n",
            "4352/4352 [==============================] - 0s 14us/step - loss: 0.0231 - acc: 0.9936 - val_loss: 0.0459 - val_acc: 0.9907\n",
            "Epoch 1351/3500\n",
            "4352/4352 [==============================] - 0s 13us/step - loss: 0.0220 - acc: 0.9954 - val_loss: 0.0419 - val_acc: 0.9925\n",
            "Epoch 1352/3500\n",
            "4352/4352 [==============================] - 0s 13us/step - loss: 0.0219 - acc: 0.9945 - val_loss: 0.0412 - val_acc: 0.9921\n",
            "Epoch 1353/3500\n",
            "4352/4352 [==============================] - 0s 13us/step - loss: 0.0215 - acc: 0.9952 - val_loss: 0.0457 - val_acc: 0.9907\n",
            "Epoch 1354/3500\n",
            "4352/4352 [==============================] - 0s 14us/step - loss: 0.0219 - acc: 0.9952 - val_loss: 0.0409 - val_acc: 0.9921\n",
            "Epoch 1355/3500\n",
            "4352/4352 [==============================] - 0s 14us/step - loss: 0.0218 - acc: 0.9949 - val_loss: 0.0418 - val_acc: 0.9925\n",
            "Epoch 1356/3500\n",
            "4352/4352 [==============================] - 0s 14us/step - loss: 0.0220 - acc: 0.9949 - val_loss: 0.0407 - val_acc: 0.9925\n",
            "Epoch 1357/3500\n",
            "4352/4352 [==============================] - 0s 15us/step - loss: 0.0217 - acc: 0.9947 - val_loss: 0.0428 - val_acc: 0.9921\n",
            "Epoch 1358/3500\n",
            "4352/4352 [==============================] - 0s 14us/step - loss: 0.0216 - acc: 0.9952 - val_loss: 0.0432 - val_acc: 0.9911\n",
            "Epoch 1359/3500\n",
            "4352/4352 [==============================] - 0s 13us/step - loss: 0.0223 - acc: 0.9943 - val_loss: 0.0458 - val_acc: 0.9911\n",
            "Epoch 1360/3500\n",
            "4352/4352 [==============================] - 0s 12us/step - loss: 0.0237 - acc: 0.9936 - val_loss: 0.0610 - val_acc: 0.9883\n",
            "Epoch 1361/3500\n",
            "4352/4352 [==============================] - 0s 13us/step - loss: 0.0260 - acc: 0.9931 - val_loss: 0.0576 - val_acc: 0.9893\n",
            "Epoch 1362/3500\n",
            "4352/4352 [==============================] - 0s 14us/step - loss: 0.0249 - acc: 0.9933 - val_loss: 0.0418 - val_acc: 0.9925\n",
            "Epoch 1363/3500\n",
            "4352/4352 [==============================] - 0s 14us/step - loss: 0.0223 - acc: 0.9943 - val_loss: 0.0441 - val_acc: 0.9921\n",
            "Epoch 1364/3500\n",
            "4352/4352 [==============================] - 0s 13us/step - loss: 0.0223 - acc: 0.9938 - val_loss: 0.0414 - val_acc: 0.9907\n",
            "Epoch 1365/3500\n",
            "4352/4352 [==============================] - 0s 14us/step - loss: 0.0227 - acc: 0.9933 - val_loss: 0.0446 - val_acc: 0.9916\n",
            "Epoch 1366/3500\n",
            "4352/4352 [==============================] - 0s 13us/step - loss: 0.0234 - acc: 0.9936 - val_loss: 0.0423 - val_acc: 0.9925\n",
            "Epoch 1367/3500\n",
            "4352/4352 [==============================] - 0s 13us/step - loss: 0.0229 - acc: 0.9947 - val_loss: 0.0422 - val_acc: 0.9916\n",
            "Epoch 1368/3500\n",
            "4352/4352 [==============================] - 0s 14us/step - loss: 0.0218 - acc: 0.9949 - val_loss: 0.0395 - val_acc: 0.9925\n",
            "Epoch 1369/3500\n",
            "4352/4352 [==============================] - 0s 14us/step - loss: 0.0224 - acc: 0.9945 - val_loss: 0.0403 - val_acc: 0.9921\n",
            "Epoch 1370/3500\n",
            "4352/4352 [==============================] - 0s 14us/step - loss: 0.0231 - acc: 0.9943 - val_loss: 0.0412 - val_acc: 0.9916\n",
            "Epoch 1371/3500\n",
            "4352/4352 [==============================] - 0s 14us/step - loss: 0.0306 - acc: 0.9913 - val_loss: 0.0391 - val_acc: 0.9916\n",
            "Epoch 1372/3500\n",
            "4352/4352 [==============================] - 0s 14us/step - loss: 0.0267 - acc: 0.9924 - val_loss: 0.0448 - val_acc: 0.9916\n",
            "Epoch 1373/3500\n",
            "4352/4352 [==============================] - 0s 16us/step - loss: 0.0222 - acc: 0.9945 - val_loss: 0.0475 - val_acc: 0.9907\n",
            "Epoch 1374/3500\n",
            "4352/4352 [==============================] - 0s 14us/step - loss: 0.0224 - acc: 0.9943 - val_loss: 0.0411 - val_acc: 0.9916\n",
            "Epoch 1375/3500\n",
            "4352/4352 [==============================] - 0s 14us/step - loss: 0.0231 - acc: 0.9943 - val_loss: 0.0433 - val_acc: 0.9921\n",
            "Epoch 1376/3500\n",
            "4352/4352 [==============================] - 0s 14us/step - loss: 0.0236 - acc: 0.9936 - val_loss: 0.0577 - val_acc: 0.9888\n",
            "Epoch 1377/3500\n",
            "4352/4352 [==============================] - 0s 14us/step - loss: 0.0313 - acc: 0.9897 - val_loss: 0.0744 - val_acc: 0.9855\n",
            "Epoch 1378/3500\n",
            "4352/4352 [==============================] - 0s 14us/step - loss: 0.0259 - acc: 0.9933 - val_loss: 0.0456 - val_acc: 0.9911\n",
            "Epoch 1379/3500\n",
            "4352/4352 [==============================] - 0s 17us/step - loss: 0.0232 - acc: 0.9949 - val_loss: 0.0418 - val_acc: 0.9916\n",
            "Epoch 1380/3500\n",
            "4352/4352 [==============================] - 0s 14us/step - loss: 0.0222 - acc: 0.9947 - val_loss: 0.0416 - val_acc: 0.9907\n",
            "Epoch 1381/3500\n",
            "4352/4352 [==============================] - 0s 15us/step - loss: 0.0225 - acc: 0.9947 - val_loss: 0.0460 - val_acc: 0.9916\n",
            "Epoch 1382/3500\n",
            "4352/4352 [==============================] - 0s 14us/step - loss: 0.0215 - acc: 0.9947 - val_loss: 0.0410 - val_acc: 0.9921\n",
            "Epoch 1383/3500\n",
            "4352/4352 [==============================] - 0s 15us/step - loss: 0.0237 - acc: 0.9940 - val_loss: 0.0395 - val_acc: 0.9916\n",
            "Epoch 1384/3500\n",
            "4352/4352 [==============================] - 0s 14us/step - loss: 0.0294 - acc: 0.9917 - val_loss: 0.0537 - val_acc: 0.9893\n",
            "Epoch 1385/3500\n",
            "4352/4352 [==============================] - 0s 15us/step - loss: 0.0306 - acc: 0.9913 - val_loss: 0.0542 - val_acc: 0.9897\n",
            "Epoch 1386/3500\n",
            "4352/4352 [==============================] - 0s 15us/step - loss: 0.0263 - acc: 0.9924 - val_loss: 0.0561 - val_acc: 0.9888\n",
            "Epoch 1387/3500\n",
            "4352/4352 [==============================] - 0s 16us/step - loss: 0.0245 - acc: 0.9936 - val_loss: 0.0419 - val_acc: 0.9921\n",
            "Epoch 1388/3500\n",
            "4352/4352 [==============================] - 0s 16us/step - loss: 0.0228 - acc: 0.9945 - val_loss: 0.0428 - val_acc: 0.9921\n",
            "Epoch 1389/3500\n",
            "4352/4352 [==============================] - 0s 14us/step - loss: 0.0223 - acc: 0.9945 - val_loss: 0.0405 - val_acc: 0.9925\n",
            "Epoch 1390/3500\n",
            "4352/4352 [==============================] - 0s 14us/step - loss: 0.0237 - acc: 0.9938 - val_loss: 0.0437 - val_acc: 0.9921\n",
            "Epoch 1391/3500\n",
            "4352/4352 [==============================] - 0s 14us/step - loss: 0.0251 - acc: 0.9929 - val_loss: 0.0569 - val_acc: 0.9893\n",
            "Epoch 1392/3500\n",
            "4352/4352 [==============================] - 0s 13us/step - loss: 0.0254 - acc: 0.9940 - val_loss: 0.0539 - val_acc: 0.9893\n",
            "Epoch 1393/3500\n",
            "4352/4352 [==============================] - 0s 13us/step - loss: 0.0257 - acc: 0.9929 - val_loss: 0.0420 - val_acc: 0.9925\n",
            "Epoch 1394/3500\n",
            "4352/4352 [==============================] - 0s 15us/step - loss: 0.0215 - acc: 0.9940 - val_loss: 0.0400 - val_acc: 0.9916\n",
            "Epoch 1395/3500\n",
            "4352/4352 [==============================] - 0s 14us/step - loss: 0.0219 - acc: 0.9945 - val_loss: 0.0433 - val_acc: 0.9921\n",
            "Epoch 1396/3500\n",
            "4352/4352 [==============================] - 0s 14us/step - loss: 0.0228 - acc: 0.9933 - val_loss: 0.0660 - val_acc: 0.9879\n",
            "Epoch 1397/3500\n",
            "4352/4352 [==============================] - 0s 16us/step - loss: 0.0251 - acc: 0.9933 - val_loss: 0.0416 - val_acc: 0.9925\n",
            "Epoch 1398/3500\n",
            "4352/4352 [==============================] - 0s 14us/step - loss: 0.0231 - acc: 0.9933 - val_loss: 0.0401 - val_acc: 0.9921\n",
            "Epoch 1399/3500\n",
            "4352/4352 [==============================] - 0s 15us/step - loss: 0.0234 - acc: 0.9933 - val_loss: 0.0395 - val_acc: 0.9930\n",
            "Epoch 1400/3500\n",
            "4352/4352 [==============================] - 0s 14us/step - loss: 0.0245 - acc: 0.9940 - val_loss: 0.0498 - val_acc: 0.9902\n",
            "Epoch 1401/3500\n",
            "4352/4352 [==============================] - 0s 15us/step - loss: 0.0239 - acc: 0.9940 - val_loss: 0.0518 - val_acc: 0.9897\n",
            "Epoch 1402/3500\n",
            "4352/4352 [==============================] - 0s 15us/step - loss: 0.0263 - acc: 0.9936 - val_loss: 0.0403 - val_acc: 0.9925\n",
            "Epoch 1403/3500\n",
            "4352/4352 [==============================] - 0s 16us/step - loss: 0.0266 - acc: 0.9926 - val_loss: 0.0402 - val_acc: 0.9916\n",
            "Epoch 1404/3500\n",
            "4352/4352 [==============================] - 0s 14us/step - loss: 0.0253 - acc: 0.9938 - val_loss: 0.0416 - val_acc: 0.9921\n",
            "Epoch 1405/3500\n",
            "4352/4352 [==============================] - 0s 15us/step - loss: 0.0255 - acc: 0.9926 - val_loss: 0.0403 - val_acc: 0.9930\n",
            "Epoch 1406/3500\n",
            "4352/4352 [==============================] - 0s 15us/step - loss: 0.0228 - acc: 0.9940 - val_loss: 0.0403 - val_acc: 0.9930\n",
            "Epoch 1407/3500\n",
            "4352/4352 [==============================] - 0s 15us/step - loss: 0.0234 - acc: 0.9949 - val_loss: 0.0444 - val_acc: 0.9907\n",
            "Epoch 1408/3500\n",
            "4352/4352 [==============================] - 0s 13us/step - loss: 0.0220 - acc: 0.9947 - val_loss: 0.0450 - val_acc: 0.9911\n",
            "Epoch 1409/3500\n",
            "4352/4352 [==============================] - 0s 13us/step - loss: 0.0222 - acc: 0.9943 - val_loss: 0.0437 - val_acc: 0.9911\n",
            "Epoch 1410/3500\n",
            "4352/4352 [==============================] - 0s 16us/step - loss: 0.0215 - acc: 0.9949 - val_loss: 0.0404 - val_acc: 0.9925\n",
            "Epoch 1411/3500\n",
            "4352/4352 [==============================] - 0s 14us/step - loss: 0.0220 - acc: 0.9945 - val_loss: 0.0396 - val_acc: 0.9921\n",
            "Epoch 1412/3500\n",
            "4352/4352 [==============================] - 0s 14us/step - loss: 0.0259 - acc: 0.9931 - val_loss: 0.0538 - val_acc: 0.9888\n",
            "Epoch 1413/3500\n",
            "4352/4352 [==============================] - 0s 14us/step - loss: 0.0235 - acc: 0.9940 - val_loss: 0.0500 - val_acc: 0.9902\n",
            "Epoch 1414/3500\n",
            "4352/4352 [==============================] - 0s 14us/step - loss: 0.0228 - acc: 0.9943 - val_loss: 0.0501 - val_acc: 0.9902\n",
            "Epoch 1415/3500\n",
            "4352/4352 [==============================] - 0s 14us/step - loss: 0.0227 - acc: 0.9943 - val_loss: 0.0420 - val_acc: 0.9916\n",
            "Epoch 1416/3500\n",
            "4352/4352 [==============================] - 0s 14us/step - loss: 0.0241 - acc: 0.9945 - val_loss: 0.0405 - val_acc: 0.9925\n",
            "Epoch 1417/3500\n",
            "4352/4352 [==============================] - 0s 15us/step - loss: 0.0231 - acc: 0.9943 - val_loss: 0.0396 - val_acc: 0.9925\n",
            "Epoch 1418/3500\n",
            "4352/4352 [==============================] - 0s 15us/step - loss: 0.0233 - acc: 0.9933 - val_loss: 0.0454 - val_acc: 0.9921\n",
            "Epoch 1419/3500\n",
            "4352/4352 [==============================] - 0s 15us/step - loss: 0.0229 - acc: 0.9936 - val_loss: 0.0517 - val_acc: 0.9897\n",
            "Epoch 1420/3500\n",
            "4352/4352 [==============================] - 0s 14us/step - loss: 0.0251 - acc: 0.9933 - val_loss: 0.0431 - val_acc: 0.9911\n",
            "Epoch 1421/3500\n",
            "4352/4352 [==============================] - 0s 14us/step - loss: 0.0216 - acc: 0.9943 - val_loss: 0.0399 - val_acc: 0.9935\n",
            "Epoch 1422/3500\n",
            "4352/4352 [==============================] - 0s 14us/step - loss: 0.0217 - acc: 0.9947 - val_loss: 0.0438 - val_acc: 0.9907\n",
            "Epoch 1423/3500\n",
            "4352/4352 [==============================] - 0s 14us/step - loss: 0.0242 - acc: 0.9936 - val_loss: 0.0527 - val_acc: 0.9893\n",
            "Epoch 1424/3500\n",
            "4352/4352 [==============================] - 0s 13us/step - loss: 0.0237 - acc: 0.9936 - val_loss: 0.0476 - val_acc: 0.9902\n",
            "Epoch 1425/3500\n",
            "4352/4352 [==============================] - 0s 14us/step - loss: 0.0217 - acc: 0.9947 - val_loss: 0.0411 - val_acc: 0.9921\n",
            "Epoch 1426/3500\n",
            "4352/4352 [==============================] - 0s 17us/step - loss: 0.0221 - acc: 0.9940 - val_loss: 0.0398 - val_acc: 0.9925\n",
            "Epoch 1427/3500\n",
            "4352/4352 [==============================] - 0s 15us/step - loss: 0.0227 - acc: 0.9945 - val_loss: 0.0414 - val_acc: 0.9921\n",
            "Epoch 1428/3500\n",
            "4352/4352 [==============================] - 0s 15us/step - loss: 0.0238 - acc: 0.9936 - val_loss: 0.0479 - val_acc: 0.9902\n",
            "Epoch 1429/3500\n",
            "4352/4352 [==============================] - 0s 13us/step - loss: 0.0221 - acc: 0.9943 - val_loss: 0.0446 - val_acc: 0.9911\n",
            "Epoch 1430/3500\n",
            "4352/4352 [==============================] - 0s 14us/step - loss: 0.0217 - acc: 0.9952 - val_loss: 0.0421 - val_acc: 0.9921\n",
            "Epoch 1431/3500\n",
            "4352/4352 [==============================] - 0s 14us/step - loss: 0.0216 - acc: 0.9945 - val_loss: 0.0391 - val_acc: 0.9911\n",
            "Epoch 1432/3500\n",
            "4352/4352 [==============================] - 0s 14us/step - loss: 0.0228 - acc: 0.9940 - val_loss: 0.0413 - val_acc: 0.9921\n",
            "Epoch 1433/3500\n",
            "4352/4352 [==============================] - 0s 15us/step - loss: 0.0219 - acc: 0.9936 - val_loss: 0.0466 - val_acc: 0.9907\n",
            "Epoch 1434/3500\n",
            "4352/4352 [==============================] - 0s 13us/step - loss: 0.0240 - acc: 0.9938 - val_loss: 0.0519 - val_acc: 0.9897\n",
            "Epoch 1435/3500\n",
            "4352/4352 [==============================] - 0s 14us/step - loss: 0.0245 - acc: 0.9933 - val_loss: 0.0470 - val_acc: 0.9907\n",
            "Epoch 1436/3500\n",
            "4352/4352 [==============================] - 0s 13us/step - loss: 0.0222 - acc: 0.9940 - val_loss: 0.0487 - val_acc: 0.9897\n",
            "Epoch 1437/3500\n",
            "4352/4352 [==============================] - 0s 14us/step - loss: 0.0301 - acc: 0.9917 - val_loss: 0.0481 - val_acc: 0.9902\n",
            "Epoch 1438/3500\n",
            "4352/4352 [==============================] - 0s 14us/step - loss: 0.0417 - acc: 0.9867 - val_loss: 0.0419 - val_acc: 0.9921\n",
            "Epoch 1439/3500\n",
            "4352/4352 [==============================] - 0s 14us/step - loss: 0.0256 - acc: 0.9931 - val_loss: 0.0416 - val_acc: 0.9916\n",
            "Epoch 1440/3500\n",
            "4352/4352 [==============================] - 0s 15us/step - loss: 0.0240 - acc: 0.9945 - val_loss: 0.0411 - val_acc: 0.9916\n",
            "Epoch 1441/3500\n",
            "4352/4352 [==============================] - 0s 16us/step - loss: 0.0256 - acc: 0.9933 - val_loss: 0.0465 - val_acc: 0.9911\n",
            "Epoch 1442/3500\n",
            "4352/4352 [==============================] - 0s 16us/step - loss: 0.0222 - acc: 0.9945 - val_loss: 0.0442 - val_acc: 0.9921\n",
            "Epoch 1443/3500\n",
            "4352/4352 [==============================] - 0s 14us/step - loss: 0.0220 - acc: 0.9945 - val_loss: 0.0402 - val_acc: 0.9925\n",
            "Epoch 1444/3500\n",
            "4352/4352 [==============================] - 0s 14us/step - loss: 0.0228 - acc: 0.9940 - val_loss: 0.0413 - val_acc: 0.9921\n",
            "Epoch 1445/3500\n",
            "4352/4352 [==============================] - 0s 13us/step - loss: 0.0221 - acc: 0.9943 - val_loss: 0.0476 - val_acc: 0.9902\n",
            "Epoch 1446/3500\n",
            "4352/4352 [==============================] - 0s 12us/step - loss: 0.0225 - acc: 0.9943 - val_loss: 0.0409 - val_acc: 0.9916\n",
            "Epoch 1447/3500\n",
            "4352/4352 [==============================] - 0s 13us/step - loss: 0.0226 - acc: 0.9947 - val_loss: 0.0411 - val_acc: 0.9921\n",
            "Epoch 1448/3500\n",
            "4352/4352 [==============================] - 0s 13us/step - loss: 0.0214 - acc: 0.9945 - val_loss: 0.0406 - val_acc: 0.9925\n",
            "Epoch 1449/3500\n",
            "4352/4352 [==============================] - 0s 14us/step - loss: 0.0233 - acc: 0.9945 - val_loss: 0.0397 - val_acc: 0.9921\n",
            "Epoch 1450/3500\n",
            "4352/4352 [==============================] - 0s 14us/step - loss: 0.0237 - acc: 0.9943 - val_loss: 0.0416 - val_acc: 0.9921\n",
            "Epoch 1451/3500\n",
            "4352/4352 [==============================] - 0s 15us/step - loss: 0.0228 - acc: 0.9947 - val_loss: 0.0526 - val_acc: 0.9897\n",
            "Epoch 1452/3500\n",
            "4352/4352 [==============================] - 0s 14us/step - loss: 0.0225 - acc: 0.9945 - val_loss: 0.0411 - val_acc: 0.9921\n",
            "Epoch 1453/3500\n",
            "4352/4352 [==============================] - 0s 14us/step - loss: 0.0233 - acc: 0.9943 - val_loss: 0.0408 - val_acc: 0.9925\n",
            "Epoch 1454/3500\n",
            "4352/4352 [==============================] - 0s 14us/step - loss: 0.0219 - acc: 0.9954 - val_loss: 0.0410 - val_acc: 0.9921\n",
            "Epoch 1455/3500\n",
            "4352/4352 [==============================] - 0s 14us/step - loss: 0.0219 - acc: 0.9947 - val_loss: 0.0420 - val_acc: 0.9921\n",
            "Epoch 1456/3500\n",
            "4352/4352 [==============================] - 0s 14us/step - loss: 0.0215 - acc: 0.9949 - val_loss: 0.0415 - val_acc: 0.9921\n",
            "Epoch 1457/3500\n",
            "4352/4352 [==============================] - 0s 15us/step - loss: 0.0212 - acc: 0.9952 - val_loss: 0.0437 - val_acc: 0.9911\n",
            "Epoch 1458/3500\n",
            "4352/4352 [==============================] - 0s 15us/step - loss: 0.0219 - acc: 0.9947 - val_loss: 0.0416 - val_acc: 0.9921\n",
            "Epoch 1459/3500\n",
            "4352/4352 [==============================] - 0s 15us/step - loss: 0.0218 - acc: 0.9947 - val_loss: 0.0399 - val_acc: 0.9925\n",
            "Epoch 1460/3500\n",
            "4352/4352 [==============================] - 0s 14us/step - loss: 0.0223 - acc: 0.9943 - val_loss: 0.0420 - val_acc: 0.9916\n",
            "Epoch 1461/3500\n",
            "4352/4352 [==============================] - 0s 15us/step - loss: 0.0215 - acc: 0.9945 - val_loss: 0.0417 - val_acc: 0.9921\n",
            "Epoch 1462/3500\n",
            "4352/4352 [==============================] - 0s 18us/step - loss: 0.0213 - acc: 0.9949 - val_loss: 0.0459 - val_acc: 0.9907\n",
            "Epoch 1463/3500\n",
            "4352/4352 [==============================] - 0s 13us/step - loss: 0.0219 - acc: 0.9947 - val_loss: 0.0424 - val_acc: 0.9921\n",
            "Epoch 1464/3500\n",
            "4352/4352 [==============================] - 0s 15us/step - loss: 0.0253 - acc: 0.9936 - val_loss: 0.0412 - val_acc: 0.9925\n",
            "Epoch 1465/3500\n",
            "4352/4352 [==============================] - 0s 14us/step - loss: 0.0257 - acc: 0.9940 - val_loss: 0.0408 - val_acc: 0.9930\n",
            "Epoch 1466/3500\n",
            "4352/4352 [==============================] - 0s 13us/step - loss: 0.0228 - acc: 0.9938 - val_loss: 0.0452 - val_acc: 0.9916\n",
            "Epoch 1467/3500\n",
            "4352/4352 [==============================] - 0s 13us/step - loss: 0.0217 - acc: 0.9947 - val_loss: 0.0414 - val_acc: 0.9921\n",
            "Epoch 1468/3500\n",
            "4352/4352 [==============================] - 0s 14us/step - loss: 0.0234 - acc: 0.9940 - val_loss: 0.0406 - val_acc: 0.9916\n",
            "Epoch 1469/3500\n",
            "4352/4352 [==============================] - 0s 15us/step - loss: 0.0230 - acc: 0.9945 - val_loss: 0.0398 - val_acc: 0.9921\n",
            "Epoch 1470/3500\n",
            "4352/4352 [==============================] - 0s 14us/step - loss: 0.0238 - acc: 0.9936 - val_loss: 0.0415 - val_acc: 0.9925\n",
            "Epoch 1471/3500\n",
            "4352/4352 [==============================] - 0s 15us/step - loss: 0.0248 - acc: 0.9933 - val_loss: 0.0407 - val_acc: 0.9935\n",
            "Epoch 1472/3500\n",
            "4352/4352 [==============================] - 0s 15us/step - loss: 0.0216 - acc: 0.9952 - val_loss: 0.0457 - val_acc: 0.9907\n",
            "Epoch 1473/3500\n",
            "4352/4352 [==============================] - 0s 15us/step - loss: 0.0225 - acc: 0.9940 - val_loss: 0.0526 - val_acc: 0.9897\n",
            "Epoch 1474/3500\n",
            "4352/4352 [==============================] - 0s 16us/step - loss: 0.0236 - acc: 0.9940 - val_loss: 0.0422 - val_acc: 0.9907\n",
            "Epoch 1475/3500\n",
            "4352/4352 [==============================] - 0s 14us/step - loss: 0.0246 - acc: 0.9936 - val_loss: 0.0395 - val_acc: 0.9925\n",
            "Epoch 1476/3500\n",
            "4352/4352 [==============================] - 0s 13us/step - loss: 0.0236 - acc: 0.9936 - val_loss: 0.0403 - val_acc: 0.9921\n",
            "Epoch 1477/3500\n",
            "4352/4352 [==============================] - 0s 15us/step - loss: 0.0287 - acc: 0.9926 - val_loss: 0.0403 - val_acc: 0.9921\n",
            "Epoch 1478/3500\n",
            "4352/4352 [==============================] - 0s 16us/step - loss: 0.0252 - acc: 0.9933 - val_loss: 0.0465 - val_acc: 0.9902\n",
            "Epoch 1479/3500\n",
            "4352/4352 [==============================] - 0s 13us/step - loss: 0.0284 - acc: 0.9920 - val_loss: 0.0400 - val_acc: 0.9925\n",
            "Epoch 1480/3500\n",
            "4352/4352 [==============================] - 0s 14us/step - loss: 0.0243 - acc: 0.9943 - val_loss: 0.0456 - val_acc: 0.9907\n",
            "Epoch 1481/3500\n",
            "4352/4352 [==============================] - 0s 16us/step - loss: 0.0217 - acc: 0.9945 - val_loss: 0.0451 - val_acc: 0.9907\n",
            "Epoch 1482/3500\n",
            "4352/4352 [==============================] - 0s 15us/step - loss: 0.0222 - acc: 0.9945 - val_loss: 0.0406 - val_acc: 0.9925\n",
            "Epoch 1483/3500\n",
            "4352/4352 [==============================] - 0s 15us/step - loss: 0.0212 - acc: 0.9947 - val_loss: 0.0425 - val_acc: 0.9911\n",
            "Epoch 1484/3500\n",
            "4352/4352 [==============================] - 0s 16us/step - loss: 0.0216 - acc: 0.9954 - val_loss: 0.0394 - val_acc: 0.9921\n",
            "Epoch 1485/3500\n",
            "4352/4352 [==============================] - 0s 15us/step - loss: 0.0222 - acc: 0.9945 - val_loss: 0.0395 - val_acc: 0.9916\n",
            "Epoch 1486/3500\n",
            "4352/4352 [==============================] - 0s 15us/step - loss: 0.0223 - acc: 0.9943 - val_loss: 0.0415 - val_acc: 0.9925\n",
            "Epoch 1487/3500\n",
            "4352/4352 [==============================] - 0s 13us/step - loss: 0.0222 - acc: 0.9945 - val_loss: 0.0414 - val_acc: 0.9925\n",
            "Epoch 1488/3500\n",
            "4352/4352 [==============================] - 0s 15us/step - loss: 0.0218 - acc: 0.9943 - val_loss: 0.0514 - val_acc: 0.9897\n",
            "Epoch 1489/3500\n",
            "4352/4352 [==============================] - 0s 13us/step - loss: 0.0220 - acc: 0.9956 - val_loss: 0.0404 - val_acc: 0.9925\n",
            "Epoch 1490/3500\n",
            "4352/4352 [==============================] - 0s 18us/step - loss: 0.0213 - acc: 0.9949 - val_loss: 0.0446 - val_acc: 0.9911\n",
            "Epoch 1491/3500\n",
            "4352/4352 [==============================] - 0s 15us/step - loss: 0.0210 - acc: 0.9956 - val_loss: 0.0400 - val_acc: 0.9911\n",
            "Epoch 1492/3500\n",
            "4352/4352 [==============================] - 0s 17us/step - loss: 0.0215 - acc: 0.9949 - val_loss: 0.0404 - val_acc: 0.9916\n",
            "Epoch 1493/3500\n",
            "4352/4352 [==============================] - 0s 14us/step - loss: 0.0219 - acc: 0.9947 - val_loss: 0.0431 - val_acc: 0.9911\n",
            "Epoch 1494/3500\n",
            "4352/4352 [==============================] - 0s 14us/step - loss: 0.0220 - acc: 0.9947 - val_loss: 0.0426 - val_acc: 0.9921\n",
            "Epoch 1495/3500\n",
            "4352/4352 [==============================] - 0s 14us/step - loss: 0.0214 - acc: 0.9949 - val_loss: 0.0449 - val_acc: 0.9907\n",
            "Epoch 1496/3500\n",
            "4352/4352 [==============================] - 0s 14us/step - loss: 0.0211 - acc: 0.9952 - val_loss: 0.0398 - val_acc: 0.9925\n",
            "Epoch 1497/3500\n",
            "4352/4352 [==============================] - 0s 16us/step - loss: 0.0215 - acc: 0.9947 - val_loss: 0.0448 - val_acc: 0.9907\n",
            "Epoch 1498/3500\n",
            "4352/4352 [==============================] - 0s 14us/step - loss: 0.0218 - acc: 0.9940 - val_loss: 0.0428 - val_acc: 0.9921\n",
            "Epoch 1499/3500\n",
            "4352/4352 [==============================] - 0s 18us/step - loss: 0.0212 - acc: 0.9956 - val_loss: 0.0408 - val_acc: 0.9930\n",
            "Epoch 1500/3500\n",
            "4352/4352 [==============================] - 0s 18us/step - loss: 0.0231 - acc: 0.9952 - val_loss: 0.0416 - val_acc: 0.9925\n",
            "Epoch 1501/3500\n",
            "4352/4352 [==============================] - 0s 15us/step - loss: 0.0246 - acc: 0.9936 - val_loss: 0.0407 - val_acc: 0.9930\n",
            "Epoch 1502/3500\n",
            "4352/4352 [==============================] - 0s 14us/step - loss: 0.0231 - acc: 0.9947 - val_loss: 0.0481 - val_acc: 0.9902\n",
            "Epoch 1503/3500\n",
            "4352/4352 [==============================] - 0s 14us/step - loss: 0.0232 - acc: 0.9936 - val_loss: 0.0439 - val_acc: 0.9925\n",
            "Epoch 1504/3500\n",
            "4352/4352 [==============================] - 0s 17us/step - loss: 0.0219 - acc: 0.9949 - val_loss: 0.0431 - val_acc: 0.9916\n",
            "Epoch 1505/3500\n",
            "4352/4352 [==============================] - 0s 24us/step - loss: 0.0217 - acc: 0.9949 - val_loss: 0.0418 - val_acc: 0.9925\n",
            "Epoch 1506/3500\n",
            "4352/4352 [==============================] - 0s 14us/step - loss: 0.0225 - acc: 0.9945 - val_loss: 0.0500 - val_acc: 0.9897\n",
            "Epoch 1507/3500\n",
            "4352/4352 [==============================] - 0s 14us/step - loss: 0.0218 - acc: 0.9954 - val_loss: 0.0408 - val_acc: 0.9916\n",
            "Epoch 1508/3500\n",
            "4352/4352 [==============================] - 0s 13us/step - loss: 0.0231 - acc: 0.9940 - val_loss: 0.0398 - val_acc: 0.9911\n",
            "Epoch 1509/3500\n",
            "4352/4352 [==============================] - 0s 14us/step - loss: 0.0235 - acc: 0.9938 - val_loss: 0.0419 - val_acc: 0.9916\n",
            "Epoch 1510/3500\n",
            "4352/4352 [==============================] - 0s 13us/step - loss: 0.0279 - acc: 0.9917 - val_loss: 0.0416 - val_acc: 0.9921\n",
            "Epoch 1511/3500\n",
            "4352/4352 [==============================] - 0s 15us/step - loss: 0.0239 - acc: 0.9931 - val_loss: 0.0471 - val_acc: 0.9902\n",
            "Epoch 1512/3500\n",
            "4352/4352 [==============================] - 0s 15us/step - loss: 0.0239 - acc: 0.9940 - val_loss: 0.0457 - val_acc: 0.9911\n",
            "Epoch 1513/3500\n",
            "4352/4352 [==============================] - 0s 16us/step - loss: 0.0222 - acc: 0.9943 - val_loss: 0.0419 - val_acc: 0.9911\n",
            "Epoch 1514/3500\n",
            "4352/4352 [==============================] - 0s 15us/step - loss: 0.0213 - acc: 0.9947 - val_loss: 0.0429 - val_acc: 0.9921\n",
            "Epoch 1515/3500\n",
            "4352/4352 [==============================] - 0s 19us/step - loss: 0.0217 - acc: 0.9947 - val_loss: 0.0442 - val_acc: 0.9911\n",
            "Epoch 1516/3500\n",
            "4352/4352 [==============================] - 0s 14us/step - loss: 0.0213 - acc: 0.9947 - val_loss: 0.0422 - val_acc: 0.9921\n",
            "Epoch 1517/3500\n",
            "4352/4352 [==============================] - 0s 13us/step - loss: 0.0214 - acc: 0.9943 - val_loss: 0.0446 - val_acc: 0.9907\n",
            "Epoch 1518/3500\n",
            "4352/4352 [==============================] - 0s 14us/step - loss: 0.0217 - acc: 0.9947 - val_loss: 0.0476 - val_acc: 0.9902\n",
            "Epoch 1519/3500\n",
            "4352/4352 [==============================] - 0s 14us/step - loss: 0.0232 - acc: 0.9940 - val_loss: 0.0555 - val_acc: 0.9893\n",
            "Epoch 1520/3500\n",
            "4352/4352 [==============================] - 0s 15us/step - loss: 0.0257 - acc: 0.9933 - val_loss: 0.0484 - val_acc: 0.9902\n",
            "Epoch 1521/3500\n",
            "4352/4352 [==============================] - 0s 15us/step - loss: 0.0220 - acc: 0.9943 - val_loss: 0.0461 - val_acc: 0.9911\n",
            "Epoch 1522/3500\n",
            "4352/4352 [==============================] - 0s 21us/step - loss: 0.0227 - acc: 0.9940 - val_loss: 0.0407 - val_acc: 0.9916\n",
            "Epoch 1523/3500\n",
            "4352/4352 [==============================] - 0s 14us/step - loss: 0.0228 - acc: 0.9943 - val_loss: 0.0402 - val_acc: 0.9925\n",
            "Epoch 1524/3500\n",
            "4352/4352 [==============================] - 0s 14us/step - loss: 0.0220 - acc: 0.9947 - val_loss: 0.0421 - val_acc: 0.9925\n",
            "Epoch 1525/3500\n",
            "4352/4352 [==============================] - 0s 14us/step - loss: 0.0214 - acc: 0.9952 - val_loss: 0.0462 - val_acc: 0.9911\n",
            "Epoch 1526/3500\n",
            "4352/4352 [==============================] - 0s 13us/step - loss: 0.0227 - acc: 0.9940 - val_loss: 0.0411 - val_acc: 0.9925\n",
            "Epoch 1527/3500\n",
            "4352/4352 [==============================] - 0s 16us/step - loss: 0.0250 - acc: 0.9933 - val_loss: 0.0438 - val_acc: 0.9921\n",
            "Epoch 1528/3500\n",
            "4352/4352 [==============================] - 0s 15us/step - loss: 0.0231 - acc: 0.9943 - val_loss: 0.0433 - val_acc: 0.9911\n",
            "Epoch 1529/3500\n",
            "4352/4352 [==============================] - 0s 15us/step - loss: 0.0218 - acc: 0.9938 - val_loss: 0.0442 - val_acc: 0.9921\n",
            "Epoch 1530/3500\n",
            "4352/4352 [==============================] - 0s 15us/step - loss: 0.0215 - acc: 0.9943 - val_loss: 0.0422 - val_acc: 0.9921\n",
            "Epoch 1531/3500\n",
            "4352/4352 [==============================] - 0s 16us/step - loss: 0.0215 - acc: 0.9952 - val_loss: 0.0411 - val_acc: 0.9925\n",
            "Epoch 1532/3500\n",
            "4352/4352 [==============================] - 0s 16us/step - loss: 0.0219 - acc: 0.9952 - val_loss: 0.0436 - val_acc: 0.9907\n",
            "Epoch 1533/3500\n",
            "4352/4352 [==============================] - 0s 14us/step - loss: 0.0217 - acc: 0.9940 - val_loss: 0.0494 - val_acc: 0.9902\n",
            "Epoch 1534/3500\n",
            "4352/4352 [==============================] - 0s 14us/step - loss: 0.0225 - acc: 0.9940 - val_loss: 0.0483 - val_acc: 0.9907\n",
            "Epoch 1535/3500\n",
            "4352/4352 [==============================] - 0s 18us/step - loss: 0.0223 - acc: 0.9945 - val_loss: 0.0434 - val_acc: 0.9911\n",
            "Epoch 1536/3500\n",
            "4352/4352 [==============================] - 0s 14us/step - loss: 0.0225 - acc: 0.9949 - val_loss: 0.0436 - val_acc: 0.9921\n",
            "Epoch 1537/3500\n",
            "4352/4352 [==============================] - 0s 14us/step - loss: 0.0242 - acc: 0.9936 - val_loss: 0.0401 - val_acc: 0.9935\n",
            "Epoch 1538/3500\n",
            "4352/4352 [==============================] - 0s 13us/step - loss: 0.0226 - acc: 0.9938 - val_loss: 0.0553 - val_acc: 0.9897\n",
            "Epoch 1539/3500\n",
            "4352/4352 [==============================] - 0s 15us/step - loss: 0.0258 - acc: 0.9926 - val_loss: 0.0448 - val_acc: 0.9911\n",
            "Epoch 1540/3500\n",
            "4352/4352 [==============================] - 0s 15us/step - loss: 0.0252 - acc: 0.9929 - val_loss: 0.0460 - val_acc: 0.9911\n",
            "Epoch 1541/3500\n",
            "4352/4352 [==============================] - 0s 13us/step - loss: 0.0237 - acc: 0.9938 - val_loss: 0.0544 - val_acc: 0.9897\n",
            "Epoch 1542/3500\n",
            "4352/4352 [==============================] - 0s 14us/step - loss: 0.0231 - acc: 0.9949 - val_loss: 0.0501 - val_acc: 0.9893\n",
            "Epoch 1543/3500\n",
            "4352/4352 [==============================] - 0s 14us/step - loss: 0.0236 - acc: 0.9945 - val_loss: 0.0460 - val_acc: 0.9907\n",
            "Epoch 1544/3500\n",
            "4352/4352 [==============================] - 0s 15us/step - loss: 0.0252 - acc: 0.9924 - val_loss: 0.0431 - val_acc: 0.9916\n",
            "Epoch 1545/3500\n",
            "4352/4352 [==============================] - 0s 15us/step - loss: 0.0246 - acc: 0.9931 - val_loss: 0.0396 - val_acc: 0.9925\n",
            "Epoch 1546/3500\n",
            "4352/4352 [==============================] - 0s 14us/step - loss: 0.0253 - acc: 0.9926 - val_loss: 0.0408 - val_acc: 0.9921\n",
            "Epoch 1547/3500\n",
            "4352/4352 [==============================] - 0s 16us/step - loss: 0.0253 - acc: 0.9938 - val_loss: 0.0415 - val_acc: 0.9925\n",
            "Epoch 1548/3500\n",
            "4352/4352 [==============================] - 0s 15us/step - loss: 0.0217 - acc: 0.9943 - val_loss: 0.0412 - val_acc: 0.9916\n",
            "Epoch 1549/3500\n",
            "4352/4352 [==============================] - 0s 15us/step - loss: 0.0252 - acc: 0.9940 - val_loss: 0.0493 - val_acc: 0.9902\n",
            "Epoch 1550/3500\n",
            "4352/4352 [==============================] - 0s 18us/step - loss: 0.0263 - acc: 0.9933 - val_loss: 0.0515 - val_acc: 0.9897\n",
            "Epoch 1551/3500\n",
            "4352/4352 [==============================] - 0s 14us/step - loss: 0.0304 - acc: 0.9903 - val_loss: 0.0501 - val_acc: 0.9897\n",
            "Epoch 1552/3500\n",
            "4352/4352 [==============================] - 0s 14us/step - loss: 0.0308 - acc: 0.9922 - val_loss: 0.0631 - val_acc: 0.9865\n",
            "Epoch 1553/3500\n",
            "4352/4352 [==============================] - 0s 14us/step - loss: 0.0285 - acc: 0.9917 - val_loss: 0.0687 - val_acc: 0.9860\n",
            "Epoch 1554/3500\n",
            "4352/4352 [==============================] - 0s 13us/step - loss: 0.0307 - acc: 0.9920 - val_loss: 0.0564 - val_acc: 0.9893\n",
            "Epoch 1555/3500\n",
            "4352/4352 [==============================] - 0s 14us/step - loss: 0.0252 - acc: 0.9929 - val_loss: 0.0406 - val_acc: 0.9916\n",
            "Epoch 1556/3500\n",
            "4352/4352 [==============================] - 0s 13us/step - loss: 0.0248 - acc: 0.9929 - val_loss: 0.0404 - val_acc: 0.9921\n",
            "Epoch 1557/3500\n",
            "4352/4352 [==============================] - 0s 13us/step - loss: 0.0239 - acc: 0.9940 - val_loss: 0.0443 - val_acc: 0.9921\n",
            "Epoch 1558/3500\n",
            "4352/4352 [==============================] - 0s 15us/step - loss: 0.0257 - acc: 0.9938 - val_loss: 0.0424 - val_acc: 0.9921\n",
            "Epoch 1559/3500\n",
            "4352/4352 [==============================] - 0s 13us/step - loss: 0.0238 - acc: 0.9929 - val_loss: 0.0408 - val_acc: 0.9921\n",
            "Epoch 1560/3500\n",
            "4352/4352 [==============================] - 0s 14us/step - loss: 0.0211 - acc: 0.9947 - val_loss: 0.0482 - val_acc: 0.9907\n",
            "Epoch 1561/3500\n",
            "4352/4352 [==============================] - 0s 14us/step - loss: 0.0224 - acc: 0.9952 - val_loss: 0.0427 - val_acc: 0.9911\n",
            "Epoch 1562/3500\n",
            "4352/4352 [==============================] - 0s 13us/step - loss: 0.0221 - acc: 0.9945 - val_loss: 0.0434 - val_acc: 0.9925\n",
            "Epoch 1563/3500\n",
            "4352/4352 [==============================] - 0s 14us/step - loss: 0.0216 - acc: 0.9949 - val_loss: 0.0398 - val_acc: 0.9911\n",
            "Epoch 1564/3500\n",
            "4352/4352 [==============================] - 0s 16us/step - loss: 0.0217 - acc: 0.9947 - val_loss: 0.0428 - val_acc: 0.9916\n",
            "Epoch 1565/3500\n",
            "4352/4352 [==============================] - 0s 18us/step - loss: 0.0216 - acc: 0.9947 - val_loss: 0.0410 - val_acc: 0.9930\n",
            "Epoch 1566/3500\n",
            "4352/4352 [==============================] - 0s 17us/step - loss: 0.0217 - acc: 0.9947 - val_loss: 0.0403 - val_acc: 0.9916\n",
            "Epoch 1567/3500\n",
            "4352/4352 [==============================] - 0s 13us/step - loss: 0.0225 - acc: 0.9947 - val_loss: 0.0408 - val_acc: 0.9925\n",
            "Epoch 1568/3500\n",
            "4352/4352 [==============================] - 0s 13us/step - loss: 0.0234 - acc: 0.9933 - val_loss: 0.0457 - val_acc: 0.9916\n",
            "Epoch 1569/3500\n",
            "4352/4352 [==============================] - 0s 15us/step - loss: 0.0239 - acc: 0.9933 - val_loss: 0.0559 - val_acc: 0.9883\n",
            "Epoch 1570/3500\n",
            "4352/4352 [==============================] - 0s 14us/step - loss: 0.0240 - acc: 0.9938 - val_loss: 0.0422 - val_acc: 0.9925\n",
            "Epoch 1571/3500\n",
            "4352/4352 [==============================] - 0s 13us/step - loss: 0.0218 - acc: 0.9947 - val_loss: 0.0454 - val_acc: 0.9911\n",
            "Epoch 1572/3500\n",
            "4352/4352 [==============================] - 0s 14us/step - loss: 0.0217 - acc: 0.9947 - val_loss: 0.0441 - val_acc: 0.9916\n",
            "Epoch 1573/3500\n",
            "4352/4352 [==============================] - 0s 13us/step - loss: 0.0224 - acc: 0.9943 - val_loss: 0.0414 - val_acc: 0.9925\n",
            "Epoch 1574/3500\n",
            "4352/4352 [==============================] - 0s 14us/step - loss: 0.0239 - acc: 0.9931 - val_loss: 0.0414 - val_acc: 0.9925\n",
            "Epoch 1575/3500\n",
            "4352/4352 [==============================] - 0s 13us/step - loss: 0.0230 - acc: 0.9945 - val_loss: 0.0408 - val_acc: 0.9916\n",
            "Epoch 1576/3500\n",
            "4352/4352 [==============================] - 0s 13us/step - loss: 0.0223 - acc: 0.9933 - val_loss: 0.0556 - val_acc: 0.9883\n",
            "Epoch 1577/3500\n",
            "4352/4352 [==============================] - 0s 13us/step - loss: 0.0223 - acc: 0.9947 - val_loss: 0.0402 - val_acc: 0.9935\n",
            "Epoch 1578/3500\n",
            "4352/4352 [==============================] - 0s 14us/step - loss: 0.0209 - acc: 0.9949 - val_loss: 0.0424 - val_acc: 0.9911\n",
            "Epoch 1579/3500\n",
            "4352/4352 [==============================] - 0s 15us/step - loss: 0.0210 - acc: 0.9952 - val_loss: 0.0459 - val_acc: 0.9907\n",
            "Epoch 1580/3500\n",
            "4352/4352 [==============================] - 0s 15us/step - loss: 0.0221 - acc: 0.9943 - val_loss: 0.0446 - val_acc: 0.9911\n",
            "Epoch 1581/3500\n",
            "4352/4352 [==============================] - 0s 14us/step - loss: 0.0216 - acc: 0.9947 - val_loss: 0.0422 - val_acc: 0.9921\n",
            "Epoch 1582/3500\n",
            "4352/4352 [==============================] - 0s 16us/step - loss: 0.0209 - acc: 0.9952 - val_loss: 0.0415 - val_acc: 0.9930\n",
            "Epoch 1583/3500\n",
            "4352/4352 [==============================] - 0s 15us/step - loss: 0.0209 - acc: 0.9952 - val_loss: 0.0417 - val_acc: 0.9921\n",
            "Epoch 1584/3500\n",
            "4352/4352 [==============================] - 0s 14us/step - loss: 0.0213 - acc: 0.9954 - val_loss: 0.0411 - val_acc: 0.9925\n",
            "Epoch 1585/3500\n",
            "4352/4352 [==============================] - 0s 15us/step - loss: 0.0206 - acc: 0.9947 - val_loss: 0.0468 - val_acc: 0.9902\n",
            "Epoch 1586/3500\n",
            "4352/4352 [==============================] - 0s 15us/step - loss: 0.0213 - acc: 0.9952 - val_loss: 0.0414 - val_acc: 0.9921\n",
            "Epoch 1587/3500\n",
            "4352/4352 [==============================] - 0s 14us/step - loss: 0.0213 - acc: 0.9949 - val_loss: 0.0483 - val_acc: 0.9902\n",
            "Epoch 1588/3500\n",
            "4352/4352 [==============================] - 0s 15us/step - loss: 0.0236 - acc: 0.9933 - val_loss: 0.0450 - val_acc: 0.9916\n",
            "Epoch 1589/3500\n",
            "4352/4352 [==============================] - 0s 15us/step - loss: 0.0223 - acc: 0.9945 - val_loss: 0.0447 - val_acc: 0.9916\n",
            "Epoch 1590/3500\n",
            "4352/4352 [==============================] - 0s 14us/step - loss: 0.0209 - acc: 0.9943 - val_loss: 0.0419 - val_acc: 0.9921\n",
            "Epoch 1591/3500\n",
            "4352/4352 [==============================] - 0s 15us/step - loss: 0.0208 - acc: 0.9949 - val_loss: 0.0419 - val_acc: 0.9921\n",
            "Epoch 1592/3500\n",
            "4352/4352 [==============================] - 0s 16us/step - loss: 0.0211 - acc: 0.9949 - val_loss: 0.0435 - val_acc: 0.9911\n",
            "Epoch 1593/3500\n",
            "4352/4352 [==============================] - 0s 15us/step - loss: 0.0215 - acc: 0.9947 - val_loss: 0.0467 - val_acc: 0.9902\n",
            "Epoch 1594/3500\n",
            "4352/4352 [==============================] - 0s 16us/step - loss: 0.0223 - acc: 0.9936 - val_loss: 0.0472 - val_acc: 0.9902\n",
            "Epoch 1595/3500\n",
            "4352/4352 [==============================] - 0s 14us/step - loss: 0.0243 - acc: 0.9945 - val_loss: 0.0412 - val_acc: 0.9925\n",
            "Epoch 1596/3500\n",
            "4352/4352 [==============================] - 0s 13us/step - loss: 0.0244 - acc: 0.9940 - val_loss: 0.0453 - val_acc: 0.9911\n",
            "Epoch 1597/3500\n",
            "4352/4352 [==============================] - 0s 14us/step - loss: 0.0225 - acc: 0.9945 - val_loss: 0.0458 - val_acc: 0.9916\n",
            "Epoch 1598/3500\n",
            "4352/4352 [==============================] - 0s 16us/step - loss: 0.0214 - acc: 0.9947 - val_loss: 0.0418 - val_acc: 0.9921\n",
            "Epoch 1599/3500\n",
            "4352/4352 [==============================] - 0s 15us/step - loss: 0.0210 - acc: 0.9949 - val_loss: 0.0404 - val_acc: 0.9916\n",
            "Epoch 1600/3500\n",
            "4352/4352 [==============================] - 0s 14us/step - loss: 0.0224 - acc: 0.9947 - val_loss: 0.0413 - val_acc: 0.9930\n",
            "Epoch 1601/3500\n",
            "4352/4352 [==============================] - 0s 14us/step - loss: 0.0230 - acc: 0.9947 - val_loss: 0.0419 - val_acc: 0.9921\n",
            "Epoch 1602/3500\n",
            "4352/4352 [==============================] - 0s 15us/step - loss: 0.0223 - acc: 0.9931 - val_loss: 0.0556 - val_acc: 0.9883\n",
            "Epoch 1603/3500\n",
            "4352/4352 [==============================] - 0s 16us/step - loss: 0.0207 - acc: 0.9954 - val_loss: 0.0402 - val_acc: 0.9930\n",
            "Epoch 1604/3500\n",
            "4352/4352 [==============================] - 0s 16us/step - loss: 0.0244 - acc: 0.9936 - val_loss: 0.0419 - val_acc: 0.9916\n",
            "Epoch 1605/3500\n",
            "4352/4352 [==============================] - 0s 14us/step - loss: 0.0223 - acc: 0.9945 - val_loss: 0.0422 - val_acc: 0.9925\n",
            "Epoch 1606/3500\n",
            "4352/4352 [==============================] - 0s 14us/step - loss: 0.0224 - acc: 0.9943 - val_loss: 0.0405 - val_acc: 0.9916\n",
            "Epoch 1607/3500\n",
            "4352/4352 [==============================] - 0s 14us/step - loss: 0.0221 - acc: 0.9945 - val_loss: 0.0418 - val_acc: 0.9935\n",
            "Epoch 1608/3500\n",
            "4352/4352 [==============================] - 0s 14us/step - loss: 0.0219 - acc: 0.9943 - val_loss: 0.0477 - val_acc: 0.9902\n",
            "Epoch 1609/3500\n",
            "4352/4352 [==============================] - 0s 14us/step - loss: 0.0228 - acc: 0.9940 - val_loss: 0.0408 - val_acc: 0.9921\n",
            "Epoch 1610/3500\n",
            "4352/4352 [==============================] - 0s 14us/step - loss: 0.0212 - acc: 0.9947 - val_loss: 0.0471 - val_acc: 0.9902\n",
            "Epoch 1611/3500\n",
            "4352/4352 [==============================] - 0s 15us/step - loss: 0.0214 - acc: 0.9947 - val_loss: 0.0432 - val_acc: 0.9911\n",
            "Epoch 1612/3500\n",
            "4352/4352 [==============================] - 0s 14us/step - loss: 0.0208 - acc: 0.9947 - val_loss: 0.0457 - val_acc: 0.9907\n",
            "Epoch 1613/3500\n",
            "4352/4352 [==============================] - 0s 14us/step - loss: 0.0220 - acc: 0.9945 - val_loss: 0.0591 - val_acc: 0.9888\n",
            "Epoch 1614/3500\n",
            "4352/4352 [==============================] - 0s 15us/step - loss: 0.0231 - acc: 0.9943 - val_loss: 0.0456 - val_acc: 0.9911\n",
            "Epoch 1615/3500\n",
            "4352/4352 [==============================] - 0s 14us/step - loss: 0.0215 - acc: 0.9949 - val_loss: 0.0410 - val_acc: 0.9925\n",
            "Epoch 1616/3500\n",
            "4352/4352 [==============================] - 0s 15us/step - loss: 0.0220 - acc: 0.9938 - val_loss: 0.0437 - val_acc: 0.9911\n",
            "Epoch 1617/3500\n",
            "4352/4352 [==============================] - 0s 15us/step - loss: 0.0218 - acc: 0.9954 - val_loss: 0.0589 - val_acc: 0.9888\n",
            "Epoch 1618/3500\n",
            "4352/4352 [==============================] - 0s 15us/step - loss: 0.0234 - acc: 0.9931 - val_loss: 0.0432 - val_acc: 0.9911\n",
            "Epoch 1619/3500\n",
            "4352/4352 [==============================] - 0s 15us/step - loss: 0.0229 - acc: 0.9940 - val_loss: 0.0425 - val_acc: 0.9916\n",
            "Epoch 1620/3500\n",
            "4352/4352 [==============================] - 0s 15us/step - loss: 0.0230 - acc: 0.9940 - val_loss: 0.0438 - val_acc: 0.9916\n",
            "Epoch 1621/3500\n",
            "4352/4352 [==============================] - 0s 17us/step - loss: 0.0268 - acc: 0.9922 - val_loss: 0.0441 - val_acc: 0.9916\n",
            "Epoch 1622/3500\n",
            "4352/4352 [==============================] - 0s 15us/step - loss: 0.0268 - acc: 0.9936 - val_loss: 0.0412 - val_acc: 0.9916\n",
            "Epoch 1623/3500\n",
            "4352/4352 [==============================] - 0s 15us/step - loss: 0.0235 - acc: 0.9945 - val_loss: 0.0412 - val_acc: 0.9930\n",
            "Epoch 1624/3500\n",
            "4352/4352 [==============================] - 0s 15us/step - loss: 0.0221 - acc: 0.9952 - val_loss: 0.0459 - val_acc: 0.9907\n",
            "Epoch 1625/3500\n",
            "4352/4352 [==============================] - 0s 16us/step - loss: 0.0225 - acc: 0.9938 - val_loss: 0.0405 - val_acc: 0.9925\n",
            "Epoch 1626/3500\n",
            "4352/4352 [==============================] - 0s 14us/step - loss: 0.0251 - acc: 0.9936 - val_loss: 0.0459 - val_acc: 0.9911\n",
            "Epoch 1627/3500\n",
            "4352/4352 [==============================] - 0s 15us/step - loss: 0.0247 - acc: 0.9947 - val_loss: 0.0465 - val_acc: 0.9911\n",
            "Epoch 1628/3500\n",
            "4352/4352 [==============================] - 0s 16us/step - loss: 0.0212 - acc: 0.9949 - val_loss: 0.0429 - val_acc: 0.9921\n",
            "Epoch 1629/3500\n",
            "4352/4352 [==============================] - 0s 14us/step - loss: 0.0238 - acc: 0.9929 - val_loss: 0.0426 - val_acc: 0.9921\n",
            "Epoch 1630/3500\n",
            "4352/4352 [==============================] - 0s 15us/step - loss: 0.0276 - acc: 0.9924 - val_loss: 0.0416 - val_acc: 0.9921\n",
            "Epoch 1631/3500\n",
            "4352/4352 [==============================] - 0s 15us/step - loss: 0.0246 - acc: 0.9933 - val_loss: 0.0413 - val_acc: 0.9921\n",
            "Epoch 1632/3500\n",
            "4352/4352 [==============================] - 0s 15us/step - loss: 0.0214 - acc: 0.9943 - val_loss: 0.0442 - val_acc: 0.9921\n",
            "Epoch 1633/3500\n",
            "4352/4352 [==============================] - 0s 15us/step - loss: 0.0213 - acc: 0.9952 - val_loss: 0.0419 - val_acc: 0.9925\n",
            "Epoch 1634/3500\n",
            "4352/4352 [==============================] - 0s 13us/step - loss: 0.0207 - acc: 0.9952 - val_loss: 0.0422 - val_acc: 0.9921\n",
            "Epoch 1635/3500\n",
            "4352/4352 [==============================] - 0s 13us/step - loss: 0.0209 - acc: 0.9947 - val_loss: 0.0421 - val_acc: 0.9925\n",
            "Epoch 1636/3500\n",
            "4352/4352 [==============================] - 0s 14us/step - loss: 0.0212 - acc: 0.9949 - val_loss: 0.0434 - val_acc: 0.9925\n",
            "Epoch 1637/3500\n",
            "4352/4352 [==============================] - 0s 15us/step - loss: 0.0213 - acc: 0.9945 - val_loss: 0.0433 - val_acc: 0.9916\n",
            "Epoch 1638/3500\n",
            "4352/4352 [==============================] - 0s 16us/step - loss: 0.0215 - acc: 0.9949 - val_loss: 0.0463 - val_acc: 0.9916\n",
            "Epoch 1639/3500\n",
            "4352/4352 [==============================] - 0s 13us/step - loss: 0.0209 - acc: 0.9949 - val_loss: 0.0416 - val_acc: 0.9921\n",
            "Epoch 1640/3500\n",
            "4352/4352 [==============================] - 0s 14us/step - loss: 0.0210 - acc: 0.9947 - val_loss: 0.0424 - val_acc: 0.9925\n",
            "Epoch 1641/3500\n",
            "4352/4352 [==============================] - 0s 14us/step - loss: 0.0213 - acc: 0.9952 - val_loss: 0.0435 - val_acc: 0.9911\n",
            "Epoch 1642/3500\n",
            "4352/4352 [==============================] - 0s 14us/step - loss: 0.0216 - acc: 0.9947 - val_loss: 0.0415 - val_acc: 0.9925\n",
            "Epoch 1643/3500\n",
            "4352/4352 [==============================] - 0s 17us/step - loss: 0.0240 - acc: 0.9943 - val_loss: 0.0419 - val_acc: 0.9921\n",
            "Epoch 1644/3500\n",
            "4352/4352 [==============================] - 0s 16us/step - loss: 0.0226 - acc: 0.9945 - val_loss: 0.0414 - val_acc: 0.9925\n",
            "Epoch 1645/3500\n",
            "4352/4352 [==============================] - 0s 15us/step - loss: 0.0241 - acc: 0.9933 - val_loss: 0.0445 - val_acc: 0.9916\n",
            "Epoch 1646/3500\n",
            "4352/4352 [==============================] - 0s 13us/step - loss: 0.0227 - acc: 0.9936 - val_loss: 0.0429 - val_acc: 0.9916\n",
            "Epoch 1647/3500\n",
            "4352/4352 [==============================] - 0s 14us/step - loss: 0.0225 - acc: 0.9931 - val_loss: 0.0484 - val_acc: 0.9911\n",
            "Epoch 1648/3500\n",
            "4352/4352 [==============================] - 0s 14us/step - loss: 0.0222 - acc: 0.9940 - val_loss: 0.0482 - val_acc: 0.9902\n",
            "Epoch 1649/3500\n",
            "4352/4352 [==============================] - 0s 14us/step - loss: 0.0224 - acc: 0.9949 - val_loss: 0.0500 - val_acc: 0.9907\n",
            "Epoch 1650/3500\n",
            "4352/4352 [==============================] - 0s 14us/step - loss: 0.0247 - acc: 0.9931 - val_loss: 0.0425 - val_acc: 0.9916\n",
            "Epoch 1651/3500\n",
            "4352/4352 [==============================] - 0s 14us/step - loss: 0.0277 - acc: 0.9920 - val_loss: 0.0406 - val_acc: 0.9925\n",
            "Epoch 1652/3500\n",
            "4352/4352 [==============================] - 0s 15us/step - loss: 0.0218 - acc: 0.9947 - val_loss: 0.0414 - val_acc: 0.9916\n",
            "Epoch 1653/3500\n",
            "4352/4352 [==============================] - 0s 15us/step - loss: 0.0233 - acc: 0.9938 - val_loss: 0.0427 - val_acc: 0.9916\n",
            "Epoch 1654/3500\n",
            "4352/4352 [==============================] - 0s 14us/step - loss: 0.0210 - acc: 0.9943 - val_loss: 0.0453 - val_acc: 0.9921\n",
            "Epoch 1655/3500\n",
            "4352/4352 [==============================] - 0s 14us/step - loss: 0.0211 - acc: 0.9949 - val_loss: 0.0453 - val_acc: 0.9907\n",
            "Epoch 1656/3500\n",
            "4352/4352 [==============================] - 0s 14us/step - loss: 0.0213 - acc: 0.9938 - val_loss: 0.0482 - val_acc: 0.9911\n",
            "Epoch 1657/3500\n",
            "4352/4352 [==============================] - 0s 14us/step - loss: 0.0218 - acc: 0.9940 - val_loss: 0.0411 - val_acc: 0.9925\n",
            "Epoch 1658/3500\n",
            "4352/4352 [==============================] - 0s 14us/step - loss: 0.0210 - acc: 0.9949 - val_loss: 0.0417 - val_acc: 0.9911\n",
            "Epoch 1659/3500\n",
            "4352/4352 [==============================] - 0s 14us/step - loss: 0.0216 - acc: 0.9945 - val_loss: 0.0444 - val_acc: 0.9916\n",
            "Epoch 1660/3500\n",
            "4352/4352 [==============================] - 0s 16us/step - loss: 0.0210 - acc: 0.9949 - val_loss: 0.0446 - val_acc: 0.9921\n",
            "Epoch 1661/3500\n",
            "4352/4352 [==============================] - 0s 15us/step - loss: 0.0202 - acc: 0.9945 - val_loss: 0.0511 - val_acc: 0.9902\n",
            "Epoch 1662/3500\n",
            "4352/4352 [==============================] - 0s 14us/step - loss: 0.0211 - acc: 0.9952 - val_loss: 0.0421 - val_acc: 0.9925\n",
            "Epoch 1663/3500\n",
            "4352/4352 [==============================] - 0s 15us/step - loss: 0.0207 - acc: 0.9949 - val_loss: 0.0410 - val_acc: 0.9916\n",
            "Epoch 1664/3500\n",
            "4352/4352 [==============================] - 0s 14us/step - loss: 0.0213 - acc: 0.9945 - val_loss: 0.0400 - val_acc: 0.9921\n",
            "Epoch 1665/3500\n",
            "4352/4352 [==============================] - 0s 14us/step - loss: 0.0212 - acc: 0.9945 - val_loss: 0.0401 - val_acc: 0.9921\n",
            "Epoch 1666/3500\n",
            "4352/4352 [==============================] - 0s 13us/step - loss: 0.0204 - acc: 0.9945 - val_loss: 0.0437 - val_acc: 0.9907\n",
            "Epoch 1667/3500\n",
            "4352/4352 [==============================] - 0s 13us/step - loss: 0.0231 - acc: 0.9938 - val_loss: 0.0770 - val_acc: 0.9828\n",
            "Epoch 1668/3500\n",
            "4352/4352 [==============================] - 0s 15us/step - loss: 0.0316 - acc: 0.9903 - val_loss: 0.0692 - val_acc: 0.9865\n",
            "Epoch 1669/3500\n",
            "4352/4352 [==============================] - 0s 14us/step - loss: 0.0251 - acc: 0.9929 - val_loss: 0.0446 - val_acc: 0.9907\n",
            "Epoch 1670/3500\n",
            "4352/4352 [==============================] - 0s 14us/step - loss: 0.0209 - acc: 0.9943 - val_loss: 0.0501 - val_acc: 0.9897\n",
            "Epoch 1671/3500\n",
            "4352/4352 [==============================] - 0s 12us/step - loss: 0.0210 - acc: 0.9947 - val_loss: 0.0469 - val_acc: 0.9911\n",
            "Epoch 1672/3500\n",
            "4352/4352 [==============================] - 0s 13us/step - loss: 0.0210 - acc: 0.9943 - val_loss: 0.0420 - val_acc: 0.9925\n",
            "Epoch 1673/3500\n",
            "4352/4352 [==============================] - 0s 14us/step - loss: 0.0212 - acc: 0.9943 - val_loss: 0.0410 - val_acc: 0.9921\n",
            "Epoch 1674/3500\n",
            "4352/4352 [==============================] - 0s 15us/step - loss: 0.0213 - acc: 0.9945 - val_loss: 0.0401 - val_acc: 0.9921\n",
            "Epoch 1675/3500\n",
            "4352/4352 [==============================] - 0s 15us/step - loss: 0.0210 - acc: 0.9952 - val_loss: 0.0409 - val_acc: 0.9930\n",
            "Epoch 1676/3500\n",
            "4352/4352 [==============================] - 0s 16us/step - loss: 0.0213 - acc: 0.9943 - val_loss: 0.0409 - val_acc: 0.9925\n",
            "Epoch 1677/3500\n",
            "4352/4352 [==============================] - 0s 16us/step - loss: 0.0222 - acc: 0.9947 - val_loss: 0.0503 - val_acc: 0.9893\n",
            "Epoch 1678/3500\n",
            "4352/4352 [==============================] - 0s 15us/step - loss: 0.0213 - acc: 0.9952 - val_loss: 0.0488 - val_acc: 0.9902\n",
            "Epoch 1679/3500\n",
            "4352/4352 [==============================] - 0s 15us/step - loss: 0.0216 - acc: 0.9949 - val_loss: 0.0406 - val_acc: 0.9925\n",
            "Epoch 1680/3500\n",
            "4352/4352 [==============================] - 0s 14us/step - loss: 0.0224 - acc: 0.9943 - val_loss: 0.0403 - val_acc: 0.9930\n",
            "Epoch 1681/3500\n",
            "4352/4352 [==============================] - 0s 15us/step - loss: 0.0224 - acc: 0.9945 - val_loss: 0.0407 - val_acc: 0.9921\n",
            "Epoch 1682/3500\n",
            "4352/4352 [==============================] - 0s 15us/step - loss: 0.0229 - acc: 0.9936 - val_loss: 0.0407 - val_acc: 0.9930\n",
            "Epoch 1683/3500\n",
            "4352/4352 [==============================] - 0s 14us/step - loss: 0.0265 - acc: 0.9917 - val_loss: 0.0417 - val_acc: 0.9925\n",
            "Epoch 1684/3500\n",
            "4352/4352 [==============================] - 0s 14us/step - loss: 0.0249 - acc: 0.9938 - val_loss: 0.0445 - val_acc: 0.9916\n",
            "Epoch 1685/3500\n",
            "4352/4352 [==============================] - 0s 15us/step - loss: 0.0220 - acc: 0.9940 - val_loss: 0.0511 - val_acc: 0.9902\n",
            "Epoch 1686/3500\n",
            "4352/4352 [==============================] - 0s 14us/step - loss: 0.0203 - acc: 0.9945 - val_loss: 0.0462 - val_acc: 0.9911\n",
            "Epoch 1687/3500\n",
            "4352/4352 [==============================] - 0s 14us/step - loss: 0.0217 - acc: 0.9945 - val_loss: 0.0405 - val_acc: 0.9921\n",
            "Epoch 1688/3500\n",
            "4352/4352 [==============================] - 0s 16us/step - loss: 0.0206 - acc: 0.9954 - val_loss: 0.0408 - val_acc: 0.9911\n",
            "Epoch 1689/3500\n",
            "4352/4352 [==============================] - 0s 15us/step - loss: 0.0207 - acc: 0.9949 - val_loss: 0.0432 - val_acc: 0.9911\n",
            "Epoch 1690/3500\n",
            "4352/4352 [==============================] - 0s 14us/step - loss: 0.0201 - acc: 0.9949 - val_loss: 0.0407 - val_acc: 0.9925\n",
            "Epoch 1691/3500\n",
            "4352/4352 [==============================] - 0s 15us/step - loss: 0.0216 - acc: 0.9933 - val_loss: 0.0434 - val_acc: 0.9911\n",
            "Epoch 1692/3500\n",
            "4352/4352 [==============================] - 0s 17us/step - loss: 0.0199 - acc: 0.9945 - val_loss: 0.0518 - val_acc: 0.9897\n",
            "Epoch 1693/3500\n",
            "4352/4352 [==============================] - 0s 14us/step - loss: 0.0210 - acc: 0.9940 - val_loss: 0.0464 - val_acc: 0.9897\n",
            "Epoch 1694/3500\n",
            "4352/4352 [==============================] - 0s 16us/step - loss: 0.0198 - acc: 0.9947 - val_loss: 0.0426 - val_acc: 0.9921\n",
            "Epoch 1695/3500\n",
            "4352/4352 [==============================] - 0s 14us/step - loss: 0.0224 - acc: 0.9938 - val_loss: 0.0444 - val_acc: 0.9911\n",
            "Epoch 1696/3500\n",
            "4352/4352 [==============================] - 0s 16us/step - loss: 0.0195 - acc: 0.9954 - val_loss: 0.0407 - val_acc: 0.9921\n",
            "Epoch 1697/3500\n",
            "4352/4352 [==============================] - 0s 14us/step - loss: 0.0223 - acc: 0.9940 - val_loss: 0.0437 - val_acc: 0.9902\n",
            "Epoch 1698/3500\n",
            "4352/4352 [==============================] - 0s 14us/step - loss: 0.0211 - acc: 0.9943 - val_loss: 0.0556 - val_acc: 0.9888\n",
            "Epoch 1699/3500\n",
            "4352/4352 [==============================] - 0s 14us/step - loss: 0.0205 - acc: 0.9945 - val_loss: 0.0439 - val_acc: 0.9907\n",
            "Epoch 1700/3500\n",
            "4352/4352 [==============================] - 0s 16us/step - loss: 0.0202 - acc: 0.9956 - val_loss: 0.0412 - val_acc: 0.9921\n",
            "Epoch 1701/3500\n",
            "4352/4352 [==============================] - 0s 14us/step - loss: 0.0202 - acc: 0.9947 - val_loss: 0.0552 - val_acc: 0.9883\n",
            "Epoch 1702/3500\n",
            "4352/4352 [==============================] - 0s 15us/step - loss: 0.0218 - acc: 0.9938 - val_loss: 0.0549 - val_acc: 0.9893\n",
            "Epoch 1703/3500\n",
            "4352/4352 [==============================] - 0s 14us/step - loss: 0.0237 - acc: 0.9936 - val_loss: 0.0473 - val_acc: 0.9907\n",
            "Epoch 1704/3500\n",
            "4352/4352 [==============================] - 0s 15us/step - loss: 0.0253 - acc: 0.9917 - val_loss: 0.0431 - val_acc: 0.9930\n",
            "Epoch 1705/3500\n",
            "4352/4352 [==============================] - 0s 15us/step - loss: 0.0225 - acc: 0.9943 - val_loss: 0.0442 - val_acc: 0.9916\n",
            "Epoch 1706/3500\n",
            "4352/4352 [==============================] - 0s 14us/step - loss: 0.0228 - acc: 0.9936 - val_loss: 0.0494 - val_acc: 0.9893\n",
            "Epoch 1707/3500\n",
            "4352/4352 [==============================] - 0s 16us/step - loss: 0.0224 - acc: 0.9947 - val_loss: 0.0492 - val_acc: 0.9893\n",
            "Epoch 1708/3500\n",
            "4352/4352 [==============================] - 0s 15us/step - loss: 0.0213 - acc: 0.9947 - val_loss: 0.0411 - val_acc: 0.9921\n",
            "Epoch 1709/3500\n",
            "4352/4352 [==============================] - 0s 15us/step - loss: 0.0210 - acc: 0.9943 - val_loss: 0.0442 - val_acc: 0.9916\n",
            "Epoch 1710/3500\n",
            "4352/4352 [==============================] - 0s 14us/step - loss: 0.0203 - acc: 0.9943 - val_loss: 0.0436 - val_acc: 0.9921\n",
            "Epoch 1711/3500\n",
            "4352/4352 [==============================] - 0s 14us/step - loss: 0.0196 - acc: 0.9956 - val_loss: 0.0456 - val_acc: 0.9907\n",
            "Epoch 1712/3500\n",
            "4352/4352 [==============================] - 0s 14us/step - loss: 0.0213 - acc: 0.9940 - val_loss: 0.0568 - val_acc: 0.9879\n",
            "Epoch 1713/3500\n",
            "4352/4352 [==============================] - 0s 14us/step - loss: 0.0206 - acc: 0.9949 - val_loss: 0.0426 - val_acc: 0.9921\n",
            "Epoch 1714/3500\n",
            "4352/4352 [==============================] - 0s 14us/step - loss: 0.0203 - acc: 0.9945 - val_loss: 0.0414 - val_acc: 0.9916\n",
            "Epoch 1715/3500\n",
            "4352/4352 [==============================] - 0s 14us/step - loss: 0.0199 - acc: 0.9943 - val_loss: 0.0446 - val_acc: 0.9911\n",
            "Epoch 1716/3500\n",
            "4352/4352 [==============================] - 0s 15us/step - loss: 0.0200 - acc: 0.9947 - val_loss: 0.0492 - val_acc: 0.9907\n",
            "Epoch 1717/3500\n",
            "4352/4352 [==============================] - 0s 15us/step - loss: 0.0196 - acc: 0.9949 - val_loss: 0.0474 - val_acc: 0.9907\n",
            "Epoch 1718/3500\n",
            "4352/4352 [==============================] - 0s 15us/step - loss: 0.0192 - acc: 0.9954 - val_loss: 0.0421 - val_acc: 0.9916\n",
            "Epoch 1719/3500\n",
            "4352/4352 [==============================] - 0s 14us/step - loss: 0.0197 - acc: 0.9949 - val_loss: 0.0438 - val_acc: 0.9921\n",
            "Epoch 1720/3500\n",
            "4352/4352 [==============================] - 0s 16us/step - loss: 0.0205 - acc: 0.9945 - val_loss: 0.0437 - val_acc: 0.9921\n",
            "Epoch 1721/3500\n",
            "4352/4352 [==============================] - 0s 15us/step - loss: 0.0198 - acc: 0.9952 - val_loss: 0.0424 - val_acc: 0.9911\n",
            "Epoch 1722/3500\n",
            "4352/4352 [==============================] - 0s 14us/step - loss: 0.0195 - acc: 0.9945 - val_loss: 0.0452 - val_acc: 0.9921\n",
            "Epoch 1723/3500\n",
            "4352/4352 [==============================] - 0s 19us/step - loss: 0.0195 - acc: 0.9954 - val_loss: 0.0498 - val_acc: 0.9907\n",
            "Epoch 1724/3500\n",
            "4352/4352 [==============================] - 0s 15us/step - loss: 0.0196 - acc: 0.9945 - val_loss: 0.0437 - val_acc: 0.9916\n",
            "Epoch 1725/3500\n",
            "4352/4352 [==============================] - 0s 16us/step - loss: 0.0195 - acc: 0.9947 - val_loss: 0.0429 - val_acc: 0.9902\n",
            "Epoch 1726/3500\n",
            "4352/4352 [==============================] - 0s 14us/step - loss: 0.0197 - acc: 0.9949 - val_loss: 0.0441 - val_acc: 0.9916\n",
            "Epoch 1727/3500\n",
            "4352/4352 [==============================] - 0s 15us/step - loss: 0.0199 - acc: 0.9949 - val_loss: 0.0436 - val_acc: 0.9916\n",
            "Epoch 1728/3500\n",
            "4352/4352 [==============================] - 0s 14us/step - loss: 0.0205 - acc: 0.9945 - val_loss: 0.0448 - val_acc: 0.9921\n",
            "Epoch 1729/3500\n",
            "4352/4352 [==============================] - 0s 13us/step - loss: 0.0195 - acc: 0.9949 - val_loss: 0.0443 - val_acc: 0.9921\n",
            "Epoch 1730/3500\n",
            "4352/4352 [==============================] - 0s 14us/step - loss: 0.0196 - acc: 0.9949 - val_loss: 0.0501 - val_acc: 0.9907\n",
            "Epoch 1731/3500\n",
            "4352/4352 [==============================] - 0s 14us/step - loss: 0.0192 - acc: 0.9952 - val_loss: 0.0469 - val_acc: 0.9911\n",
            "Epoch 1732/3500\n",
            "4352/4352 [==============================] - 0s 13us/step - loss: 0.0191 - acc: 0.9954 - val_loss: 0.0526 - val_acc: 0.9902\n",
            "Epoch 1733/3500\n",
            "4352/4352 [==============================] - 0s 14us/step - loss: 0.0206 - acc: 0.9943 - val_loss: 0.0485 - val_acc: 0.9911\n",
            "Epoch 1734/3500\n",
            "4352/4352 [==============================] - 0s 14us/step - loss: 0.0194 - acc: 0.9949 - val_loss: 0.0475 - val_acc: 0.9907\n",
            "Epoch 1735/3500\n",
            "4352/4352 [==============================] - 0s 13us/step - loss: 0.0195 - acc: 0.9952 - val_loss: 0.0484 - val_acc: 0.9921\n",
            "Epoch 1736/3500\n",
            "4352/4352 [==============================] - 0s 13us/step - loss: 0.0196 - acc: 0.9947 - val_loss: 0.0456 - val_acc: 0.9921\n",
            "Epoch 1737/3500\n",
            "4352/4352 [==============================] - 0s 14us/step - loss: 0.0200 - acc: 0.9947 - val_loss: 0.0613 - val_acc: 0.9883\n",
            "Epoch 1738/3500\n",
            "4352/4352 [==============================] - 0s 13us/step - loss: 0.0212 - acc: 0.9938 - val_loss: 0.0543 - val_acc: 0.9893\n",
            "Epoch 1739/3500\n",
            "4352/4352 [==============================] - 0s 16us/step - loss: 0.0201 - acc: 0.9954 - val_loss: 0.0467 - val_acc: 0.9921\n",
            "Epoch 1740/3500\n",
            "4352/4352 [==============================] - 0s 14us/step - loss: 0.0214 - acc: 0.9943 - val_loss: 0.0505 - val_acc: 0.9897\n",
            "Epoch 1741/3500\n",
            "4352/4352 [==============================] - 0s 18us/step - loss: 0.0209 - acc: 0.9936 - val_loss: 0.0491 - val_acc: 0.9916\n",
            "Epoch 1742/3500\n",
            "4352/4352 [==============================] - 0s 13us/step - loss: 0.0201 - acc: 0.9949 - val_loss: 0.0487 - val_acc: 0.9911\n",
            "Epoch 1743/3500\n",
            "4352/4352 [==============================] - 0s 14us/step - loss: 0.0209 - acc: 0.9943 - val_loss: 0.0533 - val_acc: 0.9893\n",
            "Epoch 1744/3500\n",
            "4352/4352 [==============================] - 0s 14us/step - loss: 0.0212 - acc: 0.9940 - val_loss: 0.0459 - val_acc: 0.9916\n",
            "Epoch 1745/3500\n",
            "4352/4352 [==============================] - 0s 14us/step - loss: 0.0208 - acc: 0.9945 - val_loss: 0.0449 - val_acc: 0.9916\n",
            "Epoch 1746/3500\n",
            "4352/4352 [==============================] - 0s 13us/step - loss: 0.0193 - acc: 0.9949 - val_loss: 0.0468 - val_acc: 0.9916\n",
            "Epoch 1747/3500\n",
            "4352/4352 [==============================] - 0s 13us/step - loss: 0.0189 - acc: 0.9954 - val_loss: 0.0458 - val_acc: 0.9907\n",
            "Epoch 1748/3500\n",
            "4352/4352 [==============================] - 0s 14us/step - loss: 0.0210 - acc: 0.9952 - val_loss: 0.0444 - val_acc: 0.9911\n",
            "Epoch 1749/3500\n",
            "4352/4352 [==============================] - 0s 14us/step - loss: 0.0198 - acc: 0.9954 - val_loss: 0.0469 - val_acc: 0.9916\n",
            "Epoch 1750/3500\n",
            "4352/4352 [==============================] - 0s 14us/step - loss: 0.0192 - acc: 0.9956 - val_loss: 0.0569 - val_acc: 0.9888\n",
            "Epoch 1751/3500\n",
            "4352/4352 [==============================] - 0s 15us/step - loss: 0.0195 - acc: 0.9954 - val_loss: 0.0472 - val_acc: 0.9921\n",
            "Epoch 1752/3500\n",
            "4352/4352 [==============================] - 0s 15us/step - loss: 0.0191 - acc: 0.9947 - val_loss: 0.0439 - val_acc: 0.9907\n",
            "Epoch 1753/3500\n",
            "4352/4352 [==============================] - 0s 13us/step - loss: 0.0210 - acc: 0.9943 - val_loss: 0.0448 - val_acc: 0.9921\n",
            "Epoch 1754/3500\n",
            "4352/4352 [==============================] - 0s 15us/step - loss: 0.0189 - acc: 0.9954 - val_loss: 0.0470 - val_acc: 0.9921\n",
            "Epoch 1755/3500\n",
            "4352/4352 [==============================] - 0s 17us/step - loss: 0.0194 - acc: 0.9949 - val_loss: 0.0473 - val_acc: 0.9921\n",
            "Epoch 1756/3500\n",
            "4352/4352 [==============================] - 0s 14us/step - loss: 0.0203 - acc: 0.9945 - val_loss: 0.0468 - val_acc: 0.9921\n",
            "Epoch 1757/3500\n",
            "4352/4352 [==============================] - 0s 14us/step - loss: 0.0201 - acc: 0.9947 - val_loss: 0.0457 - val_acc: 0.9916\n",
            "Epoch 1758/3500\n",
            "4352/4352 [==============================] - 0s 13us/step - loss: 0.0195 - acc: 0.9949 - val_loss: 0.0492 - val_acc: 0.9907\n",
            "Epoch 1759/3500\n",
            "4352/4352 [==============================] - 0s 13us/step - loss: 0.0204 - acc: 0.9945 - val_loss: 0.0534 - val_acc: 0.9907\n",
            "Epoch 1760/3500\n",
            "4352/4352 [==============================] - 0s 13us/step - loss: 0.0199 - acc: 0.9949 - val_loss: 0.0511 - val_acc: 0.9911\n",
            "Epoch 1761/3500\n",
            "4352/4352 [==============================] - 0s 15us/step - loss: 0.0200 - acc: 0.9952 - val_loss: 0.0553 - val_acc: 0.9902\n",
            "Epoch 1762/3500\n",
            "4352/4352 [==============================] - 0s 14us/step - loss: 0.0209 - acc: 0.9945 - val_loss: 0.0458 - val_acc: 0.9921\n",
            "Epoch 1763/3500\n",
            "4352/4352 [==============================] - 0s 13us/step - loss: 0.0199 - acc: 0.9949 - val_loss: 0.0470 - val_acc: 0.9921\n",
            "Epoch 1764/3500\n",
            "4352/4352 [==============================] - 0s 14us/step - loss: 0.0194 - acc: 0.9954 - val_loss: 0.0540 - val_acc: 0.9902\n",
            "Epoch 1765/3500\n",
            "4352/4352 [==============================] - 0s 14us/step - loss: 0.0202 - acc: 0.9943 - val_loss: 0.0538 - val_acc: 0.9902\n",
            "Epoch 1766/3500\n",
            "4352/4352 [==============================] - 0s 14us/step - loss: 0.0195 - acc: 0.9954 - val_loss: 0.0588 - val_acc: 0.9888\n",
            "Epoch 1767/3500\n",
            "4352/4352 [==============================] - 0s 14us/step - loss: 0.0216 - acc: 0.9945 - val_loss: 0.0571 - val_acc: 0.9897\n",
            "Epoch 1768/3500\n",
            "4352/4352 [==============================] - 0s 14us/step - loss: 0.0210 - acc: 0.9943 - val_loss: 0.0559 - val_acc: 0.9893\n",
            "Epoch 1769/3500\n",
            "4352/4352 [==============================] - 0s 14us/step - loss: 0.0205 - acc: 0.9949 - val_loss: 0.0519 - val_acc: 0.9911\n",
            "Epoch 1770/3500\n",
            "4352/4352 [==============================] - 0s 14us/step - loss: 0.0191 - acc: 0.9954 - val_loss: 0.0479 - val_acc: 0.9911\n",
            "Epoch 1771/3500\n",
            "4352/4352 [==============================] - 0s 21us/step - loss: 0.0219 - acc: 0.9949 - val_loss: 0.0539 - val_acc: 0.9907\n",
            "Epoch 1772/3500\n",
            "4352/4352 [==============================] - 0s 14us/step - loss: 0.0213 - acc: 0.9949 - val_loss: 0.0495 - val_acc: 0.9916\n",
            "Epoch 1773/3500\n",
            "4352/4352 [==============================] - 0s 14us/step - loss: 0.0205 - acc: 0.9949 - val_loss: 0.0455 - val_acc: 0.9921\n",
            "Epoch 1774/3500\n",
            "4352/4352 [==============================] - 0s 14us/step - loss: 0.0208 - acc: 0.9952 - val_loss: 0.0501 - val_acc: 0.9907\n",
            "Epoch 1775/3500\n",
            "4352/4352 [==============================] - 0s 14us/step - loss: 0.0198 - acc: 0.9947 - val_loss: 0.0452 - val_acc: 0.9916\n",
            "Epoch 1776/3500\n",
            "4352/4352 [==============================] - 0s 14us/step - loss: 0.0241 - acc: 0.9922 - val_loss: 0.0464 - val_acc: 0.9916\n",
            "Epoch 1777/3500\n",
            "4352/4352 [==============================] - 0s 13us/step - loss: 0.0232 - acc: 0.9936 - val_loss: 0.0461 - val_acc: 0.9916\n",
            "Epoch 1778/3500\n",
            "4352/4352 [==============================] - 0s 15us/step - loss: 0.0231 - acc: 0.9926 - val_loss: 0.0462 - val_acc: 0.9916\n",
            "Epoch 1779/3500\n",
            "4352/4352 [==============================] - 0s 14us/step - loss: 0.0194 - acc: 0.9949 - val_loss: 0.0470 - val_acc: 0.9925\n",
            "Epoch 1780/3500\n",
            "4352/4352 [==============================] - 0s 16us/step - loss: 0.0200 - acc: 0.9954 - val_loss: 0.0466 - val_acc: 0.9911\n",
            "Epoch 1781/3500\n",
            "4352/4352 [==============================] - 0s 15us/step - loss: 0.0220 - acc: 0.9929 - val_loss: 0.0460 - val_acc: 0.9907\n",
            "Epoch 1782/3500\n",
            "4352/4352 [==============================] - 0s 15us/step - loss: 0.0200 - acc: 0.9947 - val_loss: 0.0473 - val_acc: 0.9921\n",
            "Epoch 1783/3500\n",
            "4352/4352 [==============================] - 0s 13us/step - loss: 0.0235 - acc: 0.9924 - val_loss: 0.0462 - val_acc: 0.9921\n",
            "Epoch 1784/3500\n",
            "4352/4352 [==============================] - 0s 15us/step - loss: 0.0204 - acc: 0.9945 - val_loss: 0.0503 - val_acc: 0.9916\n",
            "Epoch 1785/3500\n",
            "4352/4352 [==============================] - 0s 15us/step - loss: 0.0200 - acc: 0.9947 - val_loss: 0.0475 - val_acc: 0.9916\n",
            "Epoch 1786/3500\n",
            "4352/4352 [==============================] - 0s 15us/step - loss: 0.0193 - acc: 0.9952 - val_loss: 0.0478 - val_acc: 0.9911\n",
            "Epoch 1787/3500\n",
            "4352/4352 [==============================] - 0s 16us/step - loss: 0.0197 - acc: 0.9945 - val_loss: 0.0479 - val_acc: 0.9916\n",
            "Epoch 1788/3500\n",
            "4352/4352 [==============================] - 0s 15us/step - loss: 0.0187 - acc: 0.9954 - val_loss: 0.0480 - val_acc: 0.9911\n",
            "Epoch 1789/3500\n",
            "4352/4352 [==============================] - 0s 15us/step - loss: 0.0189 - acc: 0.9956 - val_loss: 0.0536 - val_acc: 0.9911\n",
            "Epoch 1790/3500\n",
            "4352/4352 [==============================] - 0s 13us/step - loss: 0.0193 - acc: 0.9947 - val_loss: 0.0561 - val_acc: 0.9902\n",
            "Epoch 1791/3500\n",
            "4352/4352 [==============================] - 0s 15us/step - loss: 0.0188 - acc: 0.9956 - val_loss: 0.0464 - val_acc: 0.9911\n",
            "Epoch 1792/3500\n",
            "4352/4352 [==============================] - 0s 15us/step - loss: 0.0207 - acc: 0.9947 - val_loss: 0.0496 - val_acc: 0.9916\n",
            "Epoch 1793/3500\n",
            "4352/4352 [==============================] - 0s 13us/step - loss: 0.0203 - acc: 0.9952 - val_loss: 0.0479 - val_acc: 0.9916\n",
            "Epoch 1794/3500\n",
            "4352/4352 [==============================] - 0s 14us/step - loss: 0.0204 - acc: 0.9954 - val_loss: 0.0662 - val_acc: 0.9874\n",
            "Epoch 1795/3500\n",
            "4352/4352 [==============================] - 0s 14us/step - loss: 0.0213 - acc: 0.9947 - val_loss: 0.0539 - val_acc: 0.9911\n",
            "Epoch 1796/3500\n",
            "4352/4352 [==============================] - 0s 14us/step - loss: 0.0195 - acc: 0.9936 - val_loss: 0.0518 - val_acc: 0.9911\n",
            "Epoch 1797/3500\n",
            "4352/4352 [==============================] - 0s 14us/step - loss: 0.0203 - acc: 0.9952 - val_loss: 0.0496 - val_acc: 0.9916\n",
            "Epoch 1798/3500\n",
            "4352/4352 [==============================] - 0s 15us/step - loss: 0.0182 - acc: 0.9952 - val_loss: 0.0466 - val_acc: 0.9921\n",
            "Epoch 1799/3500\n",
            "4352/4352 [==============================] - 0s 14us/step - loss: 0.0214 - acc: 0.9940 - val_loss: 0.0526 - val_acc: 0.9907\n",
            "Epoch 1800/3500\n",
            "4352/4352 [==============================] - 0s 14us/step - loss: 0.0215 - acc: 0.9933 - val_loss: 0.0587 - val_acc: 0.9893\n",
            "Epoch 1801/3500\n",
            "4352/4352 [==============================] - 0s 14us/step - loss: 0.0214 - acc: 0.9940 - val_loss: 0.0608 - val_acc: 0.9883\n",
            "Epoch 1802/3500\n",
            "4352/4352 [==============================] - 0s 14us/step - loss: 0.0196 - acc: 0.9954 - val_loss: 0.0509 - val_acc: 0.9916\n",
            "Epoch 1803/3500\n",
            "4352/4352 [==============================] - 0s 15us/step - loss: 0.0196 - acc: 0.9947 - val_loss: 0.0609 - val_acc: 0.9897\n",
            "Epoch 1804/3500\n",
            "4352/4352 [==============================] - 0s 16us/step - loss: 0.0205 - acc: 0.9940 - val_loss: 0.0616 - val_acc: 0.9883\n",
            "Epoch 1805/3500\n",
            "4352/4352 [==============================] - 0s 15us/step - loss: 0.0216 - acc: 0.9938 - val_loss: 0.0538 - val_acc: 0.9897\n",
            "Epoch 1806/3500\n",
            "4352/4352 [==============================] - 0s 15us/step - loss: 0.0200 - acc: 0.9952 - val_loss: 0.0479 - val_acc: 0.9911\n",
            "Epoch 1807/3500\n",
            "4352/4352 [==============================] - 0s 14us/step - loss: 0.0214 - acc: 0.9940 - val_loss: 0.0505 - val_acc: 0.9921\n",
            "Epoch 1808/3500\n",
            "4352/4352 [==============================] - 0s 15us/step - loss: 0.0188 - acc: 0.9949 - val_loss: 0.0534 - val_acc: 0.9907\n",
            "Epoch 1809/3500\n",
            "4352/4352 [==============================] - 0s 15us/step - loss: 0.0188 - acc: 0.9954 - val_loss: 0.0582 - val_acc: 0.9897\n",
            "Epoch 1810/3500\n",
            "4352/4352 [==============================] - 0s 14us/step - loss: 0.0195 - acc: 0.9954 - val_loss: 0.0558 - val_acc: 0.9897\n",
            "Epoch 1811/3500\n",
            "4352/4352 [==============================] - 0s 14us/step - loss: 0.0195 - acc: 0.9954 - val_loss: 0.0587 - val_acc: 0.9893\n",
            "Epoch 1812/3500\n",
            "4352/4352 [==============================] - 0s 14us/step - loss: 0.0205 - acc: 0.9943 - val_loss: 0.0614 - val_acc: 0.9888\n",
            "Epoch 1813/3500\n",
            "4352/4352 [==============================] - 0s 14us/step - loss: 0.0204 - acc: 0.9947 - val_loss: 0.0562 - val_acc: 0.9902\n",
            "Epoch 1814/3500\n",
            "4352/4352 [==============================] - 0s 13us/step - loss: 0.0200 - acc: 0.9940 - val_loss: 0.0514 - val_acc: 0.9911\n",
            "Epoch 1815/3500\n",
            "4352/4352 [==============================] - 0s 13us/step - loss: 0.0187 - acc: 0.9956 - val_loss: 0.0514 - val_acc: 0.9911\n",
            "Epoch 1816/3500\n",
            "4352/4352 [==============================] - 0s 16us/step - loss: 0.0188 - acc: 0.9954 - val_loss: 0.0527 - val_acc: 0.9916\n",
            "Epoch 1817/3500\n",
            "4352/4352 [==============================] - 0s 14us/step - loss: 0.0184 - acc: 0.9956 - val_loss: 0.0488 - val_acc: 0.9921\n",
            "Epoch 1818/3500\n",
            "4352/4352 [==============================] - 0s 14us/step - loss: 0.0187 - acc: 0.9954 - val_loss: 0.0656 - val_acc: 0.9883\n",
            "Epoch 1819/3500\n",
            "4352/4352 [==============================] - 0s 17us/step - loss: 0.0221 - acc: 0.9938 - val_loss: 0.0616 - val_acc: 0.9883\n",
            "Epoch 1820/3500\n",
            "4352/4352 [==============================] - 0s 15us/step - loss: 0.0218 - acc: 0.9943 - val_loss: 0.0515 - val_acc: 0.9916\n",
            "Epoch 1821/3500\n",
            "4352/4352 [==============================] - 0s 15us/step - loss: 0.0209 - acc: 0.9947 - val_loss: 0.0471 - val_acc: 0.9916\n",
            "Epoch 1822/3500\n",
            "4352/4352 [==============================] - 0s 14us/step - loss: 0.0204 - acc: 0.9943 - val_loss: 0.0489 - val_acc: 0.9911\n",
            "Epoch 1823/3500\n",
            "4352/4352 [==============================] - 0s 14us/step - loss: 0.0204 - acc: 0.9952 - val_loss: 0.0523 - val_acc: 0.9902\n",
            "Epoch 1824/3500\n",
            "4352/4352 [==============================] - 0s 15us/step - loss: 0.0197 - acc: 0.9947 - val_loss: 0.0479 - val_acc: 0.9911\n",
            "Epoch 1825/3500\n",
            "4352/4352 [==============================] - 0s 15us/step - loss: 0.0204 - acc: 0.9949 - val_loss: 0.0530 - val_acc: 0.9911\n",
            "Epoch 1826/3500\n",
            "4352/4352 [==============================] - 0s 14us/step - loss: 0.0189 - acc: 0.9945 - val_loss: 0.0487 - val_acc: 0.9916\n",
            "Epoch 1827/3500\n",
            "4352/4352 [==============================] - 0s 15us/step - loss: 0.0196 - acc: 0.9949 - val_loss: 0.0518 - val_acc: 0.9907\n",
            "Epoch 1828/3500\n",
            "4352/4352 [==============================] - 0s 15us/step - loss: 0.0197 - acc: 0.9949 - val_loss: 0.0562 - val_acc: 0.9902\n",
            "Epoch 1829/3500\n",
            "4352/4352 [==============================] - 0s 15us/step - loss: 0.0207 - acc: 0.9940 - val_loss: 0.0474 - val_acc: 0.9925\n",
            "Epoch 1830/3500\n",
            "4352/4352 [==============================] - 0s 14us/step - loss: 0.0215 - acc: 0.9936 - val_loss: 0.0479 - val_acc: 0.9921\n",
            "Epoch 1831/3500\n",
            "4352/4352 [==============================] - 0s 14us/step - loss: 0.0215 - acc: 0.9929 - val_loss: 0.0490 - val_acc: 0.9921\n",
            "Epoch 1832/3500\n",
            "4352/4352 [==============================] - 0s 14us/step - loss: 0.0200 - acc: 0.9940 - val_loss: 0.0500 - val_acc: 0.9921\n",
            "Epoch 1833/3500\n",
            "4352/4352 [==============================] - 0s 15us/step - loss: 0.0192 - acc: 0.9959 - val_loss: 0.0528 - val_acc: 0.9911\n",
            "Epoch 1834/3500\n",
            "4352/4352 [==============================] - 0s 17us/step - loss: 0.0186 - acc: 0.9952 - val_loss: 0.0548 - val_acc: 0.9911\n",
            "Epoch 1835/3500\n",
            "4352/4352 [==============================] - 0s 14us/step - loss: 0.0196 - acc: 0.9949 - val_loss: 0.0549 - val_acc: 0.9902\n",
            "Epoch 1836/3500\n",
            "4352/4352 [==============================] - 0s 14us/step - loss: 0.0190 - acc: 0.9949 - val_loss: 0.0551 - val_acc: 0.9897\n",
            "Epoch 1837/3500\n",
            "4352/4352 [==============================] - 0s 14us/step - loss: 0.0191 - acc: 0.9952 - val_loss: 0.0479 - val_acc: 0.9907\n",
            "Epoch 1838/3500\n",
            "4352/4352 [==============================] - 0s 15us/step - loss: 0.0191 - acc: 0.9952 - val_loss: 0.0539 - val_acc: 0.9916\n",
            "Epoch 1839/3500\n",
            "4352/4352 [==============================] - 0s 15us/step - loss: 0.0182 - acc: 0.9956 - val_loss: 0.0493 - val_acc: 0.9907\n",
            "Epoch 1840/3500\n",
            "4352/4352 [==============================] - 0s 14us/step - loss: 0.0192 - acc: 0.9956 - val_loss: 0.0482 - val_acc: 0.9911\n",
            "Epoch 1841/3500\n",
            "4352/4352 [==============================] - 0s 15us/step - loss: 0.0201 - acc: 0.9947 - val_loss: 0.0508 - val_acc: 0.9916\n",
            "Epoch 1842/3500\n",
            "4352/4352 [==============================] - 0s 14us/step - loss: 0.0195 - acc: 0.9945 - val_loss: 0.0526 - val_acc: 0.9907\n",
            "Epoch 1843/3500\n",
            "4352/4352 [==============================] - 0s 15us/step - loss: 0.0184 - acc: 0.9956 - val_loss: 0.0515 - val_acc: 0.9921\n",
            "Epoch 1844/3500\n",
            "4352/4352 [==============================] - 0s 14us/step - loss: 0.0183 - acc: 0.9949 - val_loss: 0.0504 - val_acc: 0.9911\n",
            "Epoch 1845/3500\n",
            "4352/4352 [==============================] - 0s 15us/step - loss: 0.0187 - acc: 0.9956 - val_loss: 0.0497 - val_acc: 0.9916\n",
            "Epoch 1846/3500\n",
            "4352/4352 [==============================] - 0s 14us/step - loss: 0.0190 - acc: 0.9956 - val_loss: 0.0564 - val_acc: 0.9907\n",
            "Epoch 1847/3500\n",
            "4352/4352 [==============================] - 0s 14us/step - loss: 0.0194 - acc: 0.9949 - val_loss: 0.0541 - val_acc: 0.9911\n",
            "Epoch 1848/3500\n",
            "4352/4352 [==============================] - 0s 14us/step - loss: 0.0203 - acc: 0.9945 - val_loss: 0.0563 - val_acc: 0.9897\n",
            "Epoch 1849/3500\n",
            "4352/4352 [==============================] - 0s 14us/step - loss: 0.0211 - acc: 0.9947 - val_loss: 0.0640 - val_acc: 0.9893\n",
            "Epoch 1850/3500\n",
            "4352/4352 [==============================] - 0s 16us/step - loss: 0.0216 - acc: 0.9936 - val_loss: 0.0658 - val_acc: 0.9879\n",
            "Epoch 1851/3500\n",
            "4352/4352 [==============================] - 0s 14us/step - loss: 0.0190 - acc: 0.9947 - val_loss: 0.0519 - val_acc: 0.9921\n",
            "Epoch 1852/3500\n",
            "4352/4352 [==============================] - 0s 14us/step - loss: 0.0187 - acc: 0.9952 - val_loss: 0.0499 - val_acc: 0.9916\n",
            "Epoch 1853/3500\n",
            "4352/4352 [==============================] - 0s 15us/step - loss: 0.0184 - acc: 0.9952 - val_loss: 0.0527 - val_acc: 0.9916\n",
            "Epoch 1854/3500\n",
            "4352/4352 [==============================] - 0s 14us/step - loss: 0.0189 - acc: 0.9956 - val_loss: 0.0496 - val_acc: 0.9902\n",
            "Epoch 1855/3500\n",
            "4352/4352 [==============================] - 0s 14us/step - loss: 0.0204 - acc: 0.9940 - val_loss: 0.0482 - val_acc: 0.9921\n",
            "Epoch 1856/3500\n",
            "4352/4352 [==============================] - 0s 14us/step - loss: 0.0184 - acc: 0.9952 - val_loss: 0.0584 - val_acc: 0.9897\n",
            "Epoch 1857/3500\n",
            "4352/4352 [==============================] - 0s 14us/step - loss: 0.0222 - acc: 0.9940 - val_loss: 0.0697 - val_acc: 0.9879\n",
            "Epoch 1858/3500\n",
            "4352/4352 [==============================] - 0s 15us/step - loss: 0.0207 - acc: 0.9947 - val_loss: 0.0615 - val_acc: 0.9888\n",
            "Epoch 1859/3500\n",
            "4352/4352 [==============================] - 0s 14us/step - loss: 0.0201 - acc: 0.9954 - val_loss: 0.0549 - val_acc: 0.9911\n",
            "Epoch 1860/3500\n",
            "4352/4352 [==============================] - 0s 15us/step - loss: 0.0190 - acc: 0.9949 - val_loss: 0.0586 - val_acc: 0.9897\n",
            "Epoch 1861/3500\n",
            "4352/4352 [==============================] - 0s 15us/step - loss: 0.0195 - acc: 0.9949 - val_loss: 0.0580 - val_acc: 0.9911\n",
            "Epoch 1862/3500\n",
            "4352/4352 [==============================] - 0s 15us/step - loss: 0.0192 - acc: 0.9947 - val_loss: 0.0542 - val_acc: 0.9907\n",
            "Epoch 1863/3500\n",
            "4352/4352 [==============================] - 0s 14us/step - loss: 0.0208 - acc: 0.9945 - val_loss: 0.0509 - val_acc: 0.9916\n",
            "Epoch 1864/3500\n",
            "4352/4352 [==============================] - 0s 15us/step - loss: 0.0203 - acc: 0.9936 - val_loss: 0.0593 - val_acc: 0.9902\n",
            "Epoch 1865/3500\n",
            "4352/4352 [==============================] - 0s 19us/step - loss: 0.0190 - acc: 0.9949 - val_loss: 0.0523 - val_acc: 0.9921\n",
            "Epoch 1866/3500\n",
            "4352/4352 [==============================] - 0s 13us/step - loss: 0.0186 - acc: 0.9956 - val_loss: 0.0550 - val_acc: 0.9916\n",
            "Epoch 1867/3500\n",
            "4352/4352 [==============================] - 0s 13us/step - loss: 0.0193 - acc: 0.9949 - val_loss: 0.0597 - val_acc: 0.9902\n",
            "Epoch 1868/3500\n",
            "4352/4352 [==============================] - 0s 13us/step - loss: 0.0189 - acc: 0.9949 - val_loss: 0.0562 - val_acc: 0.9907\n",
            "Epoch 1869/3500\n",
            "4352/4352 [==============================] - 0s 13us/step - loss: 0.0186 - acc: 0.9949 - val_loss: 0.0527 - val_acc: 0.9916\n",
            "Epoch 1870/3500\n",
            "4352/4352 [==============================] - 0s 14us/step - loss: 0.0185 - acc: 0.9952 - val_loss: 0.0487 - val_acc: 0.9916\n",
            "Epoch 1871/3500\n",
            "4352/4352 [==============================] - 0s 15us/step - loss: 0.0201 - acc: 0.9947 - val_loss: 0.0499 - val_acc: 0.9907\n",
            "Epoch 1872/3500\n",
            "4352/4352 [==============================] - 0s 17us/step - loss: 0.0205 - acc: 0.9938 - val_loss: 0.0672 - val_acc: 0.9860\n",
            "Epoch 1873/3500\n",
            "4352/4352 [==============================] - 0s 14us/step - loss: 0.0328 - acc: 0.9906 - val_loss: 0.0514 - val_acc: 0.9911\n",
            "Epoch 1874/3500\n",
            "4352/4352 [==============================] - 0s 14us/step - loss: 0.0229 - acc: 0.9943 - val_loss: 0.0502 - val_acc: 0.9921\n",
            "Epoch 1875/3500\n",
            "4352/4352 [==============================] - 0s 14us/step - loss: 0.0225 - acc: 0.9933 - val_loss: 0.0512 - val_acc: 0.9907\n",
            "Epoch 1876/3500\n",
            "4352/4352 [==============================] - 0s 15us/step - loss: 0.0184 - acc: 0.9956 - val_loss: 0.0521 - val_acc: 0.9921\n",
            "Epoch 1877/3500\n",
            "4352/4352 [==============================] - 0s 15us/step - loss: 0.0187 - acc: 0.9949 - val_loss: 0.0514 - val_acc: 0.9921\n",
            "Epoch 1878/3500\n",
            "4352/4352 [==============================] - 0s 16us/step - loss: 0.0183 - acc: 0.9954 - val_loss: 0.0492 - val_acc: 0.9921\n",
            "Epoch 1879/3500\n",
            "4352/4352 [==============================] - 0s 13us/step - loss: 0.0187 - acc: 0.9956 - val_loss: 0.0516 - val_acc: 0.9921\n",
            "Epoch 1880/3500\n",
            "4352/4352 [==============================] - 0s 14us/step - loss: 0.0185 - acc: 0.9952 - val_loss: 0.0602 - val_acc: 0.9897\n",
            "Epoch 1881/3500\n",
            "4352/4352 [==============================] - 0s 16us/step - loss: 0.0212 - acc: 0.9940 - val_loss: 0.0651 - val_acc: 0.9879\n",
            "Epoch 1882/3500\n",
            "4352/4352 [==============================] - 0s 15us/step - loss: 0.0204 - acc: 0.9952 - val_loss: 0.0708 - val_acc: 0.9869\n",
            "Epoch 1883/3500\n",
            "4352/4352 [==============================] - 0s 15us/step - loss: 0.0216 - acc: 0.9938 - val_loss: 0.0559 - val_acc: 0.9911\n",
            "Epoch 1884/3500\n",
            "4352/4352 [==============================] - 0s 14us/step - loss: 0.0195 - acc: 0.9952 - val_loss: 0.0526 - val_acc: 0.9916\n",
            "Epoch 1885/3500\n",
            "4352/4352 [==============================] - 0s 16us/step - loss: 0.0191 - acc: 0.9952 - val_loss: 0.0551 - val_acc: 0.9907\n",
            "Epoch 1886/3500\n",
            "4352/4352 [==============================] - 0s 15us/step - loss: 0.0183 - acc: 0.9956 - val_loss: 0.0583 - val_acc: 0.9907\n",
            "Epoch 1887/3500\n",
            "4352/4352 [==============================] - 0s 15us/step - loss: 0.0189 - acc: 0.9954 - val_loss: 0.0550 - val_acc: 0.9907\n",
            "Epoch 1888/3500\n",
            "4352/4352 [==============================] - 0s 15us/step - loss: 0.0183 - acc: 0.9952 - val_loss: 0.0565 - val_acc: 0.9911\n",
            "Epoch 1889/3500\n",
            "4352/4352 [==============================] - 0s 14us/step - loss: 0.0197 - acc: 0.9945 - val_loss: 0.0531 - val_acc: 0.9916\n",
            "Epoch 1890/3500\n",
            "4352/4352 [==============================] - 0s 15us/step - loss: 0.0186 - acc: 0.9954 - val_loss: 0.0521 - val_acc: 0.9921\n",
            "Epoch 1891/3500\n",
            "4352/4352 [==============================] - 0s 15us/step - loss: 0.0219 - acc: 0.9943 - val_loss: 0.0487 - val_acc: 0.9911\n",
            "Epoch 1892/3500\n",
            "4352/4352 [==============================] - 0s 14us/step - loss: 0.0215 - acc: 0.9936 - val_loss: 0.0510 - val_acc: 0.9916\n",
            "Epoch 1893/3500\n",
            "4352/4352 [==============================] - 0s 15us/step - loss: 0.0262 - acc: 0.9924 - val_loss: 0.0507 - val_acc: 0.9907\n",
            "Epoch 1894/3500\n",
            "4352/4352 [==============================] - 0s 16us/step - loss: 0.0256 - acc: 0.9940 - val_loss: 0.0485 - val_acc: 0.9916\n",
            "Epoch 1895/3500\n",
            "4352/4352 [==============================] - 0s 15us/step - loss: 0.0219 - acc: 0.9945 - val_loss: 0.0491 - val_acc: 0.9911\n",
            "Epoch 1896/3500\n",
            "4352/4352 [==============================] - 0s 16us/step - loss: 0.0189 - acc: 0.9943 - val_loss: 0.0502 - val_acc: 0.9921\n",
            "Epoch 1897/3500\n",
            "4352/4352 [==============================] - 0s 16us/step - loss: 0.0188 - acc: 0.9952 - val_loss: 0.0483 - val_acc: 0.9921\n",
            "Epoch 1898/3500\n",
            "4352/4352 [==============================] - 0s 13us/step - loss: 0.0194 - acc: 0.9954 - val_loss: 0.0488 - val_acc: 0.9907\n",
            "Epoch 1899/3500\n",
            "4352/4352 [==============================] - 0s 14us/step - loss: 0.0207 - acc: 0.9949 - val_loss: 0.0484 - val_acc: 0.9911\n",
            "Epoch 1900/3500\n",
            "4352/4352 [==============================] - 0s 15us/step - loss: 0.0195 - acc: 0.9945 - val_loss: 0.0570 - val_acc: 0.9907\n",
            "Epoch 1901/3500\n",
            "4352/4352 [==============================] - 0s 15us/step - loss: 0.0205 - acc: 0.9945 - val_loss: 0.0562 - val_acc: 0.9911\n",
            "Epoch 1902/3500\n",
            "4352/4352 [==============================] - 0s 14us/step - loss: 0.0188 - acc: 0.9954 - val_loss: 0.0578 - val_acc: 0.9902\n",
            "Epoch 1903/3500\n",
            "4352/4352 [==============================] - 0s 14us/step - loss: 0.0183 - acc: 0.9954 - val_loss: 0.0531 - val_acc: 0.9921\n",
            "Epoch 1904/3500\n",
            "4352/4352 [==============================] - 0s 14us/step - loss: 0.0197 - acc: 0.9943 - val_loss: 0.0506 - val_acc: 0.9921\n",
            "Epoch 1905/3500\n",
            "4352/4352 [==============================] - 0s 14us/step - loss: 0.0179 - acc: 0.9949 - val_loss: 0.0510 - val_acc: 0.9921\n",
            "Epoch 1906/3500\n",
            "4352/4352 [==============================] - 0s 14us/step - loss: 0.0180 - acc: 0.9949 - val_loss: 0.0494 - val_acc: 0.9921\n",
            "Epoch 1907/3500\n",
            "4352/4352 [==============================] - 0s 15us/step - loss: 0.0188 - acc: 0.9949 - val_loss: 0.0487 - val_acc: 0.9911\n",
            "Epoch 1908/3500\n",
            "4352/4352 [==============================] - 0s 15us/step - loss: 0.0210 - acc: 0.9945 - val_loss: 0.0502 - val_acc: 0.9921\n",
            "Epoch 1909/3500\n",
            "4352/4352 [==============================] - 0s 15us/step - loss: 0.0192 - acc: 0.9945 - val_loss: 0.0555 - val_acc: 0.9907\n",
            "Epoch 1910/3500\n",
            "4352/4352 [==============================] - 0s 14us/step - loss: 0.0210 - acc: 0.9940 - val_loss: 0.0640 - val_acc: 0.9888\n",
            "Epoch 1911/3500\n",
            "4352/4352 [==============================] - 0s 15us/step - loss: 0.0211 - acc: 0.9943 - val_loss: 0.0634 - val_acc: 0.9888\n",
            "Epoch 1912/3500\n",
            "4352/4352 [==============================] - 0s 19us/step - loss: 0.0244 - acc: 0.9931 - val_loss: 0.0747 - val_acc: 0.9851\n",
            "Epoch 1913/3500\n",
            "4352/4352 [==============================] - 0s 15us/step - loss: 0.0253 - acc: 0.9929 - val_loss: 0.0770 - val_acc: 0.9855\n",
            "Epoch 1914/3500\n",
            "4352/4352 [==============================] - 0s 13us/step - loss: 0.0244 - acc: 0.9922 - val_loss: 0.0655 - val_acc: 0.9888\n",
            "Epoch 1915/3500\n",
            "4352/4352 [==============================] - 0s 14us/step - loss: 0.0247 - acc: 0.9926 - val_loss: 0.0692 - val_acc: 0.9879\n",
            "Epoch 1916/3500\n",
            "4352/4352 [==============================] - 0s 14us/step - loss: 0.0233 - acc: 0.9931 - val_loss: 0.0592 - val_acc: 0.9907\n",
            "Epoch 1917/3500\n",
            "4352/4352 [==============================] - 0s 15us/step - loss: 0.0217 - acc: 0.9938 - val_loss: 0.0579 - val_acc: 0.9907\n",
            "Epoch 1918/3500\n",
            "4352/4352 [==============================] - 0s 14us/step - loss: 0.0249 - acc: 0.9924 - val_loss: 0.0516 - val_acc: 0.9911\n",
            "Epoch 1919/3500\n",
            "4352/4352 [==============================] - 0s 13us/step - loss: 0.0236 - acc: 0.9926 - val_loss: 0.0509 - val_acc: 0.9916\n",
            "Epoch 1920/3500\n",
            "4352/4352 [==============================] - 0s 13us/step - loss: 0.0189 - acc: 0.9954 - val_loss: 0.0497 - val_acc: 0.9925\n",
            "Epoch 1921/3500\n",
            "4352/4352 [==============================] - 0s 14us/step - loss: 0.0189 - acc: 0.9940 - val_loss: 0.0548 - val_acc: 0.9907\n",
            "Epoch 1922/3500\n",
            "4352/4352 [==============================] - 0s 14us/step - loss: 0.0182 - acc: 0.9956 - val_loss: 0.0505 - val_acc: 0.9911\n",
            "Epoch 1923/3500\n",
            "4352/4352 [==============================] - 0s 15us/step - loss: 0.0206 - acc: 0.9940 - val_loss: 0.0536 - val_acc: 0.9911\n",
            "Epoch 1924/3500\n",
            "4352/4352 [==============================] - 0s 13us/step - loss: 0.0183 - acc: 0.9949 - val_loss: 0.0504 - val_acc: 0.9916\n",
            "Epoch 1925/3500\n",
            "4352/4352 [==============================] - 0s 15us/step - loss: 0.0191 - acc: 0.9956 - val_loss: 0.0538 - val_acc: 0.9921\n",
            "Epoch 1926/3500\n",
            "4352/4352 [==============================] - 0s 14us/step - loss: 0.0184 - acc: 0.9956 - val_loss: 0.0520 - val_acc: 0.9907\n",
            "Epoch 1927/3500\n",
            "4352/4352 [==============================] - 0s 15us/step - loss: 0.0207 - acc: 0.9947 - val_loss: 0.0523 - val_acc: 0.9911\n",
            "Epoch 1928/3500\n",
            "4352/4352 [==============================] - 0s 17us/step - loss: 0.0213 - acc: 0.9947 - val_loss: 0.0571 - val_acc: 0.9902\n",
            "Epoch 1929/3500\n",
            "4352/4352 [==============================] - 0s 13us/step - loss: 0.0190 - acc: 0.9947 - val_loss: 0.0578 - val_acc: 0.9907\n",
            "Epoch 1930/3500\n",
            "4352/4352 [==============================] - 0s 14us/step - loss: 0.0188 - acc: 0.9949 - val_loss: 0.0504 - val_acc: 0.9921\n",
            "Epoch 1931/3500\n",
            "4352/4352 [==============================] - 0s 14us/step - loss: 0.0183 - acc: 0.9949 - val_loss: 0.0522 - val_acc: 0.9916\n",
            "Epoch 1932/3500\n",
            "4352/4352 [==============================] - 0s 13us/step - loss: 0.0181 - acc: 0.9956 - val_loss: 0.0544 - val_acc: 0.9916\n",
            "Epoch 1933/3500\n",
            "4352/4352 [==============================] - 0s 13us/step - loss: 0.0191 - acc: 0.9949 - val_loss: 0.0554 - val_acc: 0.9911\n",
            "Epoch 1934/3500\n",
            "4352/4352 [==============================] - 0s 13us/step - loss: 0.0181 - acc: 0.9959 - val_loss: 0.0521 - val_acc: 0.9921\n",
            "Epoch 1935/3500\n",
            "4352/4352 [==============================] - 0s 13us/step - loss: 0.0182 - acc: 0.9949 - val_loss: 0.0514 - val_acc: 0.9921\n",
            "Epoch 1936/3500\n",
            "4352/4352 [==============================] - 0s 15us/step - loss: 0.0176 - acc: 0.9959 - val_loss: 0.0551 - val_acc: 0.9911\n",
            "Epoch 1937/3500\n",
            "4352/4352 [==============================] - 0s 14us/step - loss: 0.0194 - acc: 0.9947 - val_loss: 0.0683 - val_acc: 0.9883\n",
            "Epoch 1938/3500\n",
            "4352/4352 [==============================] - 0s 14us/step - loss: 0.0260 - acc: 0.9926 - val_loss: 0.0558 - val_acc: 0.9916\n",
            "Epoch 1939/3500\n",
            "4352/4352 [==============================] - 0s 19us/step - loss: 0.0243 - acc: 0.9938 - val_loss: 0.0526 - val_acc: 0.9921\n",
            "Epoch 1940/3500\n",
            "4352/4352 [==============================] - 0s 14us/step - loss: 0.0192 - acc: 0.9956 - val_loss: 0.0511 - val_acc: 0.9916\n",
            "Epoch 1941/3500\n",
            "4352/4352 [==============================] - 0s 14us/step - loss: 0.0214 - acc: 0.9943 - val_loss: 0.0486 - val_acc: 0.9921\n",
            "Epoch 1942/3500\n",
            "4352/4352 [==============================] - 0s 13us/step - loss: 0.0211 - acc: 0.9940 - val_loss: 0.0506 - val_acc: 0.9916\n",
            "Epoch 1943/3500\n",
            "4352/4352 [==============================] - 0s 14us/step - loss: 0.0205 - acc: 0.9938 - val_loss: 0.0505 - val_acc: 0.9916\n",
            "Epoch 1944/3500\n",
            "4352/4352 [==============================] - 0s 16us/step - loss: 0.0200 - acc: 0.9947 - val_loss: 0.0500 - val_acc: 0.9911\n",
            "Epoch 1945/3500\n",
            "4352/4352 [==============================] - 0s 13us/step - loss: 0.0214 - acc: 0.9929 - val_loss: 0.0508 - val_acc: 0.9921\n",
            "Epoch 1946/3500\n",
            "4352/4352 [==============================] - 0s 14us/step - loss: 0.0252 - acc: 0.9931 - val_loss: 0.0543 - val_acc: 0.9911\n",
            "Epoch 1947/3500\n",
            "4352/4352 [==============================] - 0s 15us/step - loss: 0.0189 - acc: 0.9943 - val_loss: 0.0579 - val_acc: 0.9907\n",
            "Epoch 1948/3500\n",
            "4352/4352 [==============================] - 0s 14us/step - loss: 0.0178 - acc: 0.9952 - val_loss: 0.0518 - val_acc: 0.9921\n",
            "Epoch 1949/3500\n",
            "4352/4352 [==============================] - 0s 14us/step - loss: 0.0183 - acc: 0.9954 - val_loss: 0.0544 - val_acc: 0.9916\n",
            "Epoch 1950/3500\n",
            "4352/4352 [==============================] - 0s 13us/step - loss: 0.0179 - acc: 0.9956 - val_loss: 0.0524 - val_acc: 0.9916\n",
            "Epoch 1951/3500\n",
            "4352/4352 [==============================] - 0s 14us/step - loss: 0.0177 - acc: 0.9956 - val_loss: 0.0557 - val_acc: 0.9916\n",
            "Epoch 1952/3500\n",
            "4352/4352 [==============================] - 0s 18us/step - loss: 0.0176 - acc: 0.9954 - val_loss: 0.0523 - val_acc: 0.9916\n",
            "Epoch 1953/3500\n",
            "4352/4352 [==============================] - 0s 14us/step - loss: 0.0183 - acc: 0.9959 - val_loss: 0.0537 - val_acc: 0.9916\n",
            "Epoch 1954/3500\n",
            "4352/4352 [==============================] - 0s 15us/step - loss: 0.0178 - acc: 0.9952 - val_loss: 0.0560 - val_acc: 0.9907\n",
            "Epoch 1955/3500\n",
            "4352/4352 [==============================] - 0s 13us/step - loss: 0.0182 - acc: 0.9949 - val_loss: 0.0538 - val_acc: 0.9916\n",
            "Epoch 1956/3500\n",
            "4352/4352 [==============================] - 0s 14us/step - loss: 0.0183 - acc: 0.9959 - val_loss: 0.0529 - val_acc: 0.9916\n",
            "Epoch 1957/3500\n",
            "4352/4352 [==============================] - 0s 14us/step - loss: 0.0185 - acc: 0.9959 - val_loss: 0.0631 - val_acc: 0.9893\n",
            "Epoch 1958/3500\n",
            "4352/4352 [==============================] - 0s 14us/step - loss: 0.0188 - acc: 0.9956 - val_loss: 0.0592 - val_acc: 0.9902\n",
            "Epoch 1959/3500\n",
            "4352/4352 [==============================] - 0s 14us/step - loss: 0.0200 - acc: 0.9949 - val_loss: 0.0588 - val_acc: 0.9907\n",
            "Epoch 1960/3500\n",
            "4352/4352 [==============================] - 0s 16us/step - loss: 0.0268 - acc: 0.9926 - val_loss: 0.0548 - val_acc: 0.9911\n",
            "Epoch 1961/3500\n",
            "4352/4352 [==============================] - 0s 14us/step - loss: 0.0204 - acc: 0.9943 - val_loss: 0.0513 - val_acc: 0.9921\n",
            "Epoch 1962/3500\n",
            "4352/4352 [==============================] - 0s 16us/step - loss: 0.0182 - acc: 0.9959 - val_loss: 0.0514 - val_acc: 0.9911\n",
            "Epoch 1963/3500\n",
            "4352/4352 [==============================] - 0s 15us/step - loss: 0.0191 - acc: 0.9945 - val_loss: 0.0521 - val_acc: 0.9921\n",
            "Epoch 1964/3500\n",
            "4352/4352 [==============================] - 0s 14us/step - loss: 0.0182 - acc: 0.9952 - val_loss: 0.0578 - val_acc: 0.9907\n",
            "Epoch 1965/3500\n",
            "4352/4352 [==============================] - 0s 14us/step - loss: 0.0194 - acc: 0.9956 - val_loss: 0.0566 - val_acc: 0.9907\n",
            "Epoch 1966/3500\n",
            "4352/4352 [==============================] - 0s 14us/step - loss: 0.0187 - acc: 0.9956 - val_loss: 0.0511 - val_acc: 0.9911\n",
            "Epoch 1967/3500\n",
            "4352/4352 [==============================] - 0s 15us/step - loss: 0.0213 - acc: 0.9936 - val_loss: 0.0506 - val_acc: 0.9921\n",
            "Epoch 1968/3500\n",
            "4352/4352 [==============================] - 0s 15us/step - loss: 0.0240 - acc: 0.9926 - val_loss: 0.0507 - val_acc: 0.9921\n",
            "Epoch 1969/3500\n",
            "4352/4352 [==============================] - 0s 14us/step - loss: 0.0227 - acc: 0.9933 - val_loss: 0.0531 - val_acc: 0.9921\n",
            "Epoch 1970/3500\n",
            "4352/4352 [==============================] - 0s 14us/step - loss: 0.0197 - acc: 0.9945 - val_loss: 0.0636 - val_acc: 0.9883\n",
            "Epoch 1971/3500\n",
            "4352/4352 [==============================] - 0s 14us/step - loss: 0.0196 - acc: 0.9945 - val_loss: 0.0537 - val_acc: 0.9916\n",
            "Epoch 1972/3500\n",
            "4352/4352 [==============================] - 0s 13us/step - loss: 0.0180 - acc: 0.9954 - val_loss: 0.0537 - val_acc: 0.9916\n",
            "Epoch 1973/3500\n",
            "4352/4352 [==============================] - 0s 13us/step - loss: 0.0182 - acc: 0.9954 - val_loss: 0.0570 - val_acc: 0.9902\n",
            "Epoch 1974/3500\n",
            "4352/4352 [==============================] - 0s 14us/step - loss: 0.0209 - acc: 0.9933 - val_loss: 0.0550 - val_acc: 0.9911\n",
            "Epoch 1975/3500\n",
            "4352/4352 [==============================] - 0s 15us/step - loss: 0.0195 - acc: 0.9947 - val_loss: 0.0526 - val_acc: 0.9916\n",
            "Epoch 1976/3500\n",
            "4352/4352 [==============================] - 0s 17us/step - loss: 0.0195 - acc: 0.9947 - val_loss: 0.0507 - val_acc: 0.9930\n",
            "Epoch 1977/3500\n",
            "4352/4352 [==============================] - 0s 15us/step - loss: 0.0178 - acc: 0.9949 - val_loss: 0.0524 - val_acc: 0.9916\n",
            "Epoch 1978/3500\n",
            "4352/4352 [==============================] - 0s 14us/step - loss: 0.0182 - acc: 0.9954 - val_loss: 0.0512 - val_acc: 0.9907\n",
            "Epoch 1979/3500\n",
            "4352/4352 [==============================] - 0s 13us/step - loss: 0.0199 - acc: 0.9943 - val_loss: 0.0566 - val_acc: 0.9916\n",
            "Epoch 1980/3500\n",
            "4352/4352 [==============================] - 0s 13us/step - loss: 0.0199 - acc: 0.9943 - val_loss: 0.0665 - val_acc: 0.9888\n",
            "Epoch 1981/3500\n",
            "4352/4352 [==============================] - 0s 13us/step - loss: 0.0201 - acc: 0.9945 - val_loss: 0.0585 - val_acc: 0.9902\n",
            "Epoch 1982/3500\n",
            "4352/4352 [==============================] - 0s 14us/step - loss: 0.0181 - acc: 0.9959 - val_loss: 0.0527 - val_acc: 0.9916\n",
            "Epoch 1983/3500\n",
            "4352/4352 [==============================] - 0s 13us/step - loss: 0.0184 - acc: 0.9956 - val_loss: 0.0561 - val_acc: 0.9907\n",
            "Epoch 1984/3500\n",
            "4352/4352 [==============================] - 0s 14us/step - loss: 0.0193 - acc: 0.9949 - val_loss: 0.0581 - val_acc: 0.9911\n",
            "Epoch 1985/3500\n",
            "4352/4352 [==============================] - 0s 14us/step - loss: 0.0211 - acc: 0.9940 - val_loss: 0.0613 - val_acc: 0.9897\n",
            "Epoch 1986/3500\n",
            "4352/4352 [==============================] - 0s 13us/step - loss: 0.0201 - acc: 0.9940 - val_loss: 0.0638 - val_acc: 0.9893\n",
            "Epoch 1987/3500\n",
            "4352/4352 [==============================] - 0s 14us/step - loss: 0.0217 - acc: 0.9945 - val_loss: 0.0607 - val_acc: 0.9907\n",
            "Epoch 1988/3500\n",
            "4352/4352 [==============================] - 0s 13us/step - loss: 0.0209 - acc: 0.9943 - val_loss: 0.0524 - val_acc: 0.9911\n",
            "Epoch 1989/3500\n",
            "4352/4352 [==============================] - 0s 13us/step - loss: 0.0214 - acc: 0.9936 - val_loss: 0.0530 - val_acc: 0.9916\n",
            "Epoch 1990/3500\n",
            "4352/4352 [==============================] - 0s 13us/step - loss: 0.0207 - acc: 0.9938 - val_loss: 0.0520 - val_acc: 0.9911\n",
            "Epoch 1991/3500\n",
            "4352/4352 [==============================] - 0s 14us/step - loss: 0.0188 - acc: 0.9952 - val_loss: 0.0512 - val_acc: 0.9911\n",
            "Epoch 1992/3500\n",
            "4352/4352 [==============================] - 0s 14us/step - loss: 0.0190 - acc: 0.9949 - val_loss: 0.0541 - val_acc: 0.9916\n",
            "Epoch 1993/3500\n",
            "4352/4352 [==============================] - 0s 16us/step - loss: 0.0175 - acc: 0.9954 - val_loss: 0.0579 - val_acc: 0.9902\n",
            "Epoch 1994/3500\n",
            "4352/4352 [==============================] - 0s 14us/step - loss: 0.0183 - acc: 0.9956 - val_loss: 0.0558 - val_acc: 0.9911\n",
            "Epoch 1995/3500\n",
            "4352/4352 [==============================] - 0s 15us/step - loss: 0.0176 - acc: 0.9956 - val_loss: 0.0537 - val_acc: 0.9911\n",
            "Epoch 1996/3500\n",
            "4352/4352 [==============================] - 0s 14us/step - loss: 0.0187 - acc: 0.9949 - val_loss: 0.0549 - val_acc: 0.9921\n",
            "Epoch 1997/3500\n",
            "4352/4352 [==============================] - 0s 15us/step - loss: 0.0185 - acc: 0.9949 - val_loss: 0.0542 - val_acc: 0.9916\n",
            "Epoch 1998/3500\n",
            "4352/4352 [==============================] - 0s 16us/step - loss: 0.0176 - acc: 0.9961 - val_loss: 0.0524 - val_acc: 0.9921\n",
            "Epoch 1999/3500\n",
            "4352/4352 [==============================] - 0s 13us/step - loss: 0.0177 - acc: 0.9959 - val_loss: 0.0557 - val_acc: 0.9911\n",
            "Epoch 2000/3500\n",
            "4352/4352 [==============================] - 0s 14us/step - loss: 0.0179 - acc: 0.9959 - val_loss: 0.0510 - val_acc: 0.9911\n",
            "Epoch 2001/3500\n",
            "4352/4352 [==============================] - 0s 14us/step - loss: 0.0183 - acc: 0.9945 - val_loss: 0.0518 - val_acc: 0.9925\n",
            "Epoch 2002/3500\n",
            "4352/4352 [==============================] - 0s 14us/step - loss: 0.0184 - acc: 0.9952 - val_loss: 0.0523 - val_acc: 0.9916\n",
            "Epoch 2003/3500\n",
            "4352/4352 [==============================] - 0s 14us/step - loss: 0.0189 - acc: 0.9945 - val_loss: 0.0535 - val_acc: 0.9911\n",
            "Epoch 2004/3500\n",
            "4352/4352 [==============================] - 0s 14us/step - loss: 0.0185 - acc: 0.9956 - val_loss: 0.0546 - val_acc: 0.9916\n",
            "Epoch 2005/3500\n",
            "4352/4352 [==============================] - 0s 15us/step - loss: 0.0181 - acc: 0.9952 - val_loss: 0.0552 - val_acc: 0.9916\n",
            "Epoch 2006/3500\n",
            "4352/4352 [==============================] - 0s 15us/step - loss: 0.0170 - acc: 0.9961 - val_loss: 0.0526 - val_acc: 0.9925\n",
            "Epoch 2007/3500\n",
            "4352/4352 [==============================] - 0s 14us/step - loss: 0.0181 - acc: 0.9952 - val_loss: 0.0539 - val_acc: 0.9916\n",
            "Epoch 2008/3500\n",
            "4352/4352 [==============================] - 0s 16us/step - loss: 0.0198 - acc: 0.9945 - val_loss: 0.0512 - val_acc: 0.9916\n",
            "Epoch 2009/3500\n",
            "4352/4352 [==============================] - 0s 17us/step - loss: 0.0173 - acc: 0.9956 - val_loss: 0.0545 - val_acc: 0.9916\n",
            "Epoch 2010/3500\n",
            "4352/4352 [==============================] - 0s 13us/step - loss: 0.0177 - acc: 0.9954 - val_loss: 0.0544 - val_acc: 0.9916\n",
            "Epoch 2011/3500\n",
            "4352/4352 [==============================] - 0s 15us/step - loss: 0.0183 - acc: 0.9954 - val_loss: 0.0618 - val_acc: 0.9893\n",
            "Epoch 2012/3500\n",
            "4352/4352 [==============================] - 0s 13us/step - loss: 0.0195 - acc: 0.9938 - val_loss: 0.0613 - val_acc: 0.9897\n",
            "Epoch 2013/3500\n",
            "4352/4352 [==============================] - 0s 13us/step - loss: 0.0185 - acc: 0.9949 - val_loss: 0.0505 - val_acc: 0.9911\n",
            "Epoch 2014/3500\n",
            "4352/4352 [==============================] - 0s 14us/step - loss: 0.0195 - acc: 0.9956 - val_loss: 0.0517 - val_acc: 0.9907\n",
            "Epoch 2015/3500\n",
            "4352/4352 [==============================] - 0s 14us/step - loss: 0.0183 - acc: 0.9945 - val_loss: 0.0528 - val_acc: 0.9916\n",
            "Epoch 2016/3500\n",
            "4352/4352 [==============================] - 0s 14us/step - loss: 0.0182 - acc: 0.9959 - val_loss: 0.0582 - val_acc: 0.9916\n",
            "Epoch 2017/3500\n",
            "4352/4352 [==============================] - 0s 13us/step - loss: 0.0179 - acc: 0.9954 - val_loss: 0.0583 - val_acc: 0.9911\n",
            "Epoch 2018/3500\n",
            "4352/4352 [==============================] - 0s 13us/step - loss: 0.0185 - acc: 0.9947 - val_loss: 0.0522 - val_acc: 0.9916\n",
            "Epoch 2019/3500\n",
            "4352/4352 [==============================] - 0s 14us/step - loss: 0.0184 - acc: 0.9949 - val_loss: 0.0523 - val_acc: 0.9907\n",
            "Epoch 2020/3500\n",
            "4352/4352 [==============================] - 0s 14us/step - loss: 0.0188 - acc: 0.9954 - val_loss: 0.0510 - val_acc: 0.9916\n",
            "Epoch 2021/3500\n",
            "4352/4352 [==============================] - 0s 14us/step - loss: 0.0215 - acc: 0.9940 - val_loss: 0.0538 - val_acc: 0.9911\n",
            "Epoch 2022/3500\n",
            "4352/4352 [==============================] - 0s 15us/step - loss: 0.0224 - acc: 0.9938 - val_loss: 0.0521 - val_acc: 0.9916\n",
            "Epoch 2023/3500\n",
            "4352/4352 [==============================] - 0s 14us/step - loss: 0.0202 - acc: 0.9940 - val_loss: 0.0595 - val_acc: 0.9902\n",
            "Epoch 2024/3500\n",
            "4352/4352 [==============================] - 0s 14us/step - loss: 0.0198 - acc: 0.9940 - val_loss: 0.0612 - val_acc: 0.9897\n",
            "Epoch 2025/3500\n",
            "4352/4352 [==============================] - 0s 15us/step - loss: 0.0201 - acc: 0.9945 - val_loss: 0.0640 - val_acc: 0.9897\n",
            "Epoch 2026/3500\n",
            "4352/4352 [==============================] - 0s 14us/step - loss: 0.0187 - acc: 0.9949 - val_loss: 0.0571 - val_acc: 0.9902\n",
            "Epoch 2027/3500\n",
            "4352/4352 [==============================] - 0s 14us/step - loss: 0.0177 - acc: 0.9961 - val_loss: 0.0555 - val_acc: 0.9916\n",
            "Epoch 2028/3500\n",
            "4352/4352 [==============================] - 0s 13us/step - loss: 0.0184 - acc: 0.9952 - val_loss: 0.0528 - val_acc: 0.9916\n",
            "Epoch 2029/3500\n",
            "4352/4352 [==============================] - 0s 16us/step - loss: 0.0184 - acc: 0.9954 - val_loss: 0.0523 - val_acc: 0.9921\n",
            "Epoch 2030/3500\n",
            "4352/4352 [==============================] - 0s 14us/step - loss: 0.0183 - acc: 0.9954 - val_loss: 0.0518 - val_acc: 0.9921\n",
            "Epoch 2031/3500\n",
            "4352/4352 [==============================] - 0s 15us/step - loss: 0.0181 - acc: 0.9954 - val_loss: 0.0516 - val_acc: 0.9907\n",
            "Epoch 2032/3500\n",
            "4352/4352 [==============================] - 0s 14us/step - loss: 0.0186 - acc: 0.9954 - val_loss: 0.0536 - val_acc: 0.9916\n",
            "Epoch 2033/3500\n",
            "4352/4352 [==============================] - 0s 14us/step - loss: 0.0176 - acc: 0.9959 - val_loss: 0.0588 - val_acc: 0.9907\n",
            "Epoch 2034/3500\n",
            "4352/4352 [==============================] - 0s 15us/step - loss: 0.0177 - acc: 0.9954 - val_loss: 0.0598 - val_acc: 0.9907\n",
            "Epoch 2035/3500\n",
            "4352/4352 [==============================] - 0s 14us/step - loss: 0.0180 - acc: 0.9954 - val_loss: 0.0539 - val_acc: 0.9911\n",
            "Epoch 2036/3500\n",
            "4352/4352 [==============================] - 0s 13us/step - loss: 0.0176 - acc: 0.9954 - val_loss: 0.0538 - val_acc: 0.9921\n",
            "Epoch 2037/3500\n",
            "4352/4352 [==============================] - 0s 15us/step - loss: 0.0179 - acc: 0.9954 - val_loss: 0.0591 - val_acc: 0.9907\n",
            "Epoch 2038/3500\n",
            "4352/4352 [==============================] - 0s 14us/step - loss: 0.0182 - acc: 0.9949 - val_loss: 0.0523 - val_acc: 0.9916\n",
            "Epoch 2039/3500\n",
            "4352/4352 [==============================] - 0s 14us/step - loss: 0.0176 - acc: 0.9952 - val_loss: 0.0528 - val_acc: 0.9907\n",
            "Epoch 2040/3500\n",
            "4352/4352 [==============================] - 0s 15us/step - loss: 0.0178 - acc: 0.9959 - val_loss: 0.0570 - val_acc: 0.9907\n",
            "Epoch 2041/3500\n",
            "4352/4352 [==============================] - 0s 16us/step - loss: 0.0175 - acc: 0.9954 - val_loss: 0.0553 - val_acc: 0.9916\n",
            "Epoch 2042/3500\n",
            "4352/4352 [==============================] - 0s 14us/step - loss: 0.0178 - acc: 0.9947 - val_loss: 0.0559 - val_acc: 0.9907\n",
            "Epoch 2043/3500\n",
            "4352/4352 [==============================] - 0s 13us/step - loss: 0.0177 - acc: 0.9963 - val_loss: 0.0547 - val_acc: 0.9916\n",
            "Epoch 2044/3500\n",
            "4352/4352 [==============================] - 0s 14us/step - loss: 0.0176 - acc: 0.9956 - val_loss: 0.0543 - val_acc: 0.9916\n",
            "Epoch 2045/3500\n",
            "4352/4352 [==============================] - 0s 14us/step - loss: 0.0175 - acc: 0.9954 - val_loss: 0.0541 - val_acc: 0.9916\n",
            "Epoch 2046/3500\n",
            "4352/4352 [==============================] - 0s 13us/step - loss: 0.0179 - acc: 0.9959 - val_loss: 0.0580 - val_acc: 0.9911\n",
            "Epoch 2047/3500\n",
            "4352/4352 [==============================] - 0s 14us/step - loss: 0.0176 - acc: 0.9959 - val_loss: 0.0543 - val_acc: 0.9916\n",
            "Epoch 2048/3500\n",
            "4352/4352 [==============================] - 0s 14us/step - loss: 0.0193 - acc: 0.9947 - val_loss: 0.0553 - val_acc: 0.9907\n",
            "Epoch 2049/3500\n",
            "4352/4352 [==============================] - 0s 14us/step - loss: 0.0184 - acc: 0.9952 - val_loss: 0.0537 - val_acc: 0.9916\n",
            "Epoch 2050/3500\n",
            "4352/4352 [==============================] - 0s 14us/step - loss: 0.0182 - acc: 0.9961 - val_loss: 0.0553 - val_acc: 0.9911\n",
            "Epoch 2051/3500\n",
            "4352/4352 [==============================] - 0s 14us/step - loss: 0.0179 - acc: 0.9956 - val_loss: 0.0551 - val_acc: 0.9916\n",
            "Epoch 2052/3500\n",
            "4352/4352 [==============================] - 0s 14us/step - loss: 0.0182 - acc: 0.9949 - val_loss: 0.0536 - val_acc: 0.9916\n",
            "Epoch 2053/3500\n",
            "4352/4352 [==============================] - 0s 16us/step - loss: 0.0184 - acc: 0.9959 - val_loss: 0.0618 - val_acc: 0.9897\n",
            "Epoch 2054/3500\n",
            "4352/4352 [==============================] - 0s 14us/step - loss: 0.0180 - acc: 0.9947 - val_loss: 0.0630 - val_acc: 0.9897\n",
            "Epoch 2055/3500\n",
            "4352/4352 [==============================] - 0s 14us/step - loss: 0.0184 - acc: 0.9949 - val_loss: 0.0535 - val_acc: 0.9916\n",
            "Epoch 2056/3500\n",
            "4352/4352 [==============================] - 0s 15us/step - loss: 0.0175 - acc: 0.9956 - val_loss: 0.0553 - val_acc: 0.9911\n",
            "Epoch 2057/3500\n",
            "4352/4352 [==============================] - 0s 16us/step - loss: 0.0172 - acc: 0.9956 - val_loss: 0.0520 - val_acc: 0.9921\n",
            "Epoch 2058/3500\n",
            "4352/4352 [==============================] - 0s 15us/step - loss: 0.0193 - acc: 0.9945 - val_loss: 0.0583 - val_acc: 0.9907\n",
            "Epoch 2059/3500\n",
            "4352/4352 [==============================] - 0s 14us/step - loss: 0.0202 - acc: 0.9940 - val_loss: 0.0561 - val_acc: 0.9907\n",
            "Epoch 2060/3500\n",
            "4352/4352 [==============================] - 0s 14us/step - loss: 0.0186 - acc: 0.9954 - val_loss: 0.0589 - val_acc: 0.9902\n",
            "Epoch 2061/3500\n",
            "4352/4352 [==============================] - 0s 14us/step - loss: 0.0178 - acc: 0.9954 - val_loss: 0.0536 - val_acc: 0.9916\n",
            "Epoch 2062/3500\n",
            "4352/4352 [==============================] - 0s 13us/step - loss: 0.0188 - acc: 0.9952 - val_loss: 0.0537 - val_acc: 0.9916\n",
            "Epoch 2063/3500\n",
            "4352/4352 [==============================] - 0s 13us/step - loss: 0.0185 - acc: 0.9952 - val_loss: 0.0562 - val_acc: 0.9916\n",
            "Epoch 2064/3500\n",
            "4352/4352 [==============================] - 0s 15us/step - loss: 0.0175 - acc: 0.9959 - val_loss: 0.0553 - val_acc: 0.9916\n",
            "Epoch 2065/3500\n",
            "4352/4352 [==============================] - 0s 14us/step - loss: 0.0189 - acc: 0.9956 - val_loss: 0.0530 - val_acc: 0.9916\n",
            "Epoch 2066/3500\n",
            "4352/4352 [==============================] - 0s 15us/step - loss: 0.0184 - acc: 0.9956 - val_loss: 0.0575 - val_acc: 0.9911\n",
            "Epoch 2067/3500\n",
            "4352/4352 [==============================] - 0s 15us/step - loss: 0.0199 - acc: 0.9940 - val_loss: 0.0583 - val_acc: 0.9911\n",
            "Epoch 2068/3500\n",
            "4352/4352 [==============================] - 0s 14us/step - loss: 0.0224 - acc: 0.9929 - val_loss: 0.0765 - val_acc: 0.9874\n",
            "Epoch 2069/3500\n",
            "4352/4352 [==============================] - 0s 14us/step - loss: 0.0215 - acc: 0.9943 - val_loss: 0.0560 - val_acc: 0.9916\n",
            "Epoch 2070/3500\n",
            "4352/4352 [==============================] - 0s 14us/step - loss: 0.0192 - acc: 0.9949 - val_loss: 0.0518 - val_acc: 0.9916\n",
            "Epoch 2071/3500\n",
            "4352/4352 [==============================] - 0s 12us/step - loss: 0.0203 - acc: 0.9949 - val_loss: 0.0529 - val_acc: 0.9916\n",
            "Epoch 2072/3500\n",
            "4352/4352 [==============================] - 0s 13us/step - loss: 0.0179 - acc: 0.9956 - val_loss: 0.0534 - val_acc: 0.9921\n",
            "Epoch 2073/3500\n",
            "4352/4352 [==============================] - 0s 15us/step - loss: 0.0178 - acc: 0.9956 - val_loss: 0.0561 - val_acc: 0.9916\n",
            "Epoch 2074/3500\n",
            "4352/4352 [==============================] - 0s 14us/step - loss: 0.0185 - acc: 0.9949 - val_loss: 0.0608 - val_acc: 0.9902\n",
            "Epoch 2075/3500\n",
            "4352/4352 [==============================] - 0s 15us/step - loss: 0.0191 - acc: 0.9940 - val_loss: 0.0602 - val_acc: 0.9897\n",
            "Epoch 2076/3500\n",
            "4352/4352 [==============================] - 0s 15us/step - loss: 0.0188 - acc: 0.9947 - val_loss: 0.0581 - val_acc: 0.9911\n",
            "Epoch 2077/3500\n",
            "4352/4352 [==============================] - 0s 14us/step - loss: 0.0180 - acc: 0.9954 - val_loss: 0.0575 - val_acc: 0.9907\n",
            "Epoch 2078/3500\n",
            "4352/4352 [==============================] - 0s 14us/step - loss: 0.0184 - acc: 0.9959 - val_loss: 0.0542 - val_acc: 0.9916\n",
            "Epoch 2079/3500\n",
            "4352/4352 [==============================] - 0s 14us/step - loss: 0.0174 - acc: 0.9961 - val_loss: 0.0516 - val_acc: 0.9907\n",
            "Epoch 2080/3500\n",
            "4352/4352 [==============================] - 0s 13us/step - loss: 0.0189 - acc: 0.9949 - val_loss: 0.0570 - val_acc: 0.9907\n",
            "Epoch 2081/3500\n",
            "4352/4352 [==============================] - 0s 15us/step - loss: 0.0175 - acc: 0.9959 - val_loss: 0.0584 - val_acc: 0.9911\n",
            "Epoch 2082/3500\n",
            "4352/4352 [==============================] - 0s 15us/step - loss: 0.0185 - acc: 0.9947 - val_loss: 0.0551 - val_acc: 0.9916\n",
            "Epoch 2083/3500\n",
            "4352/4352 [==============================] - 0s 15us/step - loss: 0.0177 - acc: 0.9954 - val_loss: 0.0559 - val_acc: 0.9911\n",
            "Epoch 2084/3500\n",
            "4352/4352 [==============================] - 0s 14us/step - loss: 0.0174 - acc: 0.9956 - val_loss: 0.0564 - val_acc: 0.9911\n",
            "Epoch 2085/3500\n",
            "4352/4352 [==============================] - 0s 13us/step - loss: 0.0174 - acc: 0.9959 - val_loss: 0.0554 - val_acc: 0.9916\n",
            "Epoch 2086/3500\n",
            "4352/4352 [==============================] - 0s 13us/step - loss: 0.0180 - acc: 0.9949 - val_loss: 0.0548 - val_acc: 0.9916\n",
            "Epoch 2087/3500\n",
            "4352/4352 [==============================] - 0s 13us/step - loss: 0.0182 - acc: 0.9949 - val_loss: 0.0543 - val_acc: 0.9911\n",
            "Epoch 2088/3500\n",
            "4352/4352 [==============================] - 0s 14us/step - loss: 0.0182 - acc: 0.9959 - val_loss: 0.0556 - val_acc: 0.9911\n",
            "Epoch 2089/3500\n",
            "4352/4352 [==============================] - 0s 16us/step - loss: 0.0197 - acc: 0.9947 - val_loss: 0.0684 - val_acc: 0.9893\n",
            "Epoch 2090/3500\n",
            "4352/4352 [==============================] - 0s 14us/step - loss: 0.0198 - acc: 0.9949 - val_loss: 0.0742 - val_acc: 0.9869\n",
            "Epoch 2091/3500\n",
            "4352/4352 [==============================] - 0s 15us/step - loss: 0.0212 - acc: 0.9938 - val_loss: 0.0644 - val_acc: 0.9897\n",
            "Epoch 2092/3500\n",
            "4352/4352 [==============================] - 0s 15us/step - loss: 0.0219 - acc: 0.9938 - val_loss: 0.0536 - val_acc: 0.9907\n",
            "Epoch 2093/3500\n",
            "4352/4352 [==============================] - 0s 15us/step - loss: 0.0227 - acc: 0.9929 - val_loss: 0.0515 - val_acc: 0.9925\n",
            "Epoch 2094/3500\n",
            "4352/4352 [==============================] - 0s 14us/step - loss: 0.0207 - acc: 0.9940 - val_loss: 0.0546 - val_acc: 0.9921\n",
            "Epoch 2095/3500\n",
            "4352/4352 [==============================] - 0s 15us/step - loss: 0.0246 - acc: 0.9915 - val_loss: 0.0536 - val_acc: 0.9921\n",
            "Epoch 2096/3500\n",
            "4352/4352 [==============================] - 0s 15us/step - loss: 0.0210 - acc: 0.9936 - val_loss: 0.0527 - val_acc: 0.9916\n",
            "Epoch 2097/3500\n",
            "4352/4352 [==============================] - 0s 14us/step - loss: 0.0183 - acc: 0.9947 - val_loss: 0.0577 - val_acc: 0.9916\n",
            "Epoch 2098/3500\n",
            "4352/4352 [==============================] - 0s 13us/step - loss: 0.0183 - acc: 0.9949 - val_loss: 0.0591 - val_acc: 0.9911\n",
            "Epoch 2099/3500\n",
            "4352/4352 [==============================] - 0s 13us/step - loss: 0.0176 - acc: 0.9959 - val_loss: 0.0533 - val_acc: 0.9921\n",
            "Epoch 2100/3500\n",
            "4352/4352 [==============================] - 0s 14us/step - loss: 0.0174 - acc: 0.9961 - val_loss: 0.0527 - val_acc: 0.9916\n",
            "Epoch 2101/3500\n",
            "4352/4352 [==============================] - 0s 15us/step - loss: 0.0177 - acc: 0.9949 - val_loss: 0.0584 - val_acc: 0.9911\n",
            "Epoch 2102/3500\n",
            "4352/4352 [==============================] - 0s 16us/step - loss: 0.0170 - acc: 0.9961 - val_loss: 0.0540 - val_acc: 0.9916\n",
            "Epoch 2103/3500\n",
            "4352/4352 [==============================] - 0s 15us/step - loss: 0.0174 - acc: 0.9961 - val_loss: 0.0580 - val_acc: 0.9911\n",
            "Epoch 2104/3500\n",
            "4352/4352 [==============================] - 0s 13us/step - loss: 0.0182 - acc: 0.9956 - val_loss: 0.0571 - val_acc: 0.9916\n",
            "Epoch 2105/3500\n",
            "4352/4352 [==============================] - 0s 18us/step - loss: 0.0174 - acc: 0.9956 - val_loss: 0.0598 - val_acc: 0.9907\n",
            "Epoch 2106/3500\n",
            "4352/4352 [==============================] - 0s 14us/step - loss: 0.0175 - acc: 0.9956 - val_loss: 0.0589 - val_acc: 0.9902\n",
            "Epoch 2107/3500\n",
            "4352/4352 [==============================] - 0s 14us/step - loss: 0.0176 - acc: 0.9947 - val_loss: 0.0609 - val_acc: 0.9897\n",
            "Epoch 2108/3500\n",
            "4352/4352 [==============================] - 0s 15us/step - loss: 0.0176 - acc: 0.9954 - val_loss: 0.0537 - val_acc: 0.9921\n",
            "Epoch 2109/3500\n",
            "4352/4352 [==============================] - 0s 14us/step - loss: 0.0194 - acc: 0.9952 - val_loss: 0.0516 - val_acc: 0.9907\n",
            "Epoch 2110/3500\n",
            "4352/4352 [==============================] - 0s 12us/step - loss: 0.0195 - acc: 0.9947 - val_loss: 0.0550 - val_acc: 0.9911\n",
            "Epoch 2111/3500\n",
            "4352/4352 [==============================] - 0s 14us/step - loss: 0.0208 - acc: 0.9940 - val_loss: 0.0533 - val_acc: 0.9911\n",
            "Epoch 2112/3500\n",
            "4352/4352 [==============================] - 0s 14us/step - loss: 0.0192 - acc: 0.9943 - val_loss: 0.0541 - val_acc: 0.9907\n",
            "Epoch 2113/3500\n",
            "4352/4352 [==============================] - 0s 13us/step - loss: 0.0202 - acc: 0.9943 - val_loss: 0.0546 - val_acc: 0.9916\n",
            "Epoch 2114/3500\n",
            "4352/4352 [==============================] - 0s 13us/step - loss: 0.0197 - acc: 0.9940 - val_loss: 0.0579 - val_acc: 0.9907\n",
            "Epoch 2115/3500\n",
            "4352/4352 [==============================] - 0s 13us/step - loss: 0.0203 - acc: 0.9943 - val_loss: 0.0596 - val_acc: 0.9902\n",
            "Epoch 2116/3500\n",
            "4352/4352 [==============================] - 0s 13us/step - loss: 0.0177 - acc: 0.9954 - val_loss: 0.0573 - val_acc: 0.9911\n",
            "Epoch 2117/3500\n",
            "4352/4352 [==============================] - 0s 14us/step - loss: 0.0177 - acc: 0.9963 - val_loss: 0.0526 - val_acc: 0.9916\n",
            "Epoch 2118/3500\n",
            "4352/4352 [==============================] - 0s 14us/step - loss: 0.0175 - acc: 0.9963 - val_loss: 0.0537 - val_acc: 0.9916\n",
            "Epoch 2119/3500\n",
            "4352/4352 [==============================] - 0s 13us/step - loss: 0.0173 - acc: 0.9956 - val_loss: 0.0531 - val_acc: 0.9925\n",
            "Epoch 2120/3500\n",
            "4352/4352 [==============================] - 0s 14us/step - loss: 0.0171 - acc: 0.9961 - val_loss: 0.0569 - val_acc: 0.9911\n",
            "Epoch 2121/3500\n",
            "4352/4352 [==============================] - 0s 13us/step - loss: 0.0175 - acc: 0.9966 - val_loss: 0.0585 - val_acc: 0.9916\n",
            "Epoch 2122/3500\n",
            "4352/4352 [==============================] - 0s 15us/step - loss: 0.0181 - acc: 0.9949 - val_loss: 0.0553 - val_acc: 0.9916\n",
            "Epoch 2123/3500\n",
            "4352/4352 [==============================] - 0s 13us/step - loss: 0.0194 - acc: 0.9945 - val_loss: 0.0539 - val_acc: 0.9916\n",
            "Epoch 2124/3500\n",
            "4352/4352 [==============================] - 0s 14us/step - loss: 0.0182 - acc: 0.9959 - val_loss: 0.0544 - val_acc: 0.9902\n",
            "Epoch 2125/3500\n",
            "4352/4352 [==============================] - 0s 13us/step - loss: 0.0181 - acc: 0.9949 - val_loss: 0.0527 - val_acc: 0.9916\n",
            "Epoch 2126/3500\n",
            "4352/4352 [==============================] - 0s 17us/step - loss: 0.0187 - acc: 0.9956 - val_loss: 0.0536 - val_acc: 0.9916\n",
            "Epoch 2127/3500\n",
            "4352/4352 [==============================] - 0s 14us/step - loss: 0.0180 - acc: 0.9949 - val_loss: 0.0576 - val_acc: 0.9911\n",
            "Epoch 2128/3500\n",
            "4352/4352 [==============================] - 0s 15us/step - loss: 0.0181 - acc: 0.9949 - val_loss: 0.0639 - val_acc: 0.9897\n",
            "Epoch 2129/3500\n",
            "4352/4352 [==============================] - 0s 13us/step - loss: 0.0197 - acc: 0.9943 - val_loss: 0.0550 - val_acc: 0.9911\n",
            "Epoch 2130/3500\n",
            "4352/4352 [==============================] - 0s 14us/step - loss: 0.0205 - acc: 0.9940 - val_loss: 0.0535 - val_acc: 0.9902\n",
            "Epoch 2131/3500\n",
            "4352/4352 [==============================] - 0s 13us/step - loss: 0.0209 - acc: 0.9938 - val_loss: 0.0517 - val_acc: 0.9907\n",
            "Epoch 2132/3500\n",
            "4352/4352 [==============================] - 0s 14us/step - loss: 0.0188 - acc: 0.9952 - val_loss: 0.0524 - val_acc: 0.9916\n",
            "Epoch 2133/3500\n",
            "4352/4352 [==============================] - 0s 14us/step - loss: 0.0200 - acc: 0.9933 - val_loss: 0.0538 - val_acc: 0.9902\n",
            "Epoch 2134/3500\n",
            "4352/4352 [==============================] - 0s 14us/step - loss: 0.0213 - acc: 0.9940 - val_loss: 0.0538 - val_acc: 0.9916\n",
            "Epoch 2135/3500\n",
            "4352/4352 [==============================] - 0s 14us/step - loss: 0.0192 - acc: 0.9952 - val_loss: 0.0582 - val_acc: 0.9911\n",
            "Epoch 2136/3500\n",
            "4352/4352 [==============================] - 0s 13us/step - loss: 0.0176 - acc: 0.9961 - val_loss: 0.0609 - val_acc: 0.9907\n",
            "Epoch 2137/3500\n",
            "4352/4352 [==============================] - 0s 14us/step - loss: 0.0175 - acc: 0.9959 - val_loss: 0.0530 - val_acc: 0.9907\n",
            "Epoch 2138/3500\n",
            "4352/4352 [==============================] - 0s 16us/step - loss: 0.0184 - acc: 0.9949 - val_loss: 0.0532 - val_acc: 0.9916\n",
            "Epoch 2139/3500\n",
            "4352/4352 [==============================] - 0s 15us/step - loss: 0.0202 - acc: 0.9947 - val_loss: 0.0597 - val_acc: 0.9907\n",
            "Epoch 2140/3500\n",
            "4352/4352 [==============================] - 0s 15us/step - loss: 0.0177 - acc: 0.9959 - val_loss: 0.0534 - val_acc: 0.9916\n",
            "Epoch 2141/3500\n",
            "4352/4352 [==============================] - 0s 16us/step - loss: 0.0178 - acc: 0.9952 - val_loss: 0.0543 - val_acc: 0.9916\n",
            "Epoch 2142/3500\n",
            "4352/4352 [==============================] - 0s 14us/step - loss: 0.0196 - acc: 0.9952 - val_loss: 0.0551 - val_acc: 0.9916\n",
            "Epoch 2143/3500\n",
            "4352/4352 [==============================] - 0s 14us/step - loss: 0.0175 - acc: 0.9959 - val_loss: 0.0590 - val_acc: 0.9907\n",
            "Epoch 2144/3500\n",
            "4352/4352 [==============================] - 0s 14us/step - loss: 0.0175 - acc: 0.9956 - val_loss: 0.0560 - val_acc: 0.9921\n",
            "Epoch 2145/3500\n",
            "4352/4352 [==============================] - 0s 14us/step - loss: 0.0174 - acc: 0.9954 - val_loss: 0.0532 - val_acc: 0.9921\n",
            "Epoch 2146/3500\n",
            "4352/4352 [==============================] - 0s 15us/step - loss: 0.0174 - acc: 0.9963 - val_loss: 0.0562 - val_acc: 0.9916\n",
            "Epoch 2147/3500\n",
            "4352/4352 [==============================] - 0s 16us/step - loss: 0.0171 - acc: 0.9961 - val_loss: 0.0529 - val_acc: 0.9907\n",
            "Epoch 2148/3500\n",
            "4352/4352 [==============================] - 0s 15us/step - loss: 0.0176 - acc: 0.9952 - val_loss: 0.0557 - val_acc: 0.9916\n",
            "Epoch 2149/3500\n",
            "4352/4352 [==============================] - 0s 14us/step - loss: 0.0168 - acc: 0.9956 - val_loss: 0.0632 - val_acc: 0.9902\n",
            "Epoch 2150/3500\n",
            "4352/4352 [==============================] - 0s 16us/step - loss: 0.0182 - acc: 0.9949 - val_loss: 0.0590 - val_acc: 0.9907\n",
            "Epoch 2151/3500\n",
            "4352/4352 [==============================] - 0s 15us/step - loss: 0.0171 - acc: 0.9956 - val_loss: 0.0543 - val_acc: 0.9921\n",
            "Epoch 2152/3500\n",
            "4352/4352 [==============================] - 0s 14us/step - loss: 0.0178 - acc: 0.9961 - val_loss: 0.0530 - val_acc: 0.9925\n",
            "Epoch 2153/3500\n",
            "4352/4352 [==============================] - 0s 16us/step - loss: 0.0181 - acc: 0.9947 - val_loss: 0.0528 - val_acc: 0.9907\n",
            "Epoch 2154/3500\n",
            "4352/4352 [==============================] - 0s 15us/step - loss: 0.0182 - acc: 0.9949 - val_loss: 0.0534 - val_acc: 0.9921\n",
            "Epoch 2155/3500\n",
            "4352/4352 [==============================] - 0s 15us/step - loss: 0.0189 - acc: 0.9947 - val_loss: 0.0527 - val_acc: 0.9907\n",
            "Epoch 2156/3500\n",
            "4352/4352 [==============================] - 0s 15us/step - loss: 0.0194 - acc: 0.9945 - val_loss: 0.0521 - val_acc: 0.9921\n",
            "Epoch 2157/3500\n",
            "4352/4352 [==============================] - 0s 15us/step - loss: 0.0204 - acc: 0.9938 - val_loss: 0.0535 - val_acc: 0.9907\n",
            "Epoch 2158/3500\n",
            "4352/4352 [==============================] - 0s 14us/step - loss: 0.0177 - acc: 0.9959 - val_loss: 0.0535 - val_acc: 0.9921\n",
            "Epoch 2159/3500\n",
            "4352/4352 [==============================] - 0s 15us/step - loss: 0.0174 - acc: 0.9952 - val_loss: 0.0579 - val_acc: 0.9907\n",
            "Epoch 2160/3500\n",
            "4352/4352 [==============================] - 0s 15us/step - loss: 0.0171 - acc: 0.9966 - val_loss: 0.0546 - val_acc: 0.9916\n",
            "Epoch 2161/3500\n",
            "4352/4352 [==============================] - 0s 16us/step - loss: 0.0175 - acc: 0.9947 - val_loss: 0.0546 - val_acc: 0.9916\n",
            "Epoch 2162/3500\n",
            "4352/4352 [==============================] - 0s 15us/step - loss: 0.0183 - acc: 0.9949 - val_loss: 0.0553 - val_acc: 0.9916\n",
            "Epoch 2163/3500\n",
            "4352/4352 [==============================] - 0s 15us/step - loss: 0.0178 - acc: 0.9945 - val_loss: 0.0538 - val_acc: 0.9916\n",
            "Epoch 2164/3500\n",
            "4352/4352 [==============================] - 0s 15us/step - loss: 0.0172 - acc: 0.9954 - val_loss: 0.0592 - val_acc: 0.9911\n",
            "Epoch 2165/3500\n",
            "4352/4352 [==============================] - 0s 14us/step - loss: 0.0187 - acc: 0.9940 - val_loss: 0.0636 - val_acc: 0.9897\n",
            "Epoch 2166/3500\n",
            "4352/4352 [==============================] - 0s 14us/step - loss: 0.0185 - acc: 0.9947 - val_loss: 0.0692 - val_acc: 0.9897\n",
            "Epoch 2167/3500\n",
            "4352/4352 [==============================] - 0s 13us/step - loss: 0.0182 - acc: 0.9947 - val_loss: 0.0540 - val_acc: 0.9921\n",
            "Epoch 2168/3500\n",
            "4352/4352 [==============================] - 0s 14us/step - loss: 0.0185 - acc: 0.9952 - val_loss: 0.0528 - val_acc: 0.9911\n",
            "Epoch 2169/3500\n",
            "4352/4352 [==============================] - 0s 17us/step - loss: 0.0177 - acc: 0.9959 - val_loss: 0.0543 - val_acc: 0.9925\n",
            "Epoch 2170/3500\n",
            "4352/4352 [==============================] - 0s 15us/step - loss: 0.0174 - acc: 0.9961 - val_loss: 0.0527 - val_acc: 0.9921\n",
            "Epoch 2171/3500\n",
            "4352/4352 [==============================] - 0s 13us/step - loss: 0.0197 - acc: 0.9947 - val_loss: 0.0553 - val_acc: 0.9925\n",
            "Epoch 2172/3500\n",
            "4352/4352 [==============================] - 0s 14us/step - loss: 0.0199 - acc: 0.9943 - val_loss: 0.0551 - val_acc: 0.9916\n",
            "Epoch 2173/3500\n",
            "4352/4352 [==============================] - 0s 13us/step - loss: 0.0187 - acc: 0.9947 - val_loss: 0.0566 - val_acc: 0.9916\n",
            "Epoch 2174/3500\n",
            "4352/4352 [==============================] - 0s 14us/step - loss: 0.0180 - acc: 0.9959 - val_loss: 0.0573 - val_acc: 0.9911\n",
            "Epoch 2175/3500\n",
            "4352/4352 [==============================] - 0s 14us/step - loss: 0.0182 - acc: 0.9954 - val_loss: 0.0550 - val_acc: 0.9911\n",
            "Epoch 2176/3500\n",
            "4352/4352 [==============================] - 0s 15us/step - loss: 0.0176 - acc: 0.9959 - val_loss: 0.0568 - val_acc: 0.9911\n",
            "Epoch 2177/3500\n",
            "4352/4352 [==============================] - 0s 15us/step - loss: 0.0182 - acc: 0.9952 - val_loss: 0.0568 - val_acc: 0.9916\n",
            "Epoch 2178/3500\n",
            "4352/4352 [==============================] - 0s 13us/step - loss: 0.0179 - acc: 0.9949 - val_loss: 0.0564 - val_acc: 0.9907\n",
            "Epoch 2179/3500\n",
            "4352/4352 [==============================] - 0s 14us/step - loss: 0.0170 - acc: 0.9961 - val_loss: 0.0590 - val_acc: 0.9907\n",
            "Epoch 2180/3500\n",
            "4352/4352 [==============================] - 0s 14us/step - loss: 0.0174 - acc: 0.9961 - val_loss: 0.0543 - val_acc: 0.9916\n",
            "Epoch 2181/3500\n",
            "4352/4352 [==============================] - 0s 14us/step - loss: 0.0183 - acc: 0.9959 - val_loss: 0.0601 - val_acc: 0.9907\n",
            "Epoch 2182/3500\n",
            "4352/4352 [==============================] - 0s 14us/step - loss: 0.0180 - acc: 0.9952 - val_loss: 0.0607 - val_acc: 0.9902\n",
            "Epoch 2183/3500\n",
            "4352/4352 [==============================] - 0s 13us/step - loss: 0.0177 - acc: 0.9956 - val_loss: 0.0638 - val_acc: 0.9897\n",
            "Epoch 2184/3500\n",
            "4352/4352 [==============================] - 0s 13us/step - loss: 0.0193 - acc: 0.9952 - val_loss: 0.0689 - val_acc: 0.9897\n",
            "Epoch 2185/3500\n",
            "4352/4352 [==============================] - 0s 15us/step - loss: 0.0208 - acc: 0.9938 - val_loss: 0.0636 - val_acc: 0.9902\n",
            "Epoch 2186/3500\n",
            "4352/4352 [==============================] - 0s 15us/step - loss: 0.0178 - acc: 0.9954 - val_loss: 0.0547 - val_acc: 0.9916\n",
            "Epoch 2187/3500\n",
            "4352/4352 [==============================] - 0s 14us/step - loss: 0.0177 - acc: 0.9956 - val_loss: 0.0536 - val_acc: 0.9907\n",
            "Epoch 2188/3500\n",
            "4352/4352 [==============================] - 0s 14us/step - loss: 0.0176 - acc: 0.9947 - val_loss: 0.0534 - val_acc: 0.9921\n",
            "Epoch 2189/3500\n",
            "4352/4352 [==============================] - 0s 15us/step - loss: 0.0174 - acc: 0.9956 - val_loss: 0.0540 - val_acc: 0.9921\n",
            "Epoch 2190/3500\n",
            "4352/4352 [==============================] - 0s 14us/step - loss: 0.0184 - acc: 0.9949 - val_loss: 0.0530 - val_acc: 0.9921\n",
            "Epoch 2191/3500\n",
            "4352/4352 [==============================] - 0s 13us/step - loss: 0.0170 - acc: 0.9954 - val_loss: 0.0619 - val_acc: 0.9902\n",
            "Epoch 2192/3500\n",
            "4352/4352 [==============================] - 0s 14us/step - loss: 0.0170 - acc: 0.9961 - val_loss: 0.0560 - val_acc: 0.9916\n",
            "Epoch 2193/3500\n",
            "4352/4352 [==============================] - 0s 13us/step - loss: 0.0187 - acc: 0.9949 - val_loss: 0.0582 - val_acc: 0.9911\n",
            "Epoch 2194/3500\n",
            "4352/4352 [==============================] - 0s 14us/step - loss: 0.0188 - acc: 0.9947 - val_loss: 0.0536 - val_acc: 0.9916\n",
            "Epoch 2195/3500\n",
            "4352/4352 [==============================] - 0s 13us/step - loss: 0.0192 - acc: 0.9947 - val_loss: 0.0537 - val_acc: 0.9916\n",
            "Epoch 2196/3500\n",
            "4352/4352 [==============================] - 0s 14us/step - loss: 0.0180 - acc: 0.9949 - val_loss: 0.0528 - val_acc: 0.9911\n",
            "Epoch 2197/3500\n",
            "4352/4352 [==============================] - 0s 14us/step - loss: 0.0232 - acc: 0.9926 - val_loss: 0.0530 - val_acc: 0.9921\n",
            "Epoch 2198/3500\n",
            "4352/4352 [==============================] - 0s 14us/step - loss: 0.0184 - acc: 0.9956 - val_loss: 0.0537 - val_acc: 0.9921\n",
            "Epoch 2199/3500\n",
            "4352/4352 [==============================] - 0s 14us/step - loss: 0.0193 - acc: 0.9947 - val_loss: 0.0524 - val_acc: 0.9916\n",
            "Epoch 2200/3500\n",
            "4352/4352 [==============================] - 0s 14us/step - loss: 0.0182 - acc: 0.9959 - val_loss: 0.0550 - val_acc: 0.9921\n",
            "Epoch 2201/3500\n",
            "4352/4352 [==============================] - 0s 12us/step - loss: 0.0175 - acc: 0.9952 - val_loss: 0.0538 - val_acc: 0.9916\n",
            "Epoch 2202/3500\n",
            "4352/4352 [==============================] - 0s 15us/step - loss: 0.0172 - acc: 0.9954 - val_loss: 0.0549 - val_acc: 0.9921\n",
            "Epoch 2203/3500\n",
            "4352/4352 [==============================] - 0s 14us/step - loss: 0.0175 - acc: 0.9952 - val_loss: 0.0590 - val_acc: 0.9907\n",
            "Epoch 2204/3500\n",
            "4352/4352 [==============================] - 0s 13us/step - loss: 0.0177 - acc: 0.9956 - val_loss: 0.0580 - val_acc: 0.9907\n",
            "Epoch 2205/3500\n",
            "4352/4352 [==============================] - 0s 14us/step - loss: 0.0173 - acc: 0.9956 - val_loss: 0.0616 - val_acc: 0.9902\n",
            "Epoch 2206/3500\n",
            "4352/4352 [==============================] - 0s 14us/step - loss: 0.0185 - acc: 0.9954 - val_loss: 0.0623 - val_acc: 0.9897\n",
            "Epoch 2207/3500\n",
            "4352/4352 [==============================] - 0s 14us/step - loss: 0.0189 - acc: 0.9949 - val_loss: 0.0602 - val_acc: 0.9897\n",
            "Epoch 2208/3500\n",
            "4352/4352 [==============================] - 0s 13us/step - loss: 0.0173 - acc: 0.9956 - val_loss: 0.0585 - val_acc: 0.9911\n",
            "Epoch 2209/3500\n",
            "4352/4352 [==============================] - 0s 14us/step - loss: 0.0183 - acc: 0.9959 - val_loss: 0.0597 - val_acc: 0.9897\n",
            "Epoch 2210/3500\n",
            "4352/4352 [==============================] - 0s 13us/step - loss: 0.0183 - acc: 0.9952 - val_loss: 0.0548 - val_acc: 0.9916\n",
            "Epoch 2211/3500\n",
            "4352/4352 [==============================] - 0s 14us/step - loss: 0.0206 - acc: 0.9936 - val_loss: 0.0531 - val_acc: 0.9916\n",
            "Epoch 2212/3500\n",
            "4352/4352 [==============================] - 0s 13us/step - loss: 0.0216 - acc: 0.9924 - val_loss: 0.0550 - val_acc: 0.9921\n",
            "Epoch 2213/3500\n",
            "4352/4352 [==============================] - 0s 13us/step - loss: 0.0238 - acc: 0.9931 - val_loss: 0.0526 - val_acc: 0.9925\n",
            "Epoch 2214/3500\n",
            "4352/4352 [==============================] - 0s 14us/step - loss: 0.0179 - acc: 0.9954 - val_loss: 0.0608 - val_acc: 0.9907\n",
            "Epoch 2215/3500\n",
            "4352/4352 [==============================] - 0s 16us/step - loss: 0.0173 - acc: 0.9963 - val_loss: 0.0557 - val_acc: 0.9911\n",
            "Epoch 2216/3500\n",
            "4352/4352 [==============================] - 0s 16us/step - loss: 0.0191 - acc: 0.9949 - val_loss: 0.0533 - val_acc: 0.9925\n",
            "Epoch 2217/3500\n",
            "4352/4352 [==============================] - 0s 15us/step - loss: 0.0210 - acc: 0.9936 - val_loss: 0.0532 - val_acc: 0.9911\n",
            "Epoch 2218/3500\n",
            "4352/4352 [==============================] - 0s 16us/step - loss: 0.0186 - acc: 0.9945 - val_loss: 0.0532 - val_acc: 0.9916\n",
            "Epoch 2219/3500\n",
            "4352/4352 [==============================] - 0s 14us/step - loss: 0.0190 - acc: 0.9949 - val_loss: 0.0528 - val_acc: 0.9911\n",
            "Epoch 2220/3500\n",
            "4352/4352 [==============================] - 0s 13us/step - loss: 0.0180 - acc: 0.9945 - val_loss: 0.0532 - val_acc: 0.9921\n",
            "Epoch 2221/3500\n",
            "4352/4352 [==============================] - 0s 15us/step - loss: 0.0181 - acc: 0.9952 - val_loss: 0.0535 - val_acc: 0.9925\n",
            "Epoch 2222/3500\n",
            "4352/4352 [==============================] - 0s 14us/step - loss: 0.0167 - acc: 0.9961 - val_loss: 0.0583 - val_acc: 0.9907\n",
            "Epoch 2223/3500\n",
            "4352/4352 [==============================] - 0s 14us/step - loss: 0.0173 - acc: 0.9954 - val_loss: 0.0597 - val_acc: 0.9907\n",
            "Epoch 2224/3500\n",
            "4352/4352 [==============================] - 0s 15us/step - loss: 0.0174 - acc: 0.9956 - val_loss: 0.0674 - val_acc: 0.9893\n",
            "Epoch 2225/3500\n",
            "4352/4352 [==============================] - 0s 14us/step - loss: 0.0205 - acc: 0.9947 - val_loss: 0.0625 - val_acc: 0.9902\n",
            "Epoch 2226/3500\n",
            "4352/4352 [==============================] - 0s 13us/step - loss: 0.0189 - acc: 0.9936 - val_loss: 0.0543 - val_acc: 0.9921\n",
            "Epoch 2227/3500\n",
            "4352/4352 [==============================] - 0s 14us/step - loss: 0.0166 - acc: 0.9956 - val_loss: 0.0585 - val_acc: 0.9916\n",
            "Epoch 2228/3500\n",
            "4352/4352 [==============================] - 0s 16us/step - loss: 0.0174 - acc: 0.9956 - val_loss: 0.0561 - val_acc: 0.9921\n",
            "Epoch 2229/3500\n",
            "4352/4352 [==============================] - 0s 15us/step - loss: 0.0171 - acc: 0.9959 - val_loss: 0.0663 - val_acc: 0.9888\n",
            "Epoch 2230/3500\n",
            "4352/4352 [==============================] - 0s 14us/step - loss: 0.0196 - acc: 0.9947 - val_loss: 0.0666 - val_acc: 0.9893\n",
            "Epoch 2231/3500\n",
            "4352/4352 [==============================] - 0s 14us/step - loss: 0.0183 - acc: 0.9947 - val_loss: 0.0618 - val_acc: 0.9902\n",
            "Epoch 2232/3500\n",
            "4352/4352 [==============================] - 0s 14us/step - loss: 0.0167 - acc: 0.9963 - val_loss: 0.0539 - val_acc: 0.9916\n",
            "Epoch 2233/3500\n",
            "4352/4352 [==============================] - 0s 13us/step - loss: 0.0168 - acc: 0.9959 - val_loss: 0.0543 - val_acc: 0.9925\n",
            "Epoch 2234/3500\n",
            "4352/4352 [==============================] - 0s 16us/step - loss: 0.0202 - acc: 0.9947 - val_loss: 0.0538 - val_acc: 0.9921\n",
            "Epoch 2235/3500\n",
            "4352/4352 [==============================] - 0s 14us/step - loss: 0.0183 - acc: 0.9949 - val_loss: 0.0592 - val_acc: 0.9911\n",
            "Epoch 2236/3500\n",
            "4352/4352 [==============================] - 0s 15us/step - loss: 0.0177 - acc: 0.9963 - val_loss: 0.0562 - val_acc: 0.9916\n",
            "Epoch 2237/3500\n",
            "4352/4352 [==============================] - 0s 13us/step - loss: 0.0168 - acc: 0.9954 - val_loss: 0.0548 - val_acc: 0.9907\n",
            "Epoch 2238/3500\n",
            "4352/4352 [==============================] - 0s 15us/step - loss: 0.0173 - acc: 0.9954 - val_loss: 0.0574 - val_acc: 0.9911\n",
            "Epoch 2239/3500\n",
            "4352/4352 [==============================] - 0s 13us/step - loss: 0.0174 - acc: 0.9954 - val_loss: 0.0598 - val_acc: 0.9907\n",
            "Epoch 2240/3500\n",
            "4352/4352 [==============================] - 0s 14us/step - loss: 0.0200 - acc: 0.9933 - val_loss: 0.0627 - val_acc: 0.9897\n",
            "Epoch 2241/3500\n",
            "4352/4352 [==============================] - 0s 14us/step - loss: 0.0190 - acc: 0.9945 - val_loss: 0.0551 - val_acc: 0.9911\n",
            "Epoch 2242/3500\n",
            "4352/4352 [==============================] - 0s 15us/step - loss: 0.0176 - acc: 0.9949 - val_loss: 0.0586 - val_acc: 0.9907\n",
            "Epoch 2243/3500\n",
            "4352/4352 [==============================] - 0s 15us/step - loss: 0.0215 - acc: 0.9943 - val_loss: 0.0707 - val_acc: 0.9893\n",
            "Epoch 2244/3500\n",
            "4352/4352 [==============================] - 0s 15us/step - loss: 0.0193 - acc: 0.9949 - val_loss: 0.0619 - val_acc: 0.9897\n",
            "Epoch 2245/3500\n",
            "4352/4352 [==============================] - 0s 14us/step - loss: 0.0183 - acc: 0.9947 - val_loss: 0.0554 - val_acc: 0.9911\n",
            "Epoch 2246/3500\n",
            "4352/4352 [==============================] - 0s 15us/step - loss: 0.0185 - acc: 0.9959 - val_loss: 0.0538 - val_acc: 0.9925\n",
            "Epoch 2247/3500\n",
            "4352/4352 [==============================] - 0s 14us/step - loss: 0.0194 - acc: 0.9947 - val_loss: 0.0535 - val_acc: 0.9916\n",
            "Epoch 2248/3500\n",
            "4352/4352 [==============================] - 0s 14us/step - loss: 0.0176 - acc: 0.9956 - val_loss: 0.0538 - val_acc: 0.9921\n",
            "Epoch 2249/3500\n",
            "4352/4352 [==============================] - 0s 15us/step - loss: 0.0172 - acc: 0.9956 - val_loss: 0.0534 - val_acc: 0.9911\n",
            "Epoch 2250/3500\n",
            "4352/4352 [==============================] - 0s 17us/step - loss: 0.0190 - acc: 0.9949 - val_loss: 0.0534 - val_acc: 0.9907\n",
            "Epoch 2251/3500\n",
            "4352/4352 [==============================] - 0s 14us/step - loss: 0.0171 - acc: 0.9949 - val_loss: 0.0653 - val_acc: 0.9893\n",
            "Epoch 2252/3500\n",
            "4352/4352 [==============================] - 0s 15us/step - loss: 0.0219 - acc: 0.9933 - val_loss: 0.0766 - val_acc: 0.9869\n",
            "Epoch 2253/3500\n",
            "4352/4352 [==============================] - 0s 14us/step - loss: 0.0229 - acc: 0.9929 - val_loss: 0.0899 - val_acc: 0.9823\n",
            "Epoch 2254/3500\n",
            "4352/4352 [==============================] - 0s 14us/step - loss: 0.0274 - acc: 0.9915 - val_loss: 0.0874 - val_acc: 0.9832\n",
            "Epoch 2255/3500\n",
            "4352/4352 [==============================] - 0s 15us/step - loss: 0.0223 - acc: 0.9931 - val_loss: 0.0641 - val_acc: 0.9897\n",
            "Epoch 2256/3500\n",
            "4352/4352 [==============================] - 0s 15us/step - loss: 0.0185 - acc: 0.9952 - val_loss: 0.0573 - val_acc: 0.9921\n",
            "Epoch 2257/3500\n",
            "4352/4352 [==============================] - 0s 15us/step - loss: 0.0175 - acc: 0.9961 - val_loss: 0.0555 - val_acc: 0.9911\n",
            "Epoch 2258/3500\n",
            "4352/4352 [==============================] - 0s 17us/step - loss: 0.0168 - acc: 0.9959 - val_loss: 0.0537 - val_acc: 0.9911\n",
            "Epoch 2259/3500\n",
            "4352/4352 [==============================] - 0s 14us/step - loss: 0.0178 - acc: 0.9952 - val_loss: 0.0541 - val_acc: 0.9911\n",
            "Epoch 2260/3500\n",
            "4352/4352 [==============================] - 0s 14us/step - loss: 0.0182 - acc: 0.9956 - val_loss: 0.0574 - val_acc: 0.9911\n",
            "Epoch 2261/3500\n",
            "4352/4352 [==============================] - 0s 13us/step - loss: 0.0177 - acc: 0.9961 - val_loss: 0.0635 - val_acc: 0.9897\n",
            "Epoch 2262/3500\n",
            "4352/4352 [==============================] - 0s 14us/step - loss: 0.0192 - acc: 0.9949 - val_loss: 0.0621 - val_acc: 0.9907\n",
            "Epoch 2263/3500\n",
            "4352/4352 [==============================] - 0s 17us/step - loss: 0.0186 - acc: 0.9947 - val_loss: 0.0705 - val_acc: 0.9888\n",
            "Epoch 2264/3500\n",
            "4352/4352 [==============================] - 0s 15us/step - loss: 0.0185 - acc: 0.9959 - val_loss: 0.0552 - val_acc: 0.9925\n",
            "Epoch 2265/3500\n",
            "4352/4352 [==============================] - 0s 17us/step - loss: 0.0166 - acc: 0.9963 - val_loss: 0.0560 - val_acc: 0.9921\n",
            "Epoch 2266/3500\n",
            "4352/4352 [==============================] - 0s 13us/step - loss: 0.0166 - acc: 0.9963 - val_loss: 0.0641 - val_acc: 0.9902\n",
            "Epoch 2267/3500\n",
            "4352/4352 [==============================] - 0s 14us/step - loss: 0.0181 - acc: 0.9945 - val_loss: 0.0585 - val_acc: 0.9911\n",
            "Epoch 2268/3500\n",
            "4352/4352 [==============================] - 0s 13us/step - loss: 0.0172 - acc: 0.9959 - val_loss: 0.0588 - val_acc: 0.9907\n",
            "Epoch 2269/3500\n",
            "4352/4352 [==============================] - 0s 14us/step - loss: 0.0184 - acc: 0.9961 - val_loss: 0.0584 - val_acc: 0.9911\n",
            "Epoch 2270/3500\n",
            "4352/4352 [==============================] - 0s 14us/step - loss: 0.0179 - acc: 0.9954 - val_loss: 0.0558 - val_acc: 0.9925\n",
            "Epoch 2271/3500\n",
            "4352/4352 [==============================] - 0s 14us/step - loss: 0.0187 - acc: 0.9947 - val_loss: 0.0543 - val_acc: 0.9921\n",
            "Epoch 2272/3500\n",
            "4352/4352 [==============================] - 0s 14us/step - loss: 0.0171 - acc: 0.9956 - val_loss: 0.0563 - val_acc: 0.9911\n",
            "Epoch 2273/3500\n",
            "4352/4352 [==============================] - 0s 19us/step - loss: 0.0170 - acc: 0.9954 - val_loss: 0.0578 - val_acc: 0.9911\n",
            "Epoch 2274/3500\n",
            "4352/4352 [==============================] - 0s 14us/step - loss: 0.0167 - acc: 0.9959 - val_loss: 0.0567 - val_acc: 0.9916\n",
            "Epoch 2275/3500\n",
            "4352/4352 [==============================] - 0s 13us/step - loss: 0.0169 - acc: 0.9961 - val_loss: 0.0553 - val_acc: 0.9921\n",
            "Epoch 2276/3500\n",
            "4352/4352 [==============================] - 0s 14us/step - loss: 0.0174 - acc: 0.9952 - val_loss: 0.0581 - val_acc: 0.9911\n",
            "Epoch 2277/3500\n",
            "4352/4352 [==============================] - 0s 13us/step - loss: 0.0163 - acc: 0.9961 - val_loss: 0.0545 - val_acc: 0.9916\n",
            "Epoch 2278/3500\n",
            "4352/4352 [==============================] - 0s 14us/step - loss: 0.0167 - acc: 0.9954 - val_loss: 0.0566 - val_acc: 0.9911\n",
            "Epoch 2279/3500\n",
            "4352/4352 [==============================] - 0s 13us/step - loss: 0.0170 - acc: 0.9961 - val_loss: 0.0575 - val_acc: 0.9921\n",
            "Epoch 2280/3500\n",
            "4352/4352 [==============================] - 0s 13us/step - loss: 0.0170 - acc: 0.9956 - val_loss: 0.0569 - val_acc: 0.9916\n",
            "Epoch 2281/3500\n",
            "4352/4352 [==============================] - 0s 15us/step - loss: 0.0202 - acc: 0.9938 - val_loss: 0.0611 - val_acc: 0.9902\n",
            "Epoch 2282/3500\n",
            "4352/4352 [==============================] - 0s 13us/step - loss: 0.0204 - acc: 0.9943 - val_loss: 0.0715 - val_acc: 0.9883\n",
            "Epoch 2283/3500\n",
            "4352/4352 [==============================] - 0s 14us/step - loss: 0.0201 - acc: 0.9943 - val_loss: 0.0762 - val_acc: 0.9869\n",
            "Epoch 2284/3500\n",
            "4352/4352 [==============================] - 0s 15us/step - loss: 0.0206 - acc: 0.9949 - val_loss: 0.0643 - val_acc: 0.9902\n",
            "Epoch 2285/3500\n",
            "4352/4352 [==============================] - 0s 14us/step - loss: 0.0176 - acc: 0.9952 - val_loss: 0.0550 - val_acc: 0.9921\n",
            "Epoch 2286/3500\n",
            "4352/4352 [==============================] - 0s 13us/step - loss: 0.0176 - acc: 0.9954 - val_loss: 0.0580 - val_acc: 0.9907\n",
            "Epoch 2287/3500\n",
            "4352/4352 [==============================] - 0s 14us/step - loss: 0.0202 - acc: 0.9943 - val_loss: 0.0568 - val_acc: 0.9911\n",
            "Epoch 2288/3500\n",
            "4352/4352 [==============================] - 0s 14us/step - loss: 0.0176 - acc: 0.9956 - val_loss: 0.0623 - val_acc: 0.9907\n",
            "Epoch 2289/3500\n",
            "4352/4352 [==============================] - 0s 13us/step - loss: 0.0191 - acc: 0.9947 - val_loss: 0.0758 - val_acc: 0.9888\n",
            "Epoch 2290/3500\n",
            "4352/4352 [==============================] - 0s 14us/step - loss: 0.0199 - acc: 0.9947 - val_loss: 0.0632 - val_acc: 0.9907\n",
            "Epoch 2291/3500\n",
            "4352/4352 [==============================] - 0s 13us/step - loss: 0.0190 - acc: 0.9938 - val_loss: 0.0659 - val_acc: 0.9893\n",
            "Epoch 2292/3500\n",
            "4352/4352 [==============================] - 0s 13us/step - loss: 0.0215 - acc: 0.9936 - val_loss: 0.0600 - val_acc: 0.9902\n",
            "Epoch 2293/3500\n",
            "4352/4352 [==============================] - 0s 13us/step - loss: 0.0201 - acc: 0.9947 - val_loss: 0.0576 - val_acc: 0.9907\n",
            "Epoch 2294/3500\n",
            "4352/4352 [==============================] - 0s 14us/step - loss: 0.0238 - acc: 0.9920 - val_loss: 0.0666 - val_acc: 0.9893\n",
            "Epoch 2295/3500\n",
            "4352/4352 [==============================] - 0s 13us/step - loss: 0.0298 - acc: 0.9908 - val_loss: 0.0858 - val_acc: 0.9855\n",
            "Epoch 2296/3500\n",
            "4352/4352 [==============================] - 0s 13us/step - loss: 0.0322 - acc: 0.9910 - val_loss: 0.0778 - val_acc: 0.9860\n",
            "Epoch 2297/3500\n",
            "4352/4352 [==============================] - 0s 14us/step - loss: 0.0250 - acc: 0.9931 - val_loss: 0.0522 - val_acc: 0.9925\n",
            "Epoch 2298/3500\n",
            "4352/4352 [==============================] - 0s 16us/step - loss: 0.0207 - acc: 0.9945 - val_loss: 0.0536 - val_acc: 0.9911\n",
            "Epoch 2299/3500\n",
            "4352/4352 [==============================] - 0s 14us/step - loss: 0.0198 - acc: 0.9943 - val_loss: 0.0544 - val_acc: 0.9911\n",
            "Epoch 2300/3500\n",
            "4352/4352 [==============================] - 0s 14us/step - loss: 0.0193 - acc: 0.9945 - val_loss: 0.0546 - val_acc: 0.9902\n",
            "Epoch 2301/3500\n",
            "4352/4352 [==============================] - 0s 15us/step - loss: 0.0198 - acc: 0.9949 - val_loss: 0.0539 - val_acc: 0.9916\n",
            "Epoch 2302/3500\n",
            "4352/4352 [==============================] - 0s 14us/step - loss: 0.0198 - acc: 0.9936 - val_loss: 0.0661 - val_acc: 0.9897\n",
            "Epoch 2303/3500\n",
            "4352/4352 [==============================] - 0s 16us/step - loss: 0.0200 - acc: 0.9938 - val_loss: 0.0621 - val_acc: 0.9907\n",
            "Epoch 2304/3500\n",
            "4352/4352 [==============================] - 0s 15us/step - loss: 0.0176 - acc: 0.9952 - val_loss: 0.0635 - val_acc: 0.9893\n",
            "Epoch 2305/3500\n",
            "4352/4352 [==============================] - 0s 15us/step - loss: 0.0170 - acc: 0.9959 - val_loss: 0.0551 - val_acc: 0.9911\n",
            "Epoch 2306/3500\n",
            "4352/4352 [==============================] - 0s 14us/step - loss: 0.0168 - acc: 0.9959 - val_loss: 0.0559 - val_acc: 0.9916\n",
            "Epoch 2307/3500\n",
            "4352/4352 [==============================] - 0s 14us/step - loss: 0.0168 - acc: 0.9961 - val_loss: 0.0564 - val_acc: 0.9907\n",
            "Epoch 2308/3500\n",
            "4352/4352 [==============================] - 0s 13us/step - loss: 0.0166 - acc: 0.9961 - val_loss: 0.0592 - val_acc: 0.9902\n",
            "Epoch 2309/3500\n",
            "4352/4352 [==============================] - 0s 14us/step - loss: 0.0169 - acc: 0.9956 - val_loss: 0.0636 - val_acc: 0.9897\n",
            "Epoch 2310/3500\n",
            "4352/4352 [==============================] - 0s 13us/step - loss: 0.0170 - acc: 0.9963 - val_loss: 0.0539 - val_acc: 0.9925\n",
            "Epoch 2311/3500\n",
            "4352/4352 [==============================] - 0s 13us/step - loss: 0.0173 - acc: 0.9954 - val_loss: 0.0554 - val_acc: 0.9925\n",
            "Epoch 2312/3500\n",
            "4352/4352 [==============================] - 0s 13us/step - loss: 0.0169 - acc: 0.9961 - val_loss: 0.0591 - val_acc: 0.9902\n",
            "Epoch 2313/3500\n",
            "4352/4352 [==============================] - 0s 14us/step - loss: 0.0171 - acc: 0.9959 - val_loss: 0.0585 - val_acc: 0.9907\n",
            "Epoch 2314/3500\n",
            "4352/4352 [==============================] - 0s 16us/step - loss: 0.0168 - acc: 0.9966 - val_loss: 0.0541 - val_acc: 0.9916\n",
            "Epoch 2315/3500\n",
            "4352/4352 [==============================] - 0s 14us/step - loss: 0.0188 - acc: 0.9945 - val_loss: 0.0551 - val_acc: 0.9925\n",
            "Epoch 2316/3500\n",
            "4352/4352 [==============================] - 0s 14us/step - loss: 0.0163 - acc: 0.9966 - val_loss: 0.0561 - val_acc: 0.9911\n",
            "Epoch 2317/3500\n",
            "4352/4352 [==============================] - 0s 14us/step - loss: 0.0164 - acc: 0.9963 - val_loss: 0.0608 - val_acc: 0.9907\n",
            "Epoch 2318/3500\n",
            "4352/4352 [==============================] - 0s 14us/step - loss: 0.0167 - acc: 0.9956 - val_loss: 0.0601 - val_acc: 0.9902\n",
            "Epoch 2319/3500\n",
            "4352/4352 [==============================] - 0s 14us/step - loss: 0.0170 - acc: 0.9954 - val_loss: 0.0585 - val_acc: 0.9911\n",
            "Epoch 2320/3500\n",
            "4352/4352 [==============================] - 0s 14us/step - loss: 0.0166 - acc: 0.9956 - val_loss: 0.0570 - val_acc: 0.9911\n",
            "Epoch 2321/3500\n",
            "4352/4352 [==============================] - 0s 13us/step - loss: 0.0167 - acc: 0.9959 - val_loss: 0.0646 - val_acc: 0.9897\n",
            "Epoch 2322/3500\n",
            "4352/4352 [==============================] - 0s 13us/step - loss: 0.0170 - acc: 0.9959 - val_loss: 0.0594 - val_acc: 0.9907\n",
            "Epoch 2323/3500\n",
            "4352/4352 [==============================] - 0s 13us/step - loss: 0.0163 - acc: 0.9968 - val_loss: 0.0554 - val_acc: 0.9907\n",
            "Epoch 2324/3500\n",
            "4352/4352 [==============================] - 0s 13us/step - loss: 0.0169 - acc: 0.9954 - val_loss: 0.0560 - val_acc: 0.9911\n",
            "Epoch 2325/3500\n",
            "4352/4352 [==============================] - 0s 14us/step - loss: 0.0169 - acc: 0.9956 - val_loss: 0.0613 - val_acc: 0.9897\n",
            "Epoch 2326/3500\n",
            "4352/4352 [==============================] - 0s 14us/step - loss: 0.0213 - acc: 0.9940 - val_loss: 0.0627 - val_acc: 0.9902\n",
            "Epoch 2327/3500\n",
            "4352/4352 [==============================] - 0s 14us/step - loss: 0.0197 - acc: 0.9945 - val_loss: 0.0727 - val_acc: 0.9883\n",
            "Epoch 2328/3500\n",
            "4352/4352 [==============================] - 0s 15us/step - loss: 0.0190 - acc: 0.9947 - val_loss: 0.0618 - val_acc: 0.9897\n",
            "Epoch 2329/3500\n",
            "4352/4352 [==============================] - 0s 15us/step - loss: 0.0178 - acc: 0.9949 - val_loss: 0.0556 - val_acc: 0.9916\n",
            "Epoch 2330/3500\n",
            "4352/4352 [==============================] - 0s 16us/step - loss: 0.0165 - acc: 0.9963 - val_loss: 0.0547 - val_acc: 0.9907\n",
            "Epoch 2331/3500\n",
            "4352/4352 [==============================] - 0s 14us/step - loss: 0.0187 - acc: 0.9947 - val_loss: 0.0560 - val_acc: 0.9916\n",
            "Epoch 2332/3500\n",
            "4352/4352 [==============================] - 0s 16us/step - loss: 0.0201 - acc: 0.9947 - val_loss: 0.0572 - val_acc: 0.9911\n",
            "Epoch 2333/3500\n",
            "4352/4352 [==============================] - 0s 15us/step - loss: 0.0207 - acc: 0.9945 - val_loss: 0.0694 - val_acc: 0.9893\n",
            "Epoch 2334/3500\n",
            "4352/4352 [==============================] - 0s 15us/step - loss: 0.0202 - acc: 0.9952 - val_loss: 0.0628 - val_acc: 0.9897\n",
            "Epoch 2335/3500\n",
            "4352/4352 [==============================] - 0s 14us/step - loss: 0.0179 - acc: 0.9943 - val_loss: 0.0709 - val_acc: 0.9897\n",
            "Epoch 2336/3500\n",
            "4352/4352 [==============================] - 0s 15us/step - loss: 0.0188 - acc: 0.9956 - val_loss: 0.0667 - val_acc: 0.9893\n",
            "Epoch 2337/3500\n",
            "4352/4352 [==============================] - 0s 14us/step - loss: 0.0220 - acc: 0.9933 - val_loss: 0.0579 - val_acc: 0.9907\n",
            "Epoch 2338/3500\n",
            "4352/4352 [==============================] - 0s 14us/step - loss: 0.0239 - acc: 0.9936 - val_loss: 0.0548 - val_acc: 0.9907\n",
            "Epoch 2339/3500\n",
            "4352/4352 [==============================] - 0s 15us/step - loss: 0.0179 - acc: 0.9959 - val_loss: 0.0542 - val_acc: 0.9921\n",
            "Epoch 2340/3500\n",
            "4352/4352 [==============================] - 0s 14us/step - loss: 0.0186 - acc: 0.9959 - val_loss: 0.0570 - val_acc: 0.9907\n",
            "Epoch 2341/3500\n",
            "4352/4352 [==============================] - 0s 14us/step - loss: 0.0176 - acc: 0.9954 - val_loss: 0.0582 - val_acc: 0.9907\n",
            "Epoch 2342/3500\n",
            "4352/4352 [==============================] - 0s 14us/step - loss: 0.0163 - acc: 0.9966 - val_loss: 0.0558 - val_acc: 0.9916\n",
            "Epoch 2343/3500\n",
            "4352/4352 [==============================] - 0s 15us/step - loss: 0.0172 - acc: 0.9961 - val_loss: 0.0653 - val_acc: 0.9893\n",
            "Epoch 2344/3500\n",
            "4352/4352 [==============================] - 0s 15us/step - loss: 0.0179 - acc: 0.9952 - val_loss: 0.0629 - val_acc: 0.9902\n",
            "Epoch 2345/3500\n",
            "4352/4352 [==============================] - 0s 13us/step - loss: 0.0175 - acc: 0.9956 - val_loss: 0.0594 - val_acc: 0.9907\n",
            "Epoch 2346/3500\n",
            "4352/4352 [==============================] - 0s 16us/step - loss: 0.0165 - acc: 0.9961 - val_loss: 0.0555 - val_acc: 0.9916\n",
            "Epoch 2347/3500\n",
            "4352/4352 [==============================] - 0s 15us/step - loss: 0.0170 - acc: 0.9961 - val_loss: 0.0625 - val_acc: 0.9893\n",
            "Epoch 2348/3500\n",
            "4352/4352 [==============================] - 0s 15us/step - loss: 0.0177 - acc: 0.9949 - val_loss: 0.0671 - val_acc: 0.9893\n",
            "Epoch 2349/3500\n",
            "4352/4352 [==============================] - 0s 14us/step - loss: 0.0176 - acc: 0.9961 - val_loss: 0.0617 - val_acc: 0.9897\n",
            "Epoch 2350/3500\n",
            "4352/4352 [==============================] - 0s 15us/step - loss: 0.0177 - acc: 0.9952 - val_loss: 0.0547 - val_acc: 0.9921\n",
            "Epoch 2351/3500\n",
            "4352/4352 [==============================] - 0s 13us/step - loss: 0.0164 - acc: 0.9959 - val_loss: 0.0628 - val_acc: 0.9902\n",
            "Epoch 2352/3500\n",
            "4352/4352 [==============================] - 0s 14us/step - loss: 0.0171 - acc: 0.9956 - val_loss: 0.0607 - val_acc: 0.9907\n",
            "Epoch 2353/3500\n",
            "4352/4352 [==============================] - 0s 14us/step - loss: 0.0180 - acc: 0.9947 - val_loss: 0.0587 - val_acc: 0.9907\n",
            "Epoch 2354/3500\n",
            "4352/4352 [==============================] - 0s 15us/step - loss: 0.0188 - acc: 0.9947 - val_loss: 0.0596 - val_acc: 0.9902\n",
            "Epoch 2355/3500\n",
            "4352/4352 [==============================] - 0s 15us/step - loss: 0.0176 - acc: 0.9952 - val_loss: 0.0545 - val_acc: 0.9911\n",
            "Epoch 2356/3500\n",
            "4352/4352 [==============================] - 0s 14us/step - loss: 0.0187 - acc: 0.9945 - val_loss: 0.0557 - val_acc: 0.9916\n",
            "Epoch 2357/3500\n",
            "4352/4352 [==============================] - 0s 13us/step - loss: 0.0165 - acc: 0.9954 - val_loss: 0.0544 - val_acc: 0.9911\n",
            "Epoch 2358/3500\n",
            "4352/4352 [==============================] - 0s 13us/step - loss: 0.0185 - acc: 0.9947 - val_loss: 0.0588 - val_acc: 0.9911\n",
            "Epoch 2359/3500\n",
            "4352/4352 [==============================] - 0s 14us/step - loss: 0.0170 - acc: 0.9959 - val_loss: 0.0621 - val_acc: 0.9902\n",
            "Epoch 2360/3500\n",
            "4352/4352 [==============================] - 0s 13us/step - loss: 0.0172 - acc: 0.9956 - val_loss: 0.0625 - val_acc: 0.9893\n",
            "Epoch 2361/3500\n",
            "4352/4352 [==============================] - 0s 13us/step - loss: 0.0175 - acc: 0.9947 - val_loss: 0.0695 - val_acc: 0.9888\n",
            "Epoch 2362/3500\n",
            "4352/4352 [==============================] - 0s 16us/step - loss: 0.0186 - acc: 0.9952 - val_loss: 0.0604 - val_acc: 0.9911\n",
            "Epoch 2363/3500\n",
            "4352/4352 [==============================] - 0s 15us/step - loss: 0.0172 - acc: 0.9954 - val_loss: 0.0555 - val_acc: 0.9925\n",
            "Epoch 2364/3500\n",
            "4352/4352 [==============================] - 0s 16us/step - loss: 0.0168 - acc: 0.9961 - val_loss: 0.0566 - val_acc: 0.9916\n",
            "Epoch 2365/3500\n",
            "4352/4352 [==============================] - 0s 14us/step - loss: 0.0161 - acc: 0.9966 - val_loss: 0.0566 - val_acc: 0.9911\n",
            "Epoch 2366/3500\n",
            "4352/4352 [==============================] - 0s 14us/step - loss: 0.0166 - acc: 0.9963 - val_loss: 0.0558 - val_acc: 0.9911\n",
            "Epoch 2367/3500\n",
            "4352/4352 [==============================] - 0s 15us/step - loss: 0.0181 - acc: 0.9947 - val_loss: 0.0697 - val_acc: 0.9888\n",
            "Epoch 2368/3500\n",
            "4352/4352 [==============================] - 0s 13us/step - loss: 0.0198 - acc: 0.9936 - val_loss: 0.0744 - val_acc: 0.9893\n",
            "Epoch 2369/3500\n",
            "4352/4352 [==============================] - 0s 14us/step - loss: 0.0183 - acc: 0.9954 - val_loss: 0.0651 - val_acc: 0.9897\n",
            "Epoch 2370/3500\n",
            "4352/4352 [==============================] - 0s 15us/step - loss: 0.0193 - acc: 0.9949 - val_loss: 0.0580 - val_acc: 0.9907\n",
            "Epoch 2371/3500\n",
            "4352/4352 [==============================] - 0s 14us/step - loss: 0.0175 - acc: 0.9956 - val_loss: 0.0549 - val_acc: 0.9911\n",
            "Epoch 2372/3500\n",
            "4352/4352 [==============================] - 0s 15us/step - loss: 0.0179 - acc: 0.9954 - val_loss: 0.0547 - val_acc: 0.9921\n",
            "Epoch 2373/3500\n",
            "4352/4352 [==============================] - 0s 15us/step - loss: 0.0171 - acc: 0.9954 - val_loss: 0.0557 - val_acc: 0.9907\n",
            "Epoch 2374/3500\n",
            "4352/4352 [==============================] - 0s 13us/step - loss: 0.0189 - acc: 0.9947 - val_loss: 0.0568 - val_acc: 0.9911\n",
            "Epoch 2375/3500\n",
            "4352/4352 [==============================] - 0s 14us/step - loss: 0.0191 - acc: 0.9952 - val_loss: 0.0634 - val_acc: 0.9897\n",
            "Epoch 2376/3500\n",
            "4352/4352 [==============================] - 0s 14us/step - loss: 0.0182 - acc: 0.9947 - val_loss: 0.0701 - val_acc: 0.9893\n",
            "Epoch 2377/3500\n",
            "4352/4352 [==============================] - 0s 15us/step - loss: 0.0183 - acc: 0.9952 - val_loss: 0.0617 - val_acc: 0.9907\n",
            "Epoch 2378/3500\n",
            "4352/4352 [==============================] - 0s 17us/step - loss: 0.0188 - acc: 0.9943 - val_loss: 0.0544 - val_acc: 0.9921\n",
            "Epoch 2379/3500\n",
            "4352/4352 [==============================] - 0s 14us/step - loss: 0.0184 - acc: 0.9949 - val_loss: 0.0569 - val_acc: 0.9907\n",
            "Epoch 2380/3500\n",
            "4352/4352 [==============================] - 0s 14us/step - loss: 0.0175 - acc: 0.9956 - val_loss: 0.0585 - val_acc: 0.9911\n",
            "Epoch 2381/3500\n",
            "4352/4352 [==============================] - 0s 15us/step - loss: 0.0162 - acc: 0.9961 - val_loss: 0.0569 - val_acc: 0.9902\n",
            "Epoch 2382/3500\n",
            "4352/4352 [==============================] - 0s 14us/step - loss: 0.0170 - acc: 0.9954 - val_loss: 0.0642 - val_acc: 0.9897\n",
            "Epoch 2383/3500\n",
            "4352/4352 [==============================] - 0s 14us/step - loss: 0.0164 - acc: 0.9963 - val_loss: 0.0573 - val_acc: 0.9911\n",
            "Epoch 2384/3500\n",
            "4352/4352 [==============================] - 0s 16us/step - loss: 0.0165 - acc: 0.9956 - val_loss: 0.0564 - val_acc: 0.9916\n",
            "Epoch 2385/3500\n",
            "4352/4352 [==============================] - 0s 15us/step - loss: 0.0182 - acc: 0.9952 - val_loss: 0.0545 - val_acc: 0.9902\n",
            "Epoch 2386/3500\n",
            "4352/4352 [==============================] - 0s 14us/step - loss: 0.0181 - acc: 0.9945 - val_loss: 0.0588 - val_acc: 0.9911\n",
            "Epoch 2387/3500\n",
            "4352/4352 [==============================] - 0s 14us/step - loss: 0.0165 - acc: 0.9963 - val_loss: 0.0550 - val_acc: 0.9916\n",
            "Epoch 2388/3500\n",
            "4352/4352 [==============================] - 0s 14us/step - loss: 0.0175 - acc: 0.9949 - val_loss: 0.0595 - val_acc: 0.9907\n",
            "Epoch 2389/3500\n",
            "4352/4352 [==============================] - 0s 14us/step - loss: 0.0173 - acc: 0.9952 - val_loss: 0.0548 - val_acc: 0.9907\n",
            "Epoch 2390/3500\n",
            "4352/4352 [==============================] - 0s 14us/step - loss: 0.0177 - acc: 0.9954 - val_loss: 0.0550 - val_acc: 0.9921\n",
            "Epoch 2391/3500\n",
            "4352/4352 [==============================] - 0s 14us/step - loss: 0.0171 - acc: 0.9959 - val_loss: 0.0556 - val_acc: 0.9902\n",
            "Epoch 2392/3500\n",
            "4352/4352 [==============================] - 0s 15us/step - loss: 0.0201 - acc: 0.9938 - val_loss: 0.0547 - val_acc: 0.9911\n",
            "Epoch 2393/3500\n",
            "4352/4352 [==============================] - 0s 14us/step - loss: 0.0171 - acc: 0.9956 - val_loss: 0.0643 - val_acc: 0.9897\n",
            "Epoch 2394/3500\n",
            "4352/4352 [==============================] - 0s 16us/step - loss: 0.0173 - acc: 0.9952 - val_loss: 0.0625 - val_acc: 0.9907\n",
            "Epoch 2395/3500\n",
            "4352/4352 [==============================] - 0s 13us/step - loss: 0.0160 - acc: 0.9959 - val_loss: 0.0571 - val_acc: 0.9916\n",
            "Epoch 2396/3500\n",
            "4352/4352 [==============================] - 0s 14us/step - loss: 0.0190 - acc: 0.9940 - val_loss: 0.0590 - val_acc: 0.9916\n",
            "Epoch 2397/3500\n",
            "4352/4352 [==============================] - 0s 15us/step - loss: 0.0233 - acc: 0.9936 - val_loss: 0.0575 - val_acc: 0.9907\n",
            "Epoch 2398/3500\n",
            "4352/4352 [==============================] - 0s 13us/step - loss: 0.0182 - acc: 0.9947 - val_loss: 0.0638 - val_acc: 0.9897\n",
            "Epoch 2399/3500\n",
            "4352/4352 [==============================] - 0s 14us/step - loss: 0.0168 - acc: 0.9956 - val_loss: 0.0563 - val_acc: 0.9907\n",
            "Epoch 2400/3500\n",
            "4352/4352 [==============================] - 0s 15us/step - loss: 0.0176 - acc: 0.9952 - val_loss: 0.0563 - val_acc: 0.9907\n",
            "Epoch 2401/3500\n",
            "4352/4352 [==============================] - 0s 14us/step - loss: 0.0169 - acc: 0.9961 - val_loss: 0.0557 - val_acc: 0.9911\n",
            "Epoch 2402/3500\n",
            "4352/4352 [==============================] - 0s 15us/step - loss: 0.0179 - acc: 0.9952 - val_loss: 0.0547 - val_acc: 0.9921\n",
            "Epoch 2403/3500\n",
            "4352/4352 [==============================] - 0s 15us/step - loss: 0.0176 - acc: 0.9949 - val_loss: 0.0562 - val_acc: 0.9916\n",
            "Epoch 2404/3500\n",
            "4352/4352 [==============================] - 0s 14us/step - loss: 0.0168 - acc: 0.9959 - val_loss: 0.0565 - val_acc: 0.9916\n",
            "Epoch 2405/3500\n",
            "4352/4352 [==============================] - 0s 14us/step - loss: 0.0170 - acc: 0.9956 - val_loss: 0.0567 - val_acc: 0.9921\n",
            "Epoch 2406/3500\n",
            "4352/4352 [==============================] - 0s 15us/step - loss: 0.0178 - acc: 0.9952 - val_loss: 0.0557 - val_acc: 0.9921\n",
            "Epoch 2407/3500\n",
            "4352/4352 [==============================] - 0s 14us/step - loss: 0.0166 - acc: 0.9961 - val_loss: 0.0562 - val_acc: 0.9925\n",
            "Epoch 2408/3500\n",
            "4352/4352 [==============================] - 0s 14us/step - loss: 0.0168 - acc: 0.9959 - val_loss: 0.0605 - val_acc: 0.9907\n",
            "Epoch 2409/3500\n",
            "4352/4352 [==============================] - 0s 14us/step - loss: 0.0159 - acc: 0.9961 - val_loss: 0.0568 - val_acc: 0.9916\n",
            "Epoch 2410/3500\n",
            "4352/4352 [==============================] - 0s 15us/step - loss: 0.0166 - acc: 0.9956 - val_loss: 0.0591 - val_acc: 0.9907\n",
            "Epoch 2411/3500\n",
            "4352/4352 [==============================] - 0s 14us/step - loss: 0.0163 - acc: 0.9956 - val_loss: 0.0591 - val_acc: 0.9911\n",
            "Epoch 2412/3500\n",
            "4352/4352 [==============================] - 0s 15us/step - loss: 0.0167 - acc: 0.9959 - val_loss: 0.0561 - val_acc: 0.9925\n",
            "Epoch 2413/3500\n",
            "4352/4352 [==============================] - 0s 15us/step - loss: 0.0164 - acc: 0.9954 - val_loss: 0.0570 - val_acc: 0.9916\n",
            "Epoch 2414/3500\n",
            "4352/4352 [==============================] - 0s 16us/step - loss: 0.0162 - acc: 0.9966 - val_loss: 0.0566 - val_acc: 0.9925\n",
            "Epoch 2415/3500\n",
            "4352/4352 [==============================] - 0s 14us/step - loss: 0.0187 - acc: 0.9947 - val_loss: 0.0556 - val_acc: 0.9916\n",
            "Epoch 2416/3500\n",
            "4352/4352 [==============================] - 0s 15us/step - loss: 0.0209 - acc: 0.9938 - val_loss: 0.0583 - val_acc: 0.9907\n",
            "Epoch 2417/3500\n",
            "4352/4352 [==============================] - 0s 15us/step - loss: 0.0164 - acc: 0.9963 - val_loss: 0.0597 - val_acc: 0.9907\n",
            "Epoch 2418/3500\n",
            "4352/4352 [==============================] - 0s 16us/step - loss: 0.0166 - acc: 0.9963 - val_loss: 0.0601 - val_acc: 0.9902\n",
            "Epoch 2419/3500\n",
            "4352/4352 [==============================] - 0s 14us/step - loss: 0.0163 - acc: 0.9956 - val_loss: 0.0560 - val_acc: 0.9907\n",
            "Epoch 2420/3500\n",
            "4352/4352 [==============================] - 0s 14us/step - loss: 0.0168 - acc: 0.9961 - val_loss: 0.0596 - val_acc: 0.9907\n",
            "Epoch 2421/3500\n",
            "4352/4352 [==============================] - 0s 17us/step - loss: 0.0174 - acc: 0.9954 - val_loss: 0.0653 - val_acc: 0.9907\n",
            "Epoch 2422/3500\n",
            "4352/4352 [==============================] - 0s 14us/step - loss: 0.0183 - acc: 0.9954 - val_loss: 0.0635 - val_acc: 0.9893\n",
            "Epoch 2423/3500\n",
            "4352/4352 [==============================] - 0s 15us/step - loss: 0.0241 - acc: 0.9920 - val_loss: 0.0621 - val_acc: 0.9911\n",
            "Epoch 2424/3500\n",
            "4352/4352 [==============================] - 0s 13us/step - loss: 0.0223 - acc: 0.9940 - val_loss: 0.0605 - val_acc: 0.9907\n",
            "Epoch 2425/3500\n",
            "4352/4352 [==============================] - 0s 16us/step - loss: 0.0201 - acc: 0.9943 - val_loss: 0.0546 - val_acc: 0.9907\n",
            "Epoch 2426/3500\n",
            "4352/4352 [==============================] - 0s 14us/step - loss: 0.0180 - acc: 0.9947 - val_loss: 0.0550 - val_acc: 0.9916\n",
            "Epoch 2427/3500\n",
            "4352/4352 [==============================] - 0s 14us/step - loss: 0.0179 - acc: 0.9952 - val_loss: 0.0577 - val_acc: 0.9916\n",
            "Epoch 2428/3500\n",
            "4352/4352 [==============================] - 0s 17us/step - loss: 0.0187 - acc: 0.9954 - val_loss: 0.0666 - val_acc: 0.9897\n",
            "Epoch 2429/3500\n",
            "4352/4352 [==============================] - 0s 17us/step - loss: 0.0169 - acc: 0.9959 - val_loss: 0.0710 - val_acc: 0.9893\n",
            "Epoch 2430/3500\n",
            "4352/4352 [==============================] - 0s 14us/step - loss: 0.0197 - acc: 0.9936 - val_loss: 0.0656 - val_acc: 0.9893\n",
            "Epoch 2431/3500\n",
            "4352/4352 [==============================] - 0s 14us/step - loss: 0.0170 - acc: 0.9952 - val_loss: 0.0602 - val_acc: 0.9897\n",
            "Epoch 2432/3500\n",
            "4352/4352 [==============================] - 0s 14us/step - loss: 0.0165 - acc: 0.9966 - val_loss: 0.0581 - val_acc: 0.9911\n",
            "Epoch 2433/3500\n",
            "4352/4352 [==============================] - 0s 13us/step - loss: 0.0171 - acc: 0.9954 - val_loss: 0.0584 - val_acc: 0.9916\n",
            "Epoch 2434/3500\n",
            "4352/4352 [==============================] - 0s 16us/step - loss: 0.0169 - acc: 0.9956 - val_loss: 0.0652 - val_acc: 0.9893\n",
            "Epoch 2435/3500\n",
            "4352/4352 [==============================] - 0s 14us/step - loss: 0.0184 - acc: 0.9952 - val_loss: 0.0583 - val_acc: 0.9911\n",
            "Epoch 2436/3500\n",
            "4352/4352 [==============================] - 0s 13us/step - loss: 0.0167 - acc: 0.9956 - val_loss: 0.0548 - val_acc: 0.9916\n",
            "Epoch 2437/3500\n",
            "4352/4352 [==============================] - 0s 15us/step - loss: 0.0173 - acc: 0.9959 - val_loss: 0.0586 - val_acc: 0.9911\n",
            "Epoch 2438/3500\n",
            "4352/4352 [==============================] - 0s 15us/step - loss: 0.0174 - acc: 0.9956 - val_loss: 0.0584 - val_acc: 0.9916\n",
            "Epoch 2439/3500\n",
            "4352/4352 [==============================] - 0s 14us/step - loss: 0.0170 - acc: 0.9954 - val_loss: 0.0570 - val_acc: 0.9911\n",
            "Epoch 2440/3500\n",
            "4352/4352 [==============================] - 0s 22us/step - loss: 0.0160 - acc: 0.9959 - val_loss: 0.0650 - val_acc: 0.9897\n",
            "Epoch 2441/3500\n",
            "4352/4352 [==============================] - 0s 19us/step - loss: 0.0180 - acc: 0.9949 - val_loss: 0.0687 - val_acc: 0.9888\n",
            "Epoch 2442/3500\n",
            "4352/4352 [==============================] - 0s 14us/step - loss: 0.0173 - acc: 0.9956 - val_loss: 0.0683 - val_acc: 0.9897\n",
            "Epoch 2443/3500\n",
            "4352/4352 [==============================] - 0s 14us/step - loss: 0.0167 - acc: 0.9956 - val_loss: 0.0576 - val_acc: 0.9916\n",
            "Epoch 2444/3500\n",
            "4352/4352 [==============================] - 0s 15us/step - loss: 0.0160 - acc: 0.9966 - val_loss: 0.0614 - val_acc: 0.9902\n",
            "Epoch 2445/3500\n",
            "4352/4352 [==============================] - 0s 16us/step - loss: 0.0160 - acc: 0.9966 - val_loss: 0.0599 - val_acc: 0.9907\n",
            "Epoch 2446/3500\n",
            "4352/4352 [==============================] - 0s 14us/step - loss: 0.0157 - acc: 0.9963 - val_loss: 0.0583 - val_acc: 0.9907\n",
            "Epoch 2447/3500\n",
            "4352/4352 [==============================] - 0s 15us/step - loss: 0.0162 - acc: 0.9963 - val_loss: 0.0564 - val_acc: 0.9911\n",
            "Epoch 2448/3500\n",
            "4352/4352 [==============================] - 0s 13us/step - loss: 0.0167 - acc: 0.9956 - val_loss: 0.0553 - val_acc: 0.9925\n",
            "Epoch 2449/3500\n",
            "4352/4352 [==============================] - 0s 13us/step - loss: 0.0166 - acc: 0.9956 - val_loss: 0.0604 - val_acc: 0.9907\n",
            "Epoch 2450/3500\n",
            "4352/4352 [==============================] - 0s 15us/step - loss: 0.0159 - acc: 0.9963 - val_loss: 0.0604 - val_acc: 0.9907\n",
            "Epoch 2451/3500\n",
            "4352/4352 [==============================] - 0s 15us/step - loss: 0.0159 - acc: 0.9961 - val_loss: 0.0676 - val_acc: 0.9897\n",
            "Epoch 2452/3500\n",
            "4352/4352 [==============================] - 0s 15us/step - loss: 0.0167 - acc: 0.9970 - val_loss: 0.0589 - val_acc: 0.9907\n",
            "Epoch 2453/3500\n",
            "4352/4352 [==============================] - 0s 14us/step - loss: 0.0161 - acc: 0.9961 - val_loss: 0.0560 - val_acc: 0.9916\n",
            "Epoch 2454/3500\n",
            "4352/4352 [==============================] - 0s 14us/step - loss: 0.0192 - acc: 0.9949 - val_loss: 0.0558 - val_acc: 0.9916\n",
            "Epoch 2455/3500\n",
            "4352/4352 [==============================] - 0s 13us/step - loss: 0.0194 - acc: 0.9947 - val_loss: 0.0550 - val_acc: 0.9907\n",
            "Epoch 2456/3500\n",
            "4352/4352 [==============================] - 0s 16us/step - loss: 0.0179 - acc: 0.9938 - val_loss: 0.0643 - val_acc: 0.9893\n",
            "Epoch 2457/3500\n",
            "4352/4352 [==============================] - 0s 14us/step - loss: 0.0194 - acc: 0.9949 - val_loss: 0.0612 - val_acc: 0.9897\n",
            "Epoch 2458/3500\n",
            "4352/4352 [==============================] - 0s 15us/step - loss: 0.0205 - acc: 0.9943 - val_loss: 0.0632 - val_acc: 0.9902\n",
            "Epoch 2459/3500\n",
            "4352/4352 [==============================] - 0s 14us/step - loss: 0.0229 - acc: 0.9926 - val_loss: 0.0552 - val_acc: 0.9921\n",
            "Epoch 2460/3500\n",
            "4352/4352 [==============================] - 0s 18us/step - loss: 0.0212 - acc: 0.9938 - val_loss: 0.0605 - val_acc: 0.9907\n",
            "Epoch 2461/3500\n",
            "4352/4352 [==============================] - 0s 15us/step - loss: 0.0183 - acc: 0.9947 - val_loss: 0.0697 - val_acc: 0.9897\n",
            "Epoch 2462/3500\n",
            "4352/4352 [==============================] - 0s 14us/step - loss: 0.0205 - acc: 0.9943 - val_loss: 0.0745 - val_acc: 0.9883\n",
            "Epoch 2463/3500\n",
            "4352/4352 [==============================] - 0s 14us/step - loss: 0.0184 - acc: 0.9947 - val_loss: 0.0569 - val_acc: 0.9911\n",
            "Epoch 2464/3500\n",
            "4352/4352 [==============================] - 0s 20us/step - loss: 0.0180 - acc: 0.9947 - val_loss: 0.0562 - val_acc: 0.9911\n",
            "Epoch 2465/3500\n",
            "4352/4352 [==============================] - 0s 14us/step - loss: 0.0171 - acc: 0.9956 - val_loss: 0.0582 - val_acc: 0.9911\n",
            "Epoch 2466/3500\n",
            "4352/4352 [==============================] - 0s 14us/step - loss: 0.0170 - acc: 0.9956 - val_loss: 0.0600 - val_acc: 0.9902\n",
            "Epoch 2467/3500\n",
            "4352/4352 [==============================] - 0s 15us/step - loss: 0.0164 - acc: 0.9956 - val_loss: 0.0612 - val_acc: 0.9907\n",
            "Epoch 2468/3500\n",
            "4352/4352 [==============================] - 0s 15us/step - loss: 0.0171 - acc: 0.9961 - val_loss: 0.0701 - val_acc: 0.9893\n",
            "Epoch 2469/3500\n",
            "4352/4352 [==============================] - 0s 14us/step - loss: 0.0201 - acc: 0.9938 - val_loss: 0.0747 - val_acc: 0.9879\n",
            "Epoch 2470/3500\n",
            "4352/4352 [==============================] - 0s 13us/step - loss: 0.0205 - acc: 0.9936 - val_loss: 0.0643 - val_acc: 0.9893\n",
            "Epoch 2471/3500\n",
            "4352/4352 [==============================] - 0s 16us/step - loss: 0.0163 - acc: 0.9956 - val_loss: 0.0588 - val_acc: 0.9911\n",
            "Epoch 2472/3500\n",
            "4352/4352 [==============================] - 0s 15us/step - loss: 0.0166 - acc: 0.9963 - val_loss: 0.0566 - val_acc: 0.9916\n",
            "Epoch 2473/3500\n",
            "4352/4352 [==============================] - 0s 13us/step - loss: 0.0168 - acc: 0.9961 - val_loss: 0.0559 - val_acc: 0.9911\n",
            "Epoch 2474/3500\n",
            "4352/4352 [==============================] - 0s 16us/step - loss: 0.0168 - acc: 0.9961 - val_loss: 0.0561 - val_acc: 0.9907\n",
            "Epoch 2475/3500\n",
            "4352/4352 [==============================] - 0s 15us/step - loss: 0.0190 - acc: 0.9938 - val_loss: 0.0572 - val_acc: 0.9921\n",
            "Epoch 2476/3500\n",
            "4352/4352 [==============================] - 0s 15us/step - loss: 0.0160 - acc: 0.9966 - val_loss: 0.0623 - val_acc: 0.9902\n",
            "Epoch 2477/3500\n",
            "4352/4352 [==============================] - 0s 14us/step - loss: 0.0164 - acc: 0.9961 - val_loss: 0.0680 - val_acc: 0.9897\n",
            "Epoch 2478/3500\n",
            "4352/4352 [==============================] - 0s 14us/step - loss: 0.0213 - acc: 0.9938 - val_loss: 0.0577 - val_acc: 0.9916\n",
            "Epoch 2479/3500\n",
            "4352/4352 [==============================] - 0s 14us/step - loss: 0.0247 - acc: 0.9915 - val_loss: 0.0553 - val_acc: 0.9911\n",
            "Epoch 2480/3500\n",
            "4352/4352 [==============================] - 0s 14us/step - loss: 0.0218 - acc: 0.9940 - val_loss: 0.0568 - val_acc: 0.9921\n",
            "Epoch 2481/3500\n",
            "4352/4352 [==============================] - 0s 14us/step - loss: 0.0189 - acc: 0.9947 - val_loss: 0.0553 - val_acc: 0.9921\n",
            "Epoch 2482/3500\n",
            "4352/4352 [==============================] - 0s 13us/step - loss: 0.0178 - acc: 0.9952 - val_loss: 0.0579 - val_acc: 0.9911\n",
            "Epoch 2483/3500\n",
            "4352/4352 [==============================] - 0s 15us/step - loss: 0.0160 - acc: 0.9963 - val_loss: 0.0579 - val_acc: 0.9897\n",
            "Epoch 2484/3500\n",
            "4352/4352 [==============================] - 0s 13us/step - loss: 0.0182 - acc: 0.9949 - val_loss: 0.0607 - val_acc: 0.9902\n",
            "Epoch 2485/3500\n",
            "4352/4352 [==============================] - 0s 14us/step - loss: 0.0158 - acc: 0.9966 - val_loss: 0.0563 - val_acc: 0.9907\n",
            "Epoch 2486/3500\n",
            "4352/4352 [==============================] - 0s 14us/step - loss: 0.0180 - acc: 0.9952 - val_loss: 0.0581 - val_acc: 0.9911\n",
            "Epoch 2487/3500\n",
            "4352/4352 [==============================] - 0s 16us/step - loss: 0.0163 - acc: 0.9961 - val_loss: 0.0562 - val_acc: 0.9916\n",
            "Epoch 2488/3500\n",
            "4352/4352 [==============================] - 0s 14us/step - loss: 0.0164 - acc: 0.9963 - val_loss: 0.0589 - val_acc: 0.9911\n",
            "Epoch 2489/3500\n",
            "4352/4352 [==============================] - 0s 15us/step - loss: 0.0171 - acc: 0.9952 - val_loss: 0.0733 - val_acc: 0.9883\n",
            "Epoch 2490/3500\n",
            "4352/4352 [==============================] - 0s 14us/step - loss: 0.0174 - acc: 0.9949 - val_loss: 0.0609 - val_acc: 0.9907\n",
            "Epoch 2491/3500\n",
            "4352/4352 [==============================] - 0s 16us/step - loss: 0.0164 - acc: 0.9963 - val_loss: 0.0626 - val_acc: 0.9897\n",
            "Epoch 2492/3500\n",
            "4352/4352 [==============================] - 0s 14us/step - loss: 0.0171 - acc: 0.9959 - val_loss: 0.0598 - val_acc: 0.9907\n",
            "Epoch 2493/3500\n",
            "4352/4352 [==============================] - 0s 14us/step - loss: 0.0161 - acc: 0.9959 - val_loss: 0.0571 - val_acc: 0.9921\n",
            "Epoch 2494/3500\n",
            "4352/4352 [==============================] - 0s 14us/step - loss: 0.0166 - acc: 0.9963 - val_loss: 0.0575 - val_acc: 0.9916\n",
            "Epoch 2495/3500\n",
            "4352/4352 [==============================] - 0s 14us/step - loss: 0.0161 - acc: 0.9963 - val_loss: 0.0601 - val_acc: 0.9902\n",
            "Epoch 2496/3500\n",
            "4352/4352 [==============================] - 0s 14us/step - loss: 0.0161 - acc: 0.9961 - val_loss: 0.0609 - val_acc: 0.9902\n",
            "Epoch 2497/3500\n",
            "4352/4352 [==============================] - 0s 16us/step - loss: 0.0167 - acc: 0.9956 - val_loss: 0.0620 - val_acc: 0.9907\n",
            "Epoch 2498/3500\n",
            "4352/4352 [==============================] - 0s 16us/step - loss: 0.0166 - acc: 0.9959 - val_loss: 0.0602 - val_acc: 0.9907\n",
            "Epoch 2499/3500\n",
            "4352/4352 [==============================] - 0s 12us/step - loss: 0.0176 - acc: 0.9952 - val_loss: 0.0621 - val_acc: 0.9902\n",
            "Epoch 2500/3500\n",
            "4352/4352 [==============================] - 0s 14us/step - loss: 0.0182 - acc: 0.9952 - val_loss: 0.0655 - val_acc: 0.9888\n",
            "Epoch 2501/3500\n",
            "4352/4352 [==============================] - 0s 16us/step - loss: 0.0162 - acc: 0.9959 - val_loss: 0.0641 - val_acc: 0.9907\n",
            "Epoch 2502/3500\n",
            "4352/4352 [==============================] - 0s 14us/step - loss: 0.0162 - acc: 0.9966 - val_loss: 0.0614 - val_acc: 0.9907\n",
            "Epoch 2503/3500\n",
            "4352/4352 [==============================] - 0s 14us/step - loss: 0.0162 - acc: 0.9963 - val_loss: 0.0617 - val_acc: 0.9907\n",
            "Epoch 2504/3500\n",
            "4352/4352 [==============================] - 0s 14us/step - loss: 0.0169 - acc: 0.9961 - val_loss: 0.0573 - val_acc: 0.9921\n",
            "Epoch 2505/3500\n",
            "4352/4352 [==============================] - 0s 14us/step - loss: 0.0175 - acc: 0.9956 - val_loss: 0.0595 - val_acc: 0.9907\n",
            "Epoch 2506/3500\n",
            "4352/4352 [==============================] - 0s 14us/step - loss: 0.0174 - acc: 0.9947 - val_loss: 0.0602 - val_acc: 0.9907\n",
            "Epoch 2507/3500\n",
            "4352/4352 [==============================] - 0s 14us/step - loss: 0.0160 - acc: 0.9961 - val_loss: 0.0565 - val_acc: 0.9907\n",
            "Epoch 2508/3500\n",
            "4352/4352 [==============================] - 0s 13us/step - loss: 0.0174 - acc: 0.9952 - val_loss: 0.0608 - val_acc: 0.9902\n",
            "Epoch 2509/3500\n",
            "4352/4352 [==============================] - 0s 14us/step - loss: 0.0173 - acc: 0.9956 - val_loss: 0.0577 - val_acc: 0.9916\n",
            "Epoch 2510/3500\n",
            "4352/4352 [==============================] - 0s 14us/step - loss: 0.0176 - acc: 0.9954 - val_loss: 0.0566 - val_acc: 0.9916\n",
            "Epoch 2511/3500\n",
            "4352/4352 [==============================] - 0s 14us/step - loss: 0.0192 - acc: 0.9943 - val_loss: 0.0573 - val_acc: 0.9907\n",
            "Epoch 2512/3500\n",
            "4352/4352 [==============================] - 0s 15us/step - loss: 0.0216 - acc: 0.9929 - val_loss: 0.0557 - val_acc: 0.9911\n",
            "Epoch 2513/3500\n",
            "4352/4352 [==============================] - 0s 15us/step - loss: 0.0246 - acc: 0.9926 - val_loss: 0.0571 - val_acc: 0.9911\n",
            "Epoch 2514/3500\n",
            "4352/4352 [==============================] - 0s 14us/step - loss: 0.0317 - acc: 0.9906 - val_loss: 0.0575 - val_acc: 0.9916\n",
            "Epoch 2515/3500\n",
            "4352/4352 [==============================] - 0s 14us/step - loss: 0.0278 - acc: 0.9908 - val_loss: 0.0562 - val_acc: 0.9911\n",
            "Epoch 2516/3500\n",
            "4352/4352 [==============================] - 0s 15us/step - loss: 0.0197 - acc: 0.9945 - val_loss: 0.0574 - val_acc: 0.9916\n",
            "Epoch 2517/3500\n",
            "4352/4352 [==============================] - 0s 15us/step - loss: 0.0172 - acc: 0.9956 - val_loss: 0.0729 - val_acc: 0.9888\n",
            "Epoch 2518/3500\n",
            "4352/4352 [==============================] - 0s 13us/step - loss: 0.0189 - acc: 0.9945 - val_loss: 0.0646 - val_acc: 0.9897\n",
            "Epoch 2519/3500\n",
            "4352/4352 [==============================] - 0s 16us/step - loss: 0.0174 - acc: 0.9949 - val_loss: 0.0786 - val_acc: 0.9865\n",
            "Epoch 2520/3500\n",
            "4352/4352 [==============================] - 0s 15us/step - loss: 0.0190 - acc: 0.9947 - val_loss: 0.0602 - val_acc: 0.9907\n",
            "Epoch 2521/3500\n",
            "4352/4352 [==============================] - 0s 15us/step - loss: 0.0199 - acc: 0.9943 - val_loss: 0.0561 - val_acc: 0.9921\n",
            "Epoch 2522/3500\n",
            "4352/4352 [==============================] - 0s 15us/step - loss: 0.0165 - acc: 0.9956 - val_loss: 0.0577 - val_acc: 0.9911\n",
            "Epoch 2523/3500\n",
            "4352/4352 [==============================] - 0s 14us/step - loss: 0.0166 - acc: 0.9961 - val_loss: 0.0592 - val_acc: 0.9911\n",
            "Epoch 2524/3500\n",
            "4352/4352 [==============================] - 0s 13us/step - loss: 0.0160 - acc: 0.9963 - val_loss: 0.0679 - val_acc: 0.9897\n",
            "Epoch 2525/3500\n",
            "4352/4352 [==============================] - 0s 14us/step - loss: 0.0172 - acc: 0.9956 - val_loss: 0.0589 - val_acc: 0.9911\n",
            "Epoch 2526/3500\n",
            "4352/4352 [==============================] - 0s 13us/step - loss: 0.0160 - acc: 0.9961 - val_loss: 0.0574 - val_acc: 0.9916\n",
            "Epoch 2527/3500\n",
            "4352/4352 [==============================] - 0s 14us/step - loss: 0.0167 - acc: 0.9956 - val_loss: 0.0597 - val_acc: 0.9911\n",
            "Epoch 2528/3500\n",
            "4352/4352 [==============================] - 0s 14us/step - loss: 0.0159 - acc: 0.9961 - val_loss: 0.0652 - val_acc: 0.9907\n",
            "Epoch 2529/3500\n",
            "4352/4352 [==============================] - 0s 13us/step - loss: 0.0156 - acc: 0.9961 - val_loss: 0.0576 - val_acc: 0.9921\n",
            "Epoch 2530/3500\n",
            "4352/4352 [==============================] - 0s 13us/step - loss: 0.0163 - acc: 0.9963 - val_loss: 0.0666 - val_acc: 0.9897\n",
            "Epoch 2531/3500\n",
            "4352/4352 [==============================] - 0s 12us/step - loss: 0.0175 - acc: 0.9961 - val_loss: 0.0711 - val_acc: 0.9897\n",
            "Epoch 2532/3500\n",
            "4352/4352 [==============================] - 0s 13us/step - loss: 0.0158 - acc: 0.9956 - val_loss: 0.0618 - val_acc: 0.9902\n",
            "Epoch 2533/3500\n",
            "4352/4352 [==============================] - 0s 14us/step - loss: 0.0158 - acc: 0.9963 - val_loss: 0.0587 - val_acc: 0.9916\n",
            "Epoch 2534/3500\n",
            "4352/4352 [==============================] - 0s 14us/step - loss: 0.0165 - acc: 0.9954 - val_loss: 0.0696 - val_acc: 0.9893\n",
            "Epoch 2535/3500\n",
            "4352/4352 [==============================] - 0s 16us/step - loss: 0.0181 - acc: 0.9952 - val_loss: 0.0654 - val_acc: 0.9897\n",
            "Epoch 2536/3500\n",
            "4352/4352 [==============================] - 0s 14us/step - loss: 0.0179 - acc: 0.9949 - val_loss: 0.0730 - val_acc: 0.9888\n",
            "Epoch 2537/3500\n",
            "4352/4352 [==============================] - 0s 14us/step - loss: 0.0231 - acc: 0.9936 - val_loss: 0.0661 - val_acc: 0.9902\n",
            "Epoch 2538/3500\n",
            "4352/4352 [==============================] - 0s 14us/step - loss: 0.0160 - acc: 0.9959 - val_loss: 0.0565 - val_acc: 0.9921\n",
            "Epoch 2539/3500\n",
            "4352/4352 [==============================] - 0s 13us/step - loss: 0.0170 - acc: 0.9952 - val_loss: 0.0571 - val_acc: 0.9907\n",
            "Epoch 2540/3500\n",
            "4352/4352 [==============================] - 0s 14us/step - loss: 0.0180 - acc: 0.9961 - val_loss: 0.0566 - val_acc: 0.9930\n",
            "Epoch 2541/3500\n",
            "4352/4352 [==============================] - 0s 13us/step - loss: 0.0176 - acc: 0.9961 - val_loss: 0.0585 - val_acc: 0.9911\n",
            "Epoch 2542/3500\n",
            "4352/4352 [==============================] - 0s 14us/step - loss: 0.0196 - acc: 0.9938 - val_loss: 0.0723 - val_acc: 0.9893\n",
            "Epoch 2543/3500\n",
            "4352/4352 [==============================] - 0s 15us/step - loss: 0.0190 - acc: 0.9943 - val_loss: 0.0797 - val_acc: 0.9855\n",
            "Epoch 2544/3500\n",
            "4352/4352 [==============================] - 0s 14us/step - loss: 0.0204 - acc: 0.9933 - val_loss: 0.0754 - val_acc: 0.9888\n",
            "Epoch 2545/3500\n",
            "4352/4352 [==============================] - 0s 13us/step - loss: 0.0173 - acc: 0.9954 - val_loss: 0.0611 - val_acc: 0.9902\n",
            "Epoch 2546/3500\n",
            "4352/4352 [==============================] - 0s 15us/step - loss: 0.0162 - acc: 0.9959 - val_loss: 0.0580 - val_acc: 0.9911\n",
            "Epoch 2547/3500\n",
            "4352/4352 [==============================] - 0s 13us/step - loss: 0.0166 - acc: 0.9956 - val_loss: 0.0604 - val_acc: 0.9907\n",
            "Epoch 2548/3500\n",
            "4352/4352 [==============================] - 0s 13us/step - loss: 0.0167 - acc: 0.9954 - val_loss: 0.0657 - val_acc: 0.9893\n",
            "Epoch 2549/3500\n",
            "4352/4352 [==============================] - 0s 13us/step - loss: 0.0164 - acc: 0.9956 - val_loss: 0.0624 - val_acc: 0.9902\n",
            "Epoch 2550/3500\n",
            "4352/4352 [==============================] - 0s 14us/step - loss: 0.0171 - acc: 0.9959 - val_loss: 0.0579 - val_acc: 0.9916\n",
            "Epoch 2551/3500\n",
            "4352/4352 [==============================] - 0s 16us/step - loss: 0.0164 - acc: 0.9959 - val_loss: 0.0596 - val_acc: 0.9911\n",
            "Epoch 2552/3500\n",
            "4352/4352 [==============================] - 0s 14us/step - loss: 0.0159 - acc: 0.9968 - val_loss: 0.0571 - val_acc: 0.9921\n",
            "Epoch 2553/3500\n",
            "4352/4352 [==============================] - 0s 14us/step - loss: 0.0175 - acc: 0.9959 - val_loss: 0.0564 - val_acc: 0.9916\n",
            "Epoch 2554/3500\n",
            "4352/4352 [==============================] - 0s 15us/step - loss: 0.0163 - acc: 0.9961 - val_loss: 0.0579 - val_acc: 0.9911\n",
            "Epoch 2555/3500\n",
            "4352/4352 [==============================] - 0s 14us/step - loss: 0.0165 - acc: 0.9961 - val_loss: 0.0635 - val_acc: 0.9911\n",
            "Epoch 2556/3500\n",
            "4352/4352 [==============================] - 0s 15us/step - loss: 0.0173 - acc: 0.9954 - val_loss: 0.0717 - val_acc: 0.9893\n",
            "Epoch 2557/3500\n",
            "4352/4352 [==============================] - 0s 13us/step - loss: 0.0198 - acc: 0.9936 - val_loss: 0.0799 - val_acc: 0.9869\n",
            "Epoch 2558/3500\n",
            "4352/4352 [==============================] - 0s 13us/step - loss: 0.0241 - acc: 0.9933 - val_loss: 0.0837 - val_acc: 0.9837\n",
            "Epoch 2559/3500\n",
            "4352/4352 [==============================] - 0s 13us/step - loss: 0.0265 - acc: 0.9938 - val_loss: 0.0764 - val_acc: 0.9869\n",
            "Epoch 2560/3500\n",
            "4352/4352 [==============================] - 0s 15us/step - loss: 0.0212 - acc: 0.9940 - val_loss: 0.0549 - val_acc: 0.9911\n",
            "Epoch 2561/3500\n",
            "4352/4352 [==============================] - 0s 15us/step - loss: 0.0209 - acc: 0.9940 - val_loss: 0.0600 - val_acc: 0.9911\n",
            "Epoch 2562/3500\n",
            "4352/4352 [==============================] - 0s 15us/step - loss: 0.0222 - acc: 0.9940 - val_loss: 0.0561 - val_acc: 0.9897\n",
            "Epoch 2563/3500\n",
            "4352/4352 [==============================] - 0s 14us/step - loss: 0.0209 - acc: 0.9938 - val_loss: 0.0579 - val_acc: 0.9902\n",
            "Epoch 2564/3500\n",
            "4352/4352 [==============================] - 0s 14us/step - loss: 0.0201 - acc: 0.9938 - val_loss: 0.0571 - val_acc: 0.9897\n",
            "Epoch 2565/3500\n",
            "4352/4352 [==============================] - 0s 14us/step - loss: 0.0237 - acc: 0.9940 - val_loss: 0.0639 - val_acc: 0.9897\n",
            "Epoch 2566/3500\n",
            "4352/4352 [==============================] - 0s 15us/step - loss: 0.0206 - acc: 0.9938 - val_loss: 0.0641 - val_acc: 0.9893\n",
            "Epoch 2567/3500\n",
            "4352/4352 [==============================] - 0s 15us/step - loss: 0.0186 - acc: 0.9954 - val_loss: 0.0761 - val_acc: 0.9874\n",
            "Epoch 2568/3500\n",
            "4352/4352 [==============================] - 0s 15us/step - loss: 0.0198 - acc: 0.9940 - val_loss: 0.0637 - val_acc: 0.9897\n",
            "Epoch 2569/3500\n",
            "4352/4352 [==============================] - 0s 14us/step - loss: 0.0229 - acc: 0.9931 - val_loss: 0.0558 - val_acc: 0.9911\n",
            "Epoch 2570/3500\n",
            "4352/4352 [==============================] - 0s 14us/step - loss: 0.0177 - acc: 0.9947 - val_loss: 0.0576 - val_acc: 0.9911\n",
            "Epoch 2571/3500\n",
            "4352/4352 [==============================] - 0s 13us/step - loss: 0.0173 - acc: 0.9963 - val_loss: 0.0556 - val_acc: 0.9916\n",
            "Epoch 2572/3500\n",
            "4352/4352 [==============================] - 0s 13us/step - loss: 0.0173 - acc: 0.9954 - val_loss: 0.0622 - val_acc: 0.9902\n",
            "Epoch 2573/3500\n",
            "4352/4352 [==============================] - 0s 13us/step - loss: 0.0163 - acc: 0.9968 - val_loss: 0.0646 - val_acc: 0.9893\n",
            "Epoch 2574/3500\n",
            "4352/4352 [==============================] - 0s 13us/step - loss: 0.0179 - acc: 0.9947 - val_loss: 0.0618 - val_acc: 0.9907\n",
            "Epoch 2575/3500\n",
            "4352/4352 [==============================] - 0s 15us/step - loss: 0.0177 - acc: 0.9943 - val_loss: 0.0557 - val_acc: 0.9911\n",
            "Epoch 2576/3500\n",
            "4352/4352 [==============================] - 0s 14us/step - loss: 0.0184 - acc: 0.9945 - val_loss: 0.0566 - val_acc: 0.9911\n",
            "Epoch 2577/3500\n",
            "4352/4352 [==============================] - 0s 14us/step - loss: 0.0173 - acc: 0.9952 - val_loss: 0.0581 - val_acc: 0.9916\n",
            "Epoch 2578/3500\n",
            "4352/4352 [==============================] - 0s 13us/step - loss: 0.0160 - acc: 0.9959 - val_loss: 0.0604 - val_acc: 0.9902\n",
            "Epoch 2579/3500\n",
            "4352/4352 [==============================] - 0s 13us/step - loss: 0.0162 - acc: 0.9959 - val_loss: 0.0615 - val_acc: 0.9907\n",
            "Epoch 2580/3500\n",
            "4352/4352 [==============================] - 0s 14us/step - loss: 0.0161 - acc: 0.9961 - val_loss: 0.0604 - val_acc: 0.9907\n",
            "Epoch 2581/3500\n",
            "4352/4352 [==============================] - 0s 13us/step - loss: 0.0153 - acc: 0.9963 - val_loss: 0.0570 - val_acc: 0.9907\n",
            "Epoch 2582/3500\n",
            "4352/4352 [==============================] - 0s 14us/step - loss: 0.0188 - acc: 0.9943 - val_loss: 0.0560 - val_acc: 0.9907\n",
            "Epoch 2583/3500\n",
            "4352/4352 [==============================] - 0s 13us/step - loss: 0.0244 - acc: 0.9920 - val_loss: 0.0571 - val_acc: 0.9911\n",
            "Epoch 2584/3500\n",
            "4352/4352 [==============================] - 0s 15us/step - loss: 0.0203 - acc: 0.9945 - val_loss: 0.0592 - val_acc: 0.9911\n",
            "Epoch 2585/3500\n",
            "4352/4352 [==============================] - 0s 16us/step - loss: 0.0168 - acc: 0.9959 - val_loss: 0.0611 - val_acc: 0.9907\n",
            "Epoch 2586/3500\n",
            "4352/4352 [==============================] - 0s 14us/step - loss: 0.0167 - acc: 0.9954 - val_loss: 0.0643 - val_acc: 0.9893\n",
            "Epoch 2587/3500\n",
            "4352/4352 [==============================] - 0s 14us/step - loss: 0.0168 - acc: 0.9959 - val_loss: 0.0579 - val_acc: 0.9916\n",
            "Epoch 2588/3500\n",
            "4352/4352 [==============================] - 0s 14us/step - loss: 0.0163 - acc: 0.9959 - val_loss: 0.0569 - val_acc: 0.9907\n",
            "Epoch 2589/3500\n",
            "4352/4352 [==============================] - 0s 13us/step - loss: 0.0163 - acc: 0.9961 - val_loss: 0.0588 - val_acc: 0.9911\n",
            "Epoch 2590/3500\n",
            "4352/4352 [==============================] - 0s 13us/step - loss: 0.0157 - acc: 0.9963 - val_loss: 0.0611 - val_acc: 0.9907\n",
            "Epoch 2591/3500\n",
            "4352/4352 [==============================] - 0s 14us/step - loss: 0.0155 - acc: 0.9961 - val_loss: 0.0637 - val_acc: 0.9907\n",
            "Epoch 2592/3500\n",
            "4352/4352 [==============================] - 0s 13us/step - loss: 0.0157 - acc: 0.9961 - val_loss: 0.0578 - val_acc: 0.9911\n",
            "Epoch 2593/3500\n",
            "4352/4352 [==============================] - 0s 15us/step - loss: 0.0160 - acc: 0.9966 - val_loss: 0.0568 - val_acc: 0.9925\n",
            "Epoch 2594/3500\n",
            "4352/4352 [==============================] - 0s 14us/step - loss: 0.0174 - acc: 0.9954 - val_loss: 0.0593 - val_acc: 0.9911\n",
            "Epoch 2595/3500\n",
            "4352/4352 [==============================] - 0s 14us/step - loss: 0.0173 - acc: 0.9954 - val_loss: 0.0598 - val_acc: 0.9907\n",
            "Epoch 2596/3500\n",
            "4352/4352 [==============================] - 0s 14us/step - loss: 0.0183 - acc: 0.9949 - val_loss: 0.0653 - val_acc: 0.9897\n",
            "Epoch 2597/3500\n",
            "4352/4352 [==============================] - 0s 14us/step - loss: 0.0165 - acc: 0.9952 - val_loss: 0.0666 - val_acc: 0.9893\n",
            "Epoch 2598/3500\n",
            "4352/4352 [==============================] - 0s 13us/step - loss: 0.0177 - acc: 0.9956 - val_loss: 0.0559 - val_acc: 0.9921\n",
            "Epoch 2599/3500\n",
            "4352/4352 [==============================] - 0s 14us/step - loss: 0.0162 - acc: 0.9966 - val_loss: 0.0591 - val_acc: 0.9911\n",
            "Epoch 2600/3500\n",
            "4352/4352 [==============================] - 0s 14us/step - loss: 0.0155 - acc: 0.9963 - val_loss: 0.0571 - val_acc: 0.9911\n",
            "Epoch 2601/3500\n",
            "4352/4352 [==============================] - 0s 15us/step - loss: 0.0157 - acc: 0.9959 - val_loss: 0.0592 - val_acc: 0.9911\n",
            "Epoch 2602/3500\n",
            "4352/4352 [==============================] - 0s 13us/step - loss: 0.0159 - acc: 0.9963 - val_loss: 0.0617 - val_acc: 0.9907\n",
            "Epoch 2603/3500\n",
            "4352/4352 [==============================] - 0s 13us/step - loss: 0.0163 - acc: 0.9956 - val_loss: 0.0571 - val_acc: 0.9916\n",
            "Epoch 2604/3500\n",
            "4352/4352 [==============================] - 0s 14us/step - loss: 0.0161 - acc: 0.9961 - val_loss: 0.0557 - val_acc: 0.9907\n",
            "Epoch 2605/3500\n",
            "4352/4352 [==============================] - 0s 15us/step - loss: 0.0198 - acc: 0.9945 - val_loss: 0.0583 - val_acc: 0.9911\n",
            "Epoch 2606/3500\n",
            "4352/4352 [==============================] - 0s 15us/step - loss: 0.0171 - acc: 0.9956 - val_loss: 0.0696 - val_acc: 0.9893\n",
            "Epoch 2607/3500\n",
            "4352/4352 [==============================] - 0s 20us/step - loss: 0.0205 - acc: 0.9945 - val_loss: 0.0724 - val_acc: 0.9888\n",
            "Epoch 2608/3500\n",
            "4352/4352 [==============================] - 0s 15us/step - loss: 0.0223 - acc: 0.9943 - val_loss: 0.0590 - val_acc: 0.9916\n",
            "Epoch 2609/3500\n",
            "4352/4352 [==============================] - 0s 15us/step - loss: 0.0196 - acc: 0.9949 - val_loss: 0.0573 - val_acc: 0.9916\n",
            "Epoch 2610/3500\n",
            "4352/4352 [==============================] - 0s 14us/step - loss: 0.0207 - acc: 0.9945 - val_loss: 0.0570 - val_acc: 0.9907\n",
            "Epoch 2611/3500\n",
            "4352/4352 [==============================] - 0s 14us/step - loss: 0.0175 - acc: 0.9954 - val_loss: 0.0567 - val_acc: 0.9911\n",
            "Epoch 2612/3500\n",
            "4352/4352 [==============================] - 0s 14us/step - loss: 0.0181 - acc: 0.9940 - val_loss: 0.0652 - val_acc: 0.9902\n",
            "Epoch 2613/3500\n",
            "4352/4352 [==============================] - 0s 15us/step - loss: 0.0162 - acc: 0.9959 - val_loss: 0.0618 - val_acc: 0.9911\n",
            "Epoch 2614/3500\n",
            "4352/4352 [==============================] - 0s 15us/step - loss: 0.0158 - acc: 0.9966 - val_loss: 0.0629 - val_acc: 0.9902\n",
            "Epoch 2615/3500\n",
            "4352/4352 [==============================] - 0s 15us/step - loss: 0.0156 - acc: 0.9956 - val_loss: 0.0583 - val_acc: 0.9916\n",
            "Epoch 2616/3500\n",
            "4352/4352 [==============================] - 0s 16us/step - loss: 0.0163 - acc: 0.9966 - val_loss: 0.0580 - val_acc: 0.9916\n",
            "Epoch 2617/3500\n",
            "4352/4352 [==============================] - 0s 13us/step - loss: 0.0163 - acc: 0.9961 - val_loss: 0.0696 - val_acc: 0.9893\n",
            "Epoch 2618/3500\n",
            "4352/4352 [==============================] - 0s 14us/step - loss: 0.0187 - acc: 0.9945 - val_loss: 0.0612 - val_acc: 0.9902\n",
            "Epoch 2619/3500\n",
            "4352/4352 [==============================] - 0s 15us/step - loss: 0.0165 - acc: 0.9954 - val_loss: 0.0648 - val_acc: 0.9911\n",
            "Epoch 2620/3500\n",
            "4352/4352 [==============================] - 0s 14us/step - loss: 0.0161 - acc: 0.9961 - val_loss: 0.0574 - val_acc: 0.9916\n",
            "Epoch 2621/3500\n",
            "4352/4352 [==============================] - 0s 13us/step - loss: 0.0159 - acc: 0.9968 - val_loss: 0.0570 - val_acc: 0.9916\n",
            "Epoch 2622/3500\n",
            "4352/4352 [==============================] - 0s 14us/step - loss: 0.0153 - acc: 0.9963 - val_loss: 0.0654 - val_acc: 0.9897\n",
            "Epoch 2623/3500\n",
            "4352/4352 [==============================] - 0s 14us/step - loss: 0.0172 - acc: 0.9952 - val_loss: 0.0723 - val_acc: 0.9893\n",
            "Epoch 2624/3500\n",
            "4352/4352 [==============================] - 0s 13us/step - loss: 0.0206 - acc: 0.9940 - val_loss: 0.0603 - val_acc: 0.9907\n",
            "Epoch 2625/3500\n",
            "4352/4352 [==============================] - 0s 13us/step - loss: 0.0187 - acc: 0.9949 - val_loss: 0.0568 - val_acc: 0.9907\n",
            "Epoch 2626/3500\n",
            "4352/4352 [==============================] - 0s 14us/step - loss: 0.0185 - acc: 0.9947 - val_loss: 0.0566 - val_acc: 0.9916\n",
            "Epoch 2627/3500\n",
            "4352/4352 [==============================] - 0s 14us/step - loss: 0.0202 - acc: 0.9949 - val_loss: 0.0569 - val_acc: 0.9902\n",
            "Epoch 2628/3500\n",
            "4352/4352 [==============================] - 0s 14us/step - loss: 0.0167 - acc: 0.9956 - val_loss: 0.0577 - val_acc: 0.9911\n",
            "Epoch 2629/3500\n",
            "4352/4352 [==============================] - 0s 14us/step - loss: 0.0162 - acc: 0.9952 - val_loss: 0.0591 - val_acc: 0.9911\n",
            "Epoch 2630/3500\n",
            "4352/4352 [==============================] - 0s 14us/step - loss: 0.0161 - acc: 0.9961 - val_loss: 0.0565 - val_acc: 0.9911\n",
            "Epoch 2631/3500\n",
            "4352/4352 [==============================] - 0s 14us/step - loss: 0.0158 - acc: 0.9959 - val_loss: 0.0623 - val_acc: 0.9902\n",
            "Epoch 2632/3500\n",
            "4352/4352 [==============================] - 0s 16us/step - loss: 0.0156 - acc: 0.9966 - val_loss: 0.0649 - val_acc: 0.9907\n",
            "Epoch 2633/3500\n",
            "4352/4352 [==============================] - 0s 14us/step - loss: 0.0184 - acc: 0.9945 - val_loss: 0.0605 - val_acc: 0.9907\n",
            "Epoch 2634/3500\n",
            "4352/4352 [==============================] - 0s 14us/step - loss: 0.0178 - acc: 0.9945 - val_loss: 0.0589 - val_acc: 0.9902\n",
            "Epoch 2635/3500\n",
            "4352/4352 [==============================] - 0s 14us/step - loss: 0.0165 - acc: 0.9956 - val_loss: 0.0603 - val_acc: 0.9907\n",
            "Epoch 2636/3500\n",
            "4352/4352 [==============================] - 0s 13us/step - loss: 0.0161 - acc: 0.9959 - val_loss: 0.0582 - val_acc: 0.9911\n",
            "Epoch 2637/3500\n",
            "4352/4352 [==============================] - 0s 14us/step - loss: 0.0152 - acc: 0.9970 - val_loss: 0.0622 - val_acc: 0.9907\n",
            "Epoch 2638/3500\n",
            "4352/4352 [==============================] - 0s 15us/step - loss: 0.0162 - acc: 0.9959 - val_loss: 0.0635 - val_acc: 0.9907\n",
            "Epoch 2639/3500\n",
            "4352/4352 [==============================] - 0s 13us/step - loss: 0.0157 - acc: 0.9961 - val_loss: 0.0585 - val_acc: 0.9911\n",
            "Epoch 2640/3500\n",
            "4352/4352 [==============================] - 0s 14us/step - loss: 0.0167 - acc: 0.9949 - val_loss: 0.0641 - val_acc: 0.9897\n",
            "Epoch 2641/3500\n",
            "4352/4352 [==============================] - 0s 14us/step - loss: 0.0169 - acc: 0.9963 - val_loss: 0.0585 - val_acc: 0.9921\n",
            "Epoch 2642/3500\n",
            "4352/4352 [==============================] - 0s 15us/step - loss: 0.0192 - acc: 0.9947 - val_loss: 0.0638 - val_acc: 0.9907\n",
            "Epoch 2643/3500\n",
            "4352/4352 [==============================] - 0s 15us/step - loss: 0.0167 - acc: 0.9954 - val_loss: 0.0624 - val_acc: 0.9907\n",
            "Epoch 2644/3500\n",
            "4352/4352 [==============================] - 0s 13us/step - loss: 0.0170 - acc: 0.9963 - val_loss: 0.0588 - val_acc: 0.9907\n",
            "Epoch 2645/3500\n",
            "4352/4352 [==============================] - 0s 12us/step - loss: 0.0165 - acc: 0.9952 - val_loss: 0.0588 - val_acc: 0.9916\n",
            "Epoch 2646/3500\n",
            "4352/4352 [==============================] - 0s 14us/step - loss: 0.0158 - acc: 0.9959 - val_loss: 0.0604 - val_acc: 0.9907\n",
            "Epoch 2647/3500\n",
            "4352/4352 [==============================] - 0s 15us/step - loss: 0.0154 - acc: 0.9966 - val_loss: 0.0599 - val_acc: 0.9916\n",
            "Epoch 2648/3500\n",
            "4352/4352 [==============================] - 0s 16us/step - loss: 0.0165 - acc: 0.9959 - val_loss: 0.0600 - val_acc: 0.9911\n",
            "Epoch 2649/3500\n",
            "4352/4352 [==============================] - 0s 13us/step - loss: 0.0167 - acc: 0.9959 - val_loss: 0.0595 - val_acc: 0.9916\n",
            "Epoch 2650/3500\n",
            "4352/4352 [==============================] - 0s 14us/step - loss: 0.0165 - acc: 0.9963 - val_loss: 0.0597 - val_acc: 0.9911\n",
            "Epoch 2651/3500\n",
            "4352/4352 [==============================] - 0s 13us/step - loss: 0.0171 - acc: 0.9954 - val_loss: 0.0596 - val_acc: 0.9907\n",
            "Epoch 2652/3500\n",
            "4352/4352 [==============================] - 0s 14us/step - loss: 0.0178 - acc: 0.9952 - val_loss: 0.0570 - val_acc: 0.9902\n",
            "Epoch 2653/3500\n",
            "4352/4352 [==============================] - 0s 14us/step - loss: 0.0185 - acc: 0.9947 - val_loss: 0.0572 - val_acc: 0.9911\n",
            "Epoch 2654/3500\n",
            "4352/4352 [==============================] - 0s 17us/step - loss: 0.0171 - acc: 0.9959 - val_loss: 0.0582 - val_acc: 0.9902\n",
            "Epoch 2655/3500\n",
            "4352/4352 [==============================] - 0s 14us/step - loss: 0.0160 - acc: 0.9963 - val_loss: 0.0588 - val_acc: 0.9911\n",
            "Epoch 2656/3500\n",
            "4352/4352 [==============================] - 0s 14us/step - loss: 0.0171 - acc: 0.9954 - val_loss: 0.0615 - val_acc: 0.9911\n",
            "Epoch 2657/3500\n",
            "4352/4352 [==============================] - 0s 15us/step - loss: 0.0168 - acc: 0.9959 - val_loss: 0.0688 - val_acc: 0.9897\n",
            "Epoch 2658/3500\n",
            "4352/4352 [==============================] - 0s 13us/step - loss: 0.0191 - acc: 0.9945 - val_loss: 0.0765 - val_acc: 0.9883\n",
            "Epoch 2659/3500\n",
            "4352/4352 [==============================] - 0s 15us/step - loss: 0.0192 - acc: 0.9943 - val_loss: 0.0669 - val_acc: 0.9893\n",
            "Epoch 2660/3500\n",
            "4352/4352 [==============================] - 0s 14us/step - loss: 0.0182 - acc: 0.9949 - val_loss: 0.0622 - val_acc: 0.9902\n",
            "Epoch 2661/3500\n",
            "4352/4352 [==============================] - 0s 14us/step - loss: 0.0169 - acc: 0.9949 - val_loss: 0.0558 - val_acc: 0.9911\n",
            "Epoch 2662/3500\n",
            "4352/4352 [==============================] - 0s 15us/step - loss: 0.0202 - acc: 0.9943 - val_loss: 0.0577 - val_acc: 0.9911\n",
            "Epoch 2663/3500\n",
            "4352/4352 [==============================] - 0s 16us/step - loss: 0.0162 - acc: 0.9959 - val_loss: 0.0595 - val_acc: 0.9911\n",
            "Epoch 2664/3500\n",
            "4352/4352 [==============================] - 0s 16us/step - loss: 0.0162 - acc: 0.9956 - val_loss: 0.0591 - val_acc: 0.9916\n",
            "Epoch 2665/3500\n",
            "4352/4352 [==============================] - 0s 13us/step - loss: 0.0153 - acc: 0.9966 - val_loss: 0.0598 - val_acc: 0.9911\n",
            "Epoch 2666/3500\n",
            "4352/4352 [==============================] - 0s 15us/step - loss: 0.0167 - acc: 0.9952 - val_loss: 0.0597 - val_acc: 0.9916\n",
            "Epoch 2667/3500\n",
            "4352/4352 [==============================] - 0s 16us/step - loss: 0.0173 - acc: 0.9956 - val_loss: 0.0572 - val_acc: 0.9911\n",
            "Epoch 2668/3500\n",
            "4352/4352 [==============================] - 0s 15us/step - loss: 0.0220 - acc: 0.9938 - val_loss: 0.0571 - val_acc: 0.9916\n",
            "Epoch 2669/3500\n",
            "4352/4352 [==============================] - 0s 15us/step - loss: 0.0184 - acc: 0.9952 - val_loss: 0.0555 - val_acc: 0.9916\n",
            "Epoch 2670/3500\n",
            "4352/4352 [==============================] - 0s 14us/step - loss: 0.0200 - acc: 0.9945 - val_loss: 0.0588 - val_acc: 0.9916\n",
            "Epoch 2671/3500\n",
            "4352/4352 [==============================] - 0s 14us/step - loss: 0.0170 - acc: 0.9952 - val_loss: 0.0666 - val_acc: 0.9897\n",
            "Epoch 2672/3500\n",
            "4352/4352 [==============================] - 0s 15us/step - loss: 0.0165 - acc: 0.9959 - val_loss: 0.0648 - val_acc: 0.9902\n",
            "Epoch 2673/3500\n",
            "4352/4352 [==============================] - 0s 15us/step - loss: 0.0158 - acc: 0.9966 - val_loss: 0.0653 - val_acc: 0.9902\n",
            "Epoch 2674/3500\n",
            "4352/4352 [==============================] - 0s 16us/step - loss: 0.0162 - acc: 0.9954 - val_loss: 0.0635 - val_acc: 0.9902\n",
            "Epoch 2675/3500\n",
            "4352/4352 [==============================] - 0s 15us/step - loss: 0.0156 - acc: 0.9959 - val_loss: 0.0580 - val_acc: 0.9916\n",
            "Epoch 2676/3500\n",
            "4352/4352 [==============================] - 0s 14us/step - loss: 0.0164 - acc: 0.9959 - val_loss: 0.0588 - val_acc: 0.9916\n",
            "Epoch 2677/3500\n",
            "4352/4352 [==============================] - 0s 14us/step - loss: 0.0160 - acc: 0.9961 - val_loss: 0.0598 - val_acc: 0.9907\n",
            "Epoch 2678/3500\n",
            "4352/4352 [==============================] - 0s 15us/step - loss: 0.0157 - acc: 0.9963 - val_loss: 0.0623 - val_acc: 0.9907\n",
            "Epoch 2679/3500\n",
            "4352/4352 [==============================] - 0s 18us/step - loss: 0.0164 - acc: 0.9956 - val_loss: 0.0694 - val_acc: 0.9893\n",
            "Epoch 2680/3500\n",
            "4352/4352 [==============================] - 0s 14us/step - loss: 0.0163 - acc: 0.9961 - val_loss: 0.0655 - val_acc: 0.9897\n",
            "Epoch 2681/3500\n",
            "4352/4352 [==============================] - 0s 13us/step - loss: 0.0158 - acc: 0.9963 - val_loss: 0.0634 - val_acc: 0.9902\n",
            "Epoch 2682/3500\n",
            "4352/4352 [==============================] - 0s 14us/step - loss: 0.0155 - acc: 0.9966 - val_loss: 0.0595 - val_acc: 0.9916\n",
            "Epoch 2683/3500\n",
            "4352/4352 [==============================] - 0s 16us/step - loss: 0.0158 - acc: 0.9961 - val_loss: 0.0586 - val_acc: 0.9916\n",
            "Epoch 2684/3500\n",
            "4352/4352 [==============================] - 0s 15us/step - loss: 0.0161 - acc: 0.9959 - val_loss: 0.0601 - val_acc: 0.9911\n",
            "Epoch 2685/3500\n",
            "4352/4352 [==============================] - 0s 15us/step - loss: 0.0181 - acc: 0.9954 - val_loss: 0.0592 - val_acc: 0.9907\n",
            "Epoch 2686/3500\n",
            "4352/4352 [==============================] - 0s 15us/step - loss: 0.0168 - acc: 0.9959 - val_loss: 0.0579 - val_acc: 0.9907\n",
            "Epoch 2687/3500\n",
            "4352/4352 [==============================] - 0s 14us/step - loss: 0.0161 - acc: 0.9956 - val_loss: 0.0599 - val_acc: 0.9911\n",
            "Epoch 2688/3500\n",
            "4352/4352 [==============================] - 0s 14us/step - loss: 0.0156 - acc: 0.9968 - val_loss: 0.0637 - val_acc: 0.9911\n",
            "Epoch 2689/3500\n",
            "4352/4352 [==============================] - 0s 13us/step - loss: 0.0162 - acc: 0.9959 - val_loss: 0.0697 - val_acc: 0.9888\n",
            "Epoch 2690/3500\n",
            "4352/4352 [==============================] - 0s 13us/step - loss: 0.0171 - acc: 0.9949 - val_loss: 0.0621 - val_acc: 0.9902\n",
            "Epoch 2691/3500\n",
            "4352/4352 [==============================] - 0s 14us/step - loss: 0.0173 - acc: 0.9945 - val_loss: 0.0588 - val_acc: 0.9911\n",
            "Epoch 2692/3500\n",
            "4352/4352 [==============================] - 0s 14us/step - loss: 0.0163 - acc: 0.9959 - val_loss: 0.0580 - val_acc: 0.9921\n",
            "Epoch 2693/3500\n",
            "4352/4352 [==============================] - 0s 13us/step - loss: 0.0175 - acc: 0.9949 - val_loss: 0.0575 - val_acc: 0.9907\n",
            "Epoch 2694/3500\n",
            "4352/4352 [==============================] - 0s 12us/step - loss: 0.0157 - acc: 0.9970 - val_loss: 0.0656 - val_acc: 0.9907\n",
            "Epoch 2695/3500\n",
            "4352/4352 [==============================] - 0s 14us/step - loss: 0.0152 - acc: 0.9968 - val_loss: 0.0628 - val_acc: 0.9907\n",
            "Epoch 2696/3500\n",
            "4352/4352 [==============================] - 0s 16us/step - loss: 0.0156 - acc: 0.9959 - val_loss: 0.0663 - val_acc: 0.9897\n",
            "Epoch 2697/3500\n",
            "4352/4352 [==============================] - 0s 14us/step - loss: 0.0178 - acc: 0.9947 - val_loss: 0.0650 - val_acc: 0.9902\n",
            "Epoch 2698/3500\n",
            "4352/4352 [==============================] - 0s 15us/step - loss: 0.0161 - acc: 0.9963 - val_loss: 0.0572 - val_acc: 0.9907\n",
            "Epoch 2699/3500\n",
            "4352/4352 [==============================] - 0s 14us/step - loss: 0.0169 - acc: 0.9961 - val_loss: 0.0583 - val_acc: 0.9907\n",
            "Epoch 2700/3500\n",
            "4352/4352 [==============================] - 0s 15us/step - loss: 0.0155 - acc: 0.9961 - val_loss: 0.0628 - val_acc: 0.9907\n",
            "Epoch 2701/3500\n",
            "4352/4352 [==============================] - 0s 15us/step - loss: 0.0154 - acc: 0.9966 - val_loss: 0.0585 - val_acc: 0.9916\n",
            "Epoch 2702/3500\n",
            "4352/4352 [==============================] - 0s 15us/step - loss: 0.0161 - acc: 0.9961 - val_loss: 0.0579 - val_acc: 0.9907\n",
            "Epoch 2703/3500\n",
            "4352/4352 [==============================] - 0s 16us/step - loss: 0.0173 - acc: 0.9954 - val_loss: 0.0621 - val_acc: 0.9907\n",
            "Epoch 2704/3500\n",
            "4352/4352 [==============================] - 0s 14us/step - loss: 0.0163 - acc: 0.9959 - val_loss: 0.0716 - val_acc: 0.9893\n",
            "Epoch 2705/3500\n",
            "4352/4352 [==============================] - 0s 17us/step - loss: 0.0169 - acc: 0.9947 - val_loss: 0.0878 - val_acc: 0.9841\n",
            "Epoch 2706/3500\n",
            "4352/4352 [==============================] - 0s 13us/step - loss: 0.0184 - acc: 0.9949 - val_loss: 0.0691 - val_acc: 0.9897\n",
            "Epoch 2707/3500\n",
            "4352/4352 [==============================] - 0s 14us/step - loss: 0.0169 - acc: 0.9952 - val_loss: 0.0708 - val_acc: 0.9888\n",
            "Epoch 2708/3500\n",
            "4352/4352 [==============================] - 0s 14us/step - loss: 0.0180 - acc: 0.9945 - val_loss: 0.0733 - val_acc: 0.9893\n",
            "Epoch 2709/3500\n",
            "4352/4352 [==============================] - 0s 13us/step - loss: 0.0176 - acc: 0.9952 - val_loss: 0.0637 - val_acc: 0.9902\n",
            "Epoch 2710/3500\n",
            "4352/4352 [==============================] - 0s 12us/step - loss: 0.0159 - acc: 0.9963 - val_loss: 0.0580 - val_acc: 0.9916\n",
            "Epoch 2711/3500\n",
            "4352/4352 [==============================] - 0s 15us/step - loss: 0.0164 - acc: 0.9954 - val_loss: 0.0573 - val_acc: 0.9907\n",
            "Epoch 2712/3500\n",
            "4352/4352 [==============================] - 0s 14us/step - loss: 0.0174 - acc: 0.9956 - val_loss: 0.0593 - val_acc: 0.9916\n",
            "Epoch 2713/3500\n",
            "4352/4352 [==============================] - 0s 13us/step - loss: 0.0191 - acc: 0.9947 - val_loss: 0.0574 - val_acc: 0.9916\n",
            "Epoch 2714/3500\n",
            "4352/4352 [==============================] - 0s 13us/step - loss: 0.0164 - acc: 0.9956 - val_loss: 0.0571 - val_acc: 0.9897\n",
            "Epoch 2715/3500\n",
            "4352/4352 [==============================] - 0s 13us/step - loss: 0.0247 - acc: 0.9922 - val_loss: 0.0593 - val_acc: 0.9916\n",
            "Epoch 2716/3500\n",
            "4352/4352 [==============================] - 0s 13us/step - loss: 0.0190 - acc: 0.9945 - val_loss: 0.0577 - val_acc: 0.9907\n",
            "Epoch 2717/3500\n",
            "4352/4352 [==============================] - 0s 13us/step - loss: 0.0202 - acc: 0.9938 - val_loss: 0.0558 - val_acc: 0.9921\n",
            "Epoch 2718/3500\n",
            "4352/4352 [==============================] - 0s 13us/step - loss: 0.0207 - acc: 0.9943 - val_loss: 0.0625 - val_acc: 0.9907\n",
            "Epoch 2719/3500\n",
            "4352/4352 [==============================] - 0s 14us/step - loss: 0.0205 - acc: 0.9938 - val_loss: 0.0603 - val_acc: 0.9907\n",
            "Epoch 2720/3500\n",
            "4352/4352 [==============================] - 0s 13us/step - loss: 0.0211 - acc: 0.9940 - val_loss: 0.0820 - val_acc: 0.9860\n",
            "Epoch 2721/3500\n",
            "4352/4352 [==============================] - 0s 13us/step - loss: 0.0186 - acc: 0.9947 - val_loss: 0.0702 - val_acc: 0.9897\n",
            "Epoch 2722/3500\n",
            "4352/4352 [==============================] - 0s 13us/step - loss: 0.0187 - acc: 0.9940 - val_loss: 0.0583 - val_acc: 0.9911\n",
            "Epoch 2723/3500\n",
            "4352/4352 [==============================] - 0s 13us/step - loss: 0.0164 - acc: 0.9959 - val_loss: 0.0623 - val_acc: 0.9907\n",
            "Epoch 2724/3500\n",
            "4352/4352 [==============================] - 0s 13us/step - loss: 0.0167 - acc: 0.9952 - val_loss: 0.0592 - val_acc: 0.9911\n",
            "Epoch 2725/3500\n",
            "4352/4352 [==============================] - 0s 14us/step - loss: 0.0152 - acc: 0.9968 - val_loss: 0.0641 - val_acc: 0.9907\n",
            "Epoch 2726/3500\n",
            "4352/4352 [==============================] - 0s 13us/step - loss: 0.0151 - acc: 0.9970 - val_loss: 0.0586 - val_acc: 0.9921\n",
            "Epoch 2727/3500\n",
            "4352/4352 [==============================] - 0s 13us/step - loss: 0.0161 - acc: 0.9956 - val_loss: 0.0593 - val_acc: 0.9916\n",
            "Epoch 2728/3500\n",
            "4352/4352 [==============================] - 0s 16us/step - loss: 0.0158 - acc: 0.9961 - val_loss: 0.0606 - val_acc: 0.9911\n",
            "Epoch 2729/3500\n",
            "4352/4352 [==============================] - 0s 14us/step - loss: 0.0160 - acc: 0.9959 - val_loss: 0.0632 - val_acc: 0.9902\n",
            "Epoch 2730/3500\n",
            "4352/4352 [==============================] - 0s 14us/step - loss: 0.0160 - acc: 0.9961 - val_loss: 0.0679 - val_acc: 0.9902\n",
            "Epoch 2731/3500\n",
            "4352/4352 [==============================] - 0s 13us/step - loss: 0.0162 - acc: 0.9956 - val_loss: 0.0723 - val_acc: 0.9888\n",
            "Epoch 2732/3500\n",
            "4352/4352 [==============================] - 0s 14us/step - loss: 0.0189 - acc: 0.9947 - val_loss: 0.0741 - val_acc: 0.9888\n",
            "Epoch 2733/3500\n",
            "4352/4352 [==============================] - 0s 15us/step - loss: 0.0198 - acc: 0.9945 - val_loss: 0.0605 - val_acc: 0.9907\n",
            "Epoch 2734/3500\n",
            "4352/4352 [==============================] - 0s 14us/step - loss: 0.0190 - acc: 0.9952 - val_loss: 0.0662 - val_acc: 0.9897\n",
            "Epoch 2735/3500\n",
            "4352/4352 [==============================] - 0s 13us/step - loss: 0.0195 - acc: 0.9947 - val_loss: 0.0614 - val_acc: 0.9916\n",
            "Epoch 2736/3500\n",
            "4352/4352 [==============================] - 0s 13us/step - loss: 0.0186 - acc: 0.9945 - val_loss: 0.0590 - val_acc: 0.9911\n",
            "Epoch 2737/3500\n",
            "4352/4352 [==============================] - 0s 14us/step - loss: 0.0154 - acc: 0.9963 - val_loss: 0.0619 - val_acc: 0.9907\n",
            "Epoch 2738/3500\n",
            "4352/4352 [==============================] - 0s 14us/step - loss: 0.0159 - acc: 0.9961 - val_loss: 0.0704 - val_acc: 0.9897\n",
            "Epoch 2739/3500\n",
            "4352/4352 [==============================] - 0s 15us/step - loss: 0.0171 - acc: 0.9952 - val_loss: 0.0615 - val_acc: 0.9907\n",
            "Epoch 2740/3500\n",
            "4352/4352 [==============================] - 0s 14us/step - loss: 0.0166 - acc: 0.9966 - val_loss: 0.0611 - val_acc: 0.9911\n",
            "Epoch 2741/3500\n",
            "4352/4352 [==============================] - 0s 14us/step - loss: 0.0153 - acc: 0.9963 - val_loss: 0.0665 - val_acc: 0.9897\n",
            "Epoch 2742/3500\n",
            "4352/4352 [==============================] - 0s 14us/step - loss: 0.0157 - acc: 0.9966 - val_loss: 0.0622 - val_acc: 0.9907\n",
            "Epoch 2743/3500\n",
            "4352/4352 [==============================] - 0s 15us/step - loss: 0.0155 - acc: 0.9961 - val_loss: 0.0655 - val_acc: 0.9897\n",
            "Epoch 2744/3500\n",
            "4352/4352 [==============================] - 0s 16us/step - loss: 0.0153 - acc: 0.9968 - val_loss: 0.0603 - val_acc: 0.9911\n",
            "Epoch 2745/3500\n",
            "4352/4352 [==============================] - 0s 16us/step - loss: 0.0157 - acc: 0.9963 - val_loss: 0.0610 - val_acc: 0.9916\n",
            "Epoch 2746/3500\n",
            "4352/4352 [==============================] - 0s 14us/step - loss: 0.0154 - acc: 0.9961 - val_loss: 0.0592 - val_acc: 0.9911\n",
            "Epoch 2747/3500\n",
            "4352/4352 [==============================] - 0s 14us/step - loss: 0.0157 - acc: 0.9966 - val_loss: 0.0647 - val_acc: 0.9902\n",
            "Epoch 2748/3500\n",
            "4352/4352 [==============================] - 0s 14us/step - loss: 0.0180 - acc: 0.9947 - val_loss: 0.0628 - val_acc: 0.9902\n",
            "Epoch 2749/3500\n",
            "4352/4352 [==============================] - 0s 14us/step - loss: 0.0177 - acc: 0.9952 - val_loss: 0.0619 - val_acc: 0.9907\n",
            "Epoch 2750/3500\n",
            "4352/4352 [==============================] - 0s 13us/step - loss: 0.0188 - acc: 0.9943 - val_loss: 0.0610 - val_acc: 0.9911\n",
            "Epoch 2751/3500\n",
            "4352/4352 [==============================] - 0s 14us/step - loss: 0.0162 - acc: 0.9956 - val_loss: 0.0573 - val_acc: 0.9911\n",
            "Epoch 2752/3500\n",
            "4352/4352 [==============================] - 0s 17us/step - loss: 0.0157 - acc: 0.9959 - val_loss: 0.0659 - val_acc: 0.9897\n",
            "Epoch 2753/3500\n",
            "4352/4352 [==============================] - 0s 14us/step - loss: 0.0158 - acc: 0.9968 - val_loss: 0.0643 - val_acc: 0.9907\n",
            "Epoch 2754/3500\n",
            "4352/4352 [==============================] - 0s 15us/step - loss: 0.0155 - acc: 0.9961 - val_loss: 0.0590 - val_acc: 0.9911\n",
            "Epoch 2755/3500\n",
            "4352/4352 [==============================] - 0s 14us/step - loss: 0.0158 - acc: 0.9959 - val_loss: 0.0591 - val_acc: 0.9916\n",
            "Epoch 2756/3500\n",
            "4352/4352 [==============================] - 0s 13us/step - loss: 0.0158 - acc: 0.9961 - val_loss: 0.0579 - val_acc: 0.9911\n",
            "Epoch 2757/3500\n",
            "4352/4352 [==============================] - 0s 13us/step - loss: 0.0177 - acc: 0.9956 - val_loss: 0.0601 - val_acc: 0.9911\n",
            "Epoch 2758/3500\n",
            "4352/4352 [==============================] - 0s 15us/step - loss: 0.0153 - acc: 0.9963 - val_loss: 0.0582 - val_acc: 0.9911\n",
            "Epoch 2759/3500\n",
            "4352/4352 [==============================] - 0s 15us/step - loss: 0.0182 - acc: 0.9952 - val_loss: 0.0576 - val_acc: 0.9907\n",
            "Epoch 2760/3500\n",
            "4352/4352 [==============================] - 0s 17us/step - loss: 0.0170 - acc: 0.9954 - val_loss: 0.0622 - val_acc: 0.9907\n",
            "Epoch 2761/3500\n",
            "4352/4352 [==============================] - 0s 14us/step - loss: 0.0165 - acc: 0.9963 - val_loss: 0.0599 - val_acc: 0.9916\n",
            "Epoch 2762/3500\n",
            "4352/4352 [==============================] - 0s 14us/step - loss: 0.0166 - acc: 0.9954 - val_loss: 0.0634 - val_acc: 0.9907\n",
            "Epoch 2763/3500\n",
            "4352/4352 [==============================] - 0s 13us/step - loss: 0.0162 - acc: 0.9963 - val_loss: 0.0724 - val_acc: 0.9897\n",
            "Epoch 2764/3500\n",
            "4352/4352 [==============================] - 0s 14us/step - loss: 0.0164 - acc: 0.9961 - val_loss: 0.0697 - val_acc: 0.9897\n",
            "Epoch 2765/3500\n",
            "4352/4352 [==============================] - 0s 13us/step - loss: 0.0193 - acc: 0.9943 - val_loss: 0.0649 - val_acc: 0.9907\n",
            "Epoch 2766/3500\n",
            "4352/4352 [==============================] - 0s 14us/step - loss: 0.0164 - acc: 0.9961 - val_loss: 0.0657 - val_acc: 0.9897\n",
            "Epoch 2767/3500\n",
            "4352/4352 [==============================] - 0s 14us/step - loss: 0.0161 - acc: 0.9956 - val_loss: 0.0611 - val_acc: 0.9911\n",
            "Epoch 2768/3500\n",
            "4352/4352 [==============================] - 0s 14us/step - loss: 0.0153 - acc: 0.9963 - val_loss: 0.0582 - val_acc: 0.9916\n",
            "Epoch 2769/3500\n",
            "4352/4352 [==============================] - 0s 14us/step - loss: 0.0171 - acc: 0.9954 - val_loss: 0.0694 - val_acc: 0.9893\n",
            "Epoch 2770/3500\n",
            "4352/4352 [==============================] - 0s 13us/step - loss: 0.0175 - acc: 0.9954 - val_loss: 0.0705 - val_acc: 0.9888\n",
            "Epoch 2771/3500\n",
            "4352/4352 [==============================] - 0s 14us/step - loss: 0.0179 - acc: 0.9949 - val_loss: 0.0694 - val_acc: 0.9897\n",
            "Epoch 2772/3500\n",
            "4352/4352 [==============================] - 0s 15us/step - loss: 0.0171 - acc: 0.9959 - val_loss: 0.0812 - val_acc: 0.9865\n",
            "Epoch 2773/3500\n",
            "4352/4352 [==============================] - 0s 14us/step - loss: 0.0183 - acc: 0.9947 - val_loss: 0.0685 - val_acc: 0.9902\n",
            "Epoch 2774/3500\n",
            "4352/4352 [==============================] - 0s 14us/step - loss: 0.0181 - acc: 0.9952 - val_loss: 0.0607 - val_acc: 0.9916\n",
            "Epoch 2775/3500\n",
            "4352/4352 [==============================] - 0s 16us/step - loss: 0.0191 - acc: 0.9943 - val_loss: 0.0568 - val_acc: 0.9907\n",
            "Epoch 2776/3500\n",
            "4352/4352 [==============================] - 0s 16us/step - loss: 0.0189 - acc: 0.9945 - val_loss: 0.0578 - val_acc: 0.9911\n",
            "Epoch 2777/3500\n",
            "4352/4352 [==============================] - 0s 14us/step - loss: 0.0177 - acc: 0.9949 - val_loss: 0.0577 - val_acc: 0.9902\n",
            "Epoch 2778/3500\n",
            "4352/4352 [==============================] - 0s 13us/step - loss: 0.0168 - acc: 0.9961 - val_loss: 0.0595 - val_acc: 0.9911\n",
            "Epoch 2779/3500\n",
            "4352/4352 [==============================] - 0s 14us/step - loss: 0.0162 - acc: 0.9959 - val_loss: 0.0630 - val_acc: 0.9902\n",
            "Epoch 2780/3500\n",
            "4352/4352 [==============================] - 0s 13us/step - loss: 0.0161 - acc: 0.9963 - val_loss: 0.0634 - val_acc: 0.9902\n",
            "Epoch 2781/3500\n",
            "4352/4352 [==============================] - 0s 14us/step - loss: 0.0159 - acc: 0.9961 - val_loss: 0.0610 - val_acc: 0.9911\n",
            "Epoch 2782/3500\n",
            "4352/4352 [==============================] - 0s 14us/step - loss: 0.0163 - acc: 0.9959 - val_loss: 0.0661 - val_acc: 0.9897\n",
            "Epoch 2783/3500\n",
            "4352/4352 [==============================] - 0s 14us/step - loss: 0.0159 - acc: 0.9956 - val_loss: 0.0690 - val_acc: 0.9897\n",
            "Epoch 2784/3500\n",
            "4352/4352 [==============================] - 0s 15us/step - loss: 0.0170 - acc: 0.9954 - val_loss: 0.0703 - val_acc: 0.9897\n",
            "Epoch 2785/3500\n",
            "4352/4352 [==============================] - 0s 14us/step - loss: 0.0171 - acc: 0.9952 - val_loss: 0.0774 - val_acc: 0.9883\n",
            "Epoch 2786/3500\n",
            "4352/4352 [==============================] - 0s 13us/step - loss: 0.0161 - acc: 0.9959 - val_loss: 0.0610 - val_acc: 0.9916\n",
            "Epoch 2787/3500\n",
            "4352/4352 [==============================] - 0s 13us/step - loss: 0.0163 - acc: 0.9959 - val_loss: 0.0620 - val_acc: 0.9907\n",
            "Epoch 2788/3500\n",
            "4352/4352 [==============================] - 0s 14us/step - loss: 0.0161 - acc: 0.9961 - val_loss: 0.0563 - val_acc: 0.9921\n",
            "Epoch 2789/3500\n",
            "4352/4352 [==============================] - 0s 16us/step - loss: 0.0173 - acc: 0.9947 - val_loss: 0.0594 - val_acc: 0.9916\n",
            "Epoch 2790/3500\n",
            "4352/4352 [==============================] - 0s 14us/step - loss: 0.0180 - acc: 0.9952 - val_loss: 0.0606 - val_acc: 0.9907\n",
            "Epoch 2791/3500\n",
            "4352/4352 [==============================] - 0s 14us/step - loss: 0.0162 - acc: 0.9959 - val_loss: 0.0704 - val_acc: 0.9893\n",
            "Epoch 2792/3500\n",
            "4352/4352 [==============================] - 0s 15us/step - loss: 0.0161 - acc: 0.9959 - val_loss: 0.0683 - val_acc: 0.9902\n",
            "Epoch 2793/3500\n",
            "4352/4352 [==============================] - 0s 15us/step - loss: 0.0180 - acc: 0.9952 - val_loss: 0.0729 - val_acc: 0.9893\n",
            "Epoch 2794/3500\n",
            "4352/4352 [==============================] - 0s 14us/step - loss: 0.0183 - acc: 0.9947 - val_loss: 0.0598 - val_acc: 0.9907\n",
            "Epoch 2795/3500\n",
            "4352/4352 [==============================] - 0s 14us/step - loss: 0.0189 - acc: 0.9936 - val_loss: 0.0603 - val_acc: 0.9916\n",
            "Epoch 2796/3500\n",
            "4352/4352 [==============================] - 0s 14us/step - loss: 0.0170 - acc: 0.9959 - val_loss: 0.0624 - val_acc: 0.9902\n",
            "Epoch 2797/3500\n",
            "4352/4352 [==============================] - 0s 14us/step - loss: 0.0167 - acc: 0.9949 - val_loss: 0.0607 - val_acc: 0.9911\n",
            "Epoch 2798/3500\n",
            "4352/4352 [==============================] - 0s 13us/step - loss: 0.0162 - acc: 0.9954 - val_loss: 0.0603 - val_acc: 0.9902\n",
            "Epoch 2799/3500\n",
            "4352/4352 [==============================] - 0s 13us/step - loss: 0.0154 - acc: 0.9966 - val_loss: 0.0686 - val_acc: 0.9902\n",
            "Epoch 2800/3500\n",
            "4352/4352 [==============================] - 0s 13us/step - loss: 0.0160 - acc: 0.9956 - val_loss: 0.0590 - val_acc: 0.9907\n",
            "Epoch 2801/3500\n",
            "4352/4352 [==============================] - 0s 13us/step - loss: 0.0164 - acc: 0.9952 - val_loss: 0.0626 - val_acc: 0.9907\n",
            "Epoch 2802/3500\n",
            "4352/4352 [==============================] - 0s 14us/step - loss: 0.0151 - acc: 0.9968 - val_loss: 0.0726 - val_acc: 0.9888\n",
            "Epoch 2803/3500\n",
            "4352/4352 [==============================] - 0s 13us/step - loss: 0.0155 - acc: 0.9959 - val_loss: 0.0602 - val_acc: 0.9916\n",
            "Epoch 2804/3500\n",
            "4352/4352 [==============================] - 0s 16us/step - loss: 0.0149 - acc: 0.9966 - val_loss: 0.0634 - val_acc: 0.9907\n",
            "Epoch 2805/3500\n",
            "4352/4352 [==============================] - 0s 14us/step - loss: 0.0152 - acc: 0.9963 - val_loss: 0.0676 - val_acc: 0.9897\n",
            "Epoch 2806/3500\n",
            "4352/4352 [==============================] - 0s 15us/step - loss: 0.0151 - acc: 0.9968 - val_loss: 0.0605 - val_acc: 0.9911\n",
            "Epoch 2807/3500\n",
            "4352/4352 [==============================] - 0s 15us/step - loss: 0.0152 - acc: 0.9968 - val_loss: 0.0626 - val_acc: 0.9907\n",
            "Epoch 2808/3500\n",
            "4352/4352 [==============================] - 0s 16us/step - loss: 0.0149 - acc: 0.9966 - val_loss: 0.0631 - val_acc: 0.9907\n",
            "Epoch 2809/3500\n",
            "4352/4352 [==============================] - 0s 14us/step - loss: 0.0165 - acc: 0.9966 - val_loss: 0.0736 - val_acc: 0.9897\n",
            "Epoch 2810/3500\n",
            "4352/4352 [==============================] - 0s 15us/step - loss: 0.0200 - acc: 0.9940 - val_loss: 0.1050 - val_acc: 0.9814\n",
            "Epoch 2811/3500\n",
            "4352/4352 [==============================] - 0s 14us/step - loss: 0.0202 - acc: 0.9940 - val_loss: 0.0766 - val_acc: 0.9874\n",
            "Epoch 2812/3500\n",
            "4352/4352 [==============================] - 0s 15us/step - loss: 0.0170 - acc: 0.9956 - val_loss: 0.0613 - val_acc: 0.9916\n",
            "Epoch 2813/3500\n",
            "4352/4352 [==============================] - 0s 14us/step - loss: 0.0154 - acc: 0.9963 - val_loss: 0.0649 - val_acc: 0.9897\n",
            "Epoch 2814/3500\n",
            "4352/4352 [==============================] - 0s 14us/step - loss: 0.0174 - acc: 0.9952 - val_loss: 0.0652 - val_acc: 0.9902\n",
            "Epoch 2815/3500\n",
            "4352/4352 [==============================] - 0s 14us/step - loss: 0.0176 - acc: 0.9952 - val_loss: 0.0593 - val_acc: 0.9911\n",
            "Epoch 2816/3500\n",
            "4352/4352 [==============================] - 0s 15us/step - loss: 0.0157 - acc: 0.9954 - val_loss: 0.0686 - val_acc: 0.9897\n",
            "Epoch 2817/3500\n",
            "4352/4352 [==============================] - 0s 15us/step - loss: 0.0170 - acc: 0.9949 - val_loss: 0.0740 - val_acc: 0.9897\n",
            "Epoch 2818/3500\n",
            "4352/4352 [==============================] - 0s 15us/step - loss: 0.0214 - acc: 0.9936 - val_loss: 0.0642 - val_acc: 0.9907\n",
            "Epoch 2819/3500\n",
            "4352/4352 [==============================] - 0s 13us/step - loss: 0.0211 - acc: 0.9940 - val_loss: 0.0648 - val_acc: 0.9902\n",
            "Epoch 2820/3500\n",
            "4352/4352 [==============================] - 0s 15us/step - loss: 0.0179 - acc: 0.9949 - val_loss: 0.0638 - val_acc: 0.9902\n",
            "Epoch 2821/3500\n",
            "4352/4352 [==============================] - 0s 15us/step - loss: 0.0170 - acc: 0.9952 - val_loss: 0.0618 - val_acc: 0.9907\n",
            "Epoch 2822/3500\n",
            "4352/4352 [==============================] - 0s 15us/step - loss: 0.0165 - acc: 0.9954 - val_loss: 0.0587 - val_acc: 0.9911\n",
            "Epoch 2823/3500\n",
            "4352/4352 [==============================] - 0s 12us/step - loss: 0.0163 - acc: 0.9961 - val_loss: 0.0568 - val_acc: 0.9911\n",
            "Epoch 2824/3500\n",
            "4352/4352 [==============================] - 0s 16us/step - loss: 0.0171 - acc: 0.9949 - val_loss: 0.0624 - val_acc: 0.9907\n",
            "Epoch 2825/3500\n",
            "4352/4352 [==============================] - 0s 14us/step - loss: 0.0164 - acc: 0.9961 - val_loss: 0.0614 - val_acc: 0.9916\n",
            "Epoch 2826/3500\n",
            "4352/4352 [==============================] - 0s 14us/step - loss: 0.0163 - acc: 0.9956 - val_loss: 0.0643 - val_acc: 0.9907\n",
            "Epoch 2827/3500\n",
            "4352/4352 [==============================] - 0s 13us/step - loss: 0.0151 - acc: 0.9968 - val_loss: 0.0590 - val_acc: 0.9907\n",
            "Epoch 2828/3500\n",
            "4352/4352 [==============================] - 0s 14us/step - loss: 0.0171 - acc: 0.9952 - val_loss: 0.0593 - val_acc: 0.9916\n",
            "Epoch 2829/3500\n",
            "4352/4352 [==============================] - 0s 13us/step - loss: 0.0206 - acc: 0.9940 - val_loss: 0.0592 - val_acc: 0.9911\n",
            "Epoch 2830/3500\n",
            "4352/4352 [==============================] - 0s 14us/step - loss: 0.0178 - acc: 0.9956 - val_loss: 0.0665 - val_acc: 0.9897\n",
            "Epoch 2831/3500\n",
            "4352/4352 [==============================] - 0s 13us/step - loss: 0.0160 - acc: 0.9956 - val_loss: 0.0622 - val_acc: 0.9911\n",
            "Epoch 2832/3500\n",
            "4352/4352 [==============================] - 0s 13us/step - loss: 0.0157 - acc: 0.9966 - val_loss: 0.0608 - val_acc: 0.9911\n",
            "Epoch 2833/3500\n",
            "4352/4352 [==============================] - 0s 13us/step - loss: 0.0155 - acc: 0.9966 - val_loss: 0.0635 - val_acc: 0.9911\n",
            "Epoch 2834/3500\n",
            "4352/4352 [==============================] - 0s 13us/step - loss: 0.0159 - acc: 0.9963 - val_loss: 0.0614 - val_acc: 0.9911\n",
            "Epoch 2835/3500\n",
            "4352/4352 [==============================] - 0s 14us/step - loss: 0.0149 - acc: 0.9968 - val_loss: 0.0597 - val_acc: 0.9916\n",
            "Epoch 2836/3500\n",
            "4352/4352 [==============================] - 0s 12us/step - loss: 0.0150 - acc: 0.9956 - val_loss: 0.0621 - val_acc: 0.9911\n",
            "Epoch 2837/3500\n",
            "4352/4352 [==============================] - 0s 13us/step - loss: 0.0163 - acc: 0.9959 - val_loss: 0.0650 - val_acc: 0.9897\n",
            "Epoch 2838/3500\n",
            "4352/4352 [==============================] - 0s 15us/step - loss: 0.0176 - acc: 0.9949 - val_loss: 0.0671 - val_acc: 0.9902\n",
            "Epoch 2839/3500\n",
            "4352/4352 [==============================] - 0s 15us/step - loss: 0.0164 - acc: 0.9954 - val_loss: 0.0754 - val_acc: 0.9893\n",
            "Epoch 2840/3500\n",
            "4352/4352 [==============================] - 0s 14us/step - loss: 0.0161 - acc: 0.9956 - val_loss: 0.0620 - val_acc: 0.9916\n",
            "Epoch 2841/3500\n",
            "4352/4352 [==============================] - 0s 15us/step - loss: 0.0155 - acc: 0.9963 - val_loss: 0.0645 - val_acc: 0.9897\n",
            "Epoch 2842/3500\n",
            "4352/4352 [==============================] - 0s 15us/step - loss: 0.0157 - acc: 0.9959 - val_loss: 0.0642 - val_acc: 0.9902\n",
            "Epoch 2843/3500\n",
            "4352/4352 [==============================] - 0s 14us/step - loss: 0.0155 - acc: 0.9966 - val_loss: 0.0643 - val_acc: 0.9902\n",
            "Epoch 2844/3500\n",
            "4352/4352 [==============================] - 0s 15us/step - loss: 0.0152 - acc: 0.9966 - val_loss: 0.0636 - val_acc: 0.9902\n",
            "Epoch 2845/3500\n",
            "4352/4352 [==============================] - 0s 14us/step - loss: 0.0152 - acc: 0.9963 - val_loss: 0.0600 - val_acc: 0.9916\n",
            "Epoch 2846/3500\n",
            "4352/4352 [==============================] - 0s 13us/step - loss: 0.0154 - acc: 0.9963 - val_loss: 0.0584 - val_acc: 0.9902\n",
            "Epoch 2847/3500\n",
            "4352/4352 [==============================] - 0s 13us/step - loss: 0.0170 - acc: 0.9956 - val_loss: 0.0586 - val_acc: 0.9916\n",
            "Epoch 2848/3500\n",
            "4352/4352 [==============================] - 0s 14us/step - loss: 0.0165 - acc: 0.9959 - val_loss: 0.0624 - val_acc: 0.9907\n",
            "Epoch 2849/3500\n",
            "4352/4352 [==============================] - 0s 14us/step - loss: 0.0158 - acc: 0.9959 - val_loss: 0.0642 - val_acc: 0.9902\n",
            "Epoch 2850/3500\n",
            "4352/4352 [==============================] - 0s 15us/step - loss: 0.0150 - acc: 0.9961 - val_loss: 0.0623 - val_acc: 0.9911\n",
            "Epoch 2851/3500\n",
            "4352/4352 [==============================] - 0s 13us/step - loss: 0.0155 - acc: 0.9963 - val_loss: 0.0700 - val_acc: 0.9897\n",
            "Epoch 2852/3500\n",
            "4352/4352 [==============================] - 0s 13us/step - loss: 0.0146 - acc: 0.9963 - val_loss: 0.0582 - val_acc: 0.9911\n",
            "Epoch 2853/3500\n",
            "4352/4352 [==============================] - 0s 13us/step - loss: 0.0162 - acc: 0.9952 - val_loss: 0.0579 - val_acc: 0.9921\n",
            "Epoch 2854/3500\n",
            "4352/4352 [==============================] - 0s 14us/step - loss: 0.0161 - acc: 0.9959 - val_loss: 0.0621 - val_acc: 0.9911\n",
            "Epoch 2855/3500\n",
            "4352/4352 [==============================] - 0s 14us/step - loss: 0.0155 - acc: 0.9963 - val_loss: 0.0614 - val_acc: 0.9911\n",
            "Epoch 2856/3500\n",
            "4352/4352 [==============================] - 0s 14us/step - loss: 0.0156 - acc: 0.9959 - val_loss: 0.0635 - val_acc: 0.9907\n",
            "Epoch 2857/3500\n",
            "4352/4352 [==============================] - 0s 16us/step - loss: 0.0163 - acc: 0.9959 - val_loss: 0.0636 - val_acc: 0.9902\n",
            "Epoch 2858/3500\n",
            "4352/4352 [==============================] - 0s 15us/step - loss: 0.0198 - acc: 0.9945 - val_loss: 0.0651 - val_acc: 0.9907\n",
            "Epoch 2859/3500\n",
            "4352/4352 [==============================] - 0s 15us/step - loss: 0.0259 - acc: 0.9922 - val_loss: 0.0579 - val_acc: 0.9907\n",
            "Epoch 2860/3500\n",
            "4352/4352 [==============================] - 0s 14us/step - loss: 0.0219 - acc: 0.9938 - val_loss: 0.0587 - val_acc: 0.9921\n",
            "Epoch 2861/3500\n",
            "4352/4352 [==============================] - 0s 15us/step - loss: 0.0197 - acc: 0.9947 - val_loss: 0.0620 - val_acc: 0.9911\n",
            "Epoch 2862/3500\n",
            "4352/4352 [==============================] - 0s 15us/step - loss: 0.0167 - acc: 0.9956 - val_loss: 0.0563 - val_acc: 0.9916\n",
            "Epoch 2863/3500\n",
            "4352/4352 [==============================] - 0s 14us/step - loss: 0.0207 - acc: 0.9936 - val_loss: 0.0593 - val_acc: 0.9897\n",
            "Epoch 2864/3500\n",
            "4352/4352 [==============================] - 0s 13us/step - loss: 0.0172 - acc: 0.9952 - val_loss: 0.0595 - val_acc: 0.9911\n",
            "Epoch 2865/3500\n",
            "4352/4352 [==============================] - 0s 14us/step - loss: 0.0170 - acc: 0.9952 - val_loss: 0.0703 - val_acc: 0.9897\n",
            "Epoch 2866/3500\n",
            "4352/4352 [==============================] - 0s 13us/step - loss: 0.0209 - acc: 0.9938 - val_loss: 0.0750 - val_acc: 0.9888\n",
            "Epoch 2867/3500\n",
            "4352/4352 [==============================] - 0s 14us/step - loss: 0.0211 - acc: 0.9933 - val_loss: 0.0968 - val_acc: 0.9814\n",
            "Epoch 2868/3500\n",
            "4352/4352 [==============================] - 0s 14us/step - loss: 0.0242 - acc: 0.9922 - val_loss: 0.0877 - val_acc: 0.9841\n",
            "Epoch 2869/3500\n",
            "4352/4352 [==============================] - 0s 14us/step - loss: 0.0182 - acc: 0.9940 - val_loss: 0.0622 - val_acc: 0.9902\n",
            "Epoch 2870/3500\n",
            "4352/4352 [==============================] - 0s 13us/step - loss: 0.0156 - acc: 0.9961 - val_loss: 0.0585 - val_acc: 0.9916\n",
            "Epoch 2871/3500\n",
            "4352/4352 [==============================] - 0s 14us/step - loss: 0.0195 - acc: 0.9954 - val_loss: 0.0594 - val_acc: 0.9911\n",
            "Epoch 2872/3500\n",
            "4352/4352 [==============================] - 0s 14us/step - loss: 0.0175 - acc: 0.9954 - val_loss: 0.0760 - val_acc: 0.9893\n",
            "Epoch 2873/3500\n",
            "4352/4352 [==============================] - 0s 15us/step - loss: 0.0164 - acc: 0.9949 - val_loss: 0.0685 - val_acc: 0.9907\n",
            "Epoch 2874/3500\n",
            "4352/4352 [==============================] - 0s 16us/step - loss: 0.0167 - acc: 0.9952 - val_loss: 0.0596 - val_acc: 0.9916\n",
            "Epoch 2875/3500\n",
            "4352/4352 [==============================] - 0s 14us/step - loss: 0.0153 - acc: 0.9966 - val_loss: 0.0647 - val_acc: 0.9911\n",
            "Epoch 2876/3500\n",
            "4352/4352 [==============================] - 0s 14us/step - loss: 0.0150 - acc: 0.9972 - val_loss: 0.0614 - val_acc: 0.9916\n",
            "Epoch 2877/3500\n",
            "4352/4352 [==============================] - 0s 15us/step - loss: 0.0174 - acc: 0.9949 - val_loss: 0.0604 - val_acc: 0.9916\n",
            "Epoch 2878/3500\n",
            "4352/4352 [==============================] - 0s 16us/step - loss: 0.0172 - acc: 0.9949 - val_loss: 0.0606 - val_acc: 0.9911\n",
            "Epoch 2879/3500\n",
            "4352/4352 [==============================] - 0s 15us/step - loss: 0.0153 - acc: 0.9961 - val_loss: 0.0697 - val_acc: 0.9897\n",
            "Epoch 2880/3500\n",
            "4352/4352 [==============================] - 0s 15us/step - loss: 0.0153 - acc: 0.9961 - val_loss: 0.0660 - val_acc: 0.9902\n",
            "Epoch 2881/3500\n",
            "4352/4352 [==============================] - 0s 14us/step - loss: 0.0158 - acc: 0.9963 - val_loss: 0.0617 - val_acc: 0.9907\n",
            "Epoch 2882/3500\n",
            "4352/4352 [==============================] - 0s 14us/step - loss: 0.0149 - acc: 0.9966 - val_loss: 0.0622 - val_acc: 0.9916\n",
            "Epoch 2883/3500\n",
            "4352/4352 [==============================] - 0s 15us/step - loss: 0.0146 - acc: 0.9970 - val_loss: 0.0618 - val_acc: 0.9911\n",
            "Epoch 2884/3500\n",
            "4352/4352 [==============================] - 0s 15us/step - loss: 0.0166 - acc: 0.9954 - val_loss: 0.0597 - val_acc: 0.9911\n",
            "Epoch 2885/3500\n",
            "4352/4352 [==============================] - 0s 14us/step - loss: 0.0175 - acc: 0.9947 - val_loss: 0.0648 - val_acc: 0.9902\n",
            "Epoch 2886/3500\n",
            "4352/4352 [==============================] - 0s 16us/step - loss: 0.0198 - acc: 0.9943 - val_loss: 0.0657 - val_acc: 0.9907\n",
            "Epoch 2887/3500\n",
            "4352/4352 [==============================] - 0s 14us/step - loss: 0.0180 - acc: 0.9959 - val_loss: 0.0776 - val_acc: 0.9893\n",
            "Epoch 2888/3500\n",
            "4352/4352 [==============================] - 0s 14us/step - loss: 0.0178 - acc: 0.9949 - val_loss: 0.0712 - val_acc: 0.9893\n",
            "Epoch 2889/3500\n",
            "4352/4352 [==============================] - 0s 17us/step - loss: 0.0172 - acc: 0.9956 - val_loss: 0.0744 - val_acc: 0.9888\n",
            "Epoch 2890/3500\n",
            "4352/4352 [==============================] - 0s 15us/step - loss: 0.0175 - acc: 0.9952 - val_loss: 0.0724 - val_acc: 0.9883\n",
            "Epoch 2891/3500\n",
            "4352/4352 [==============================] - 0s 15us/step - loss: 0.0223 - acc: 0.9933 - val_loss: 0.0811 - val_acc: 0.9879\n",
            "Epoch 2892/3500\n",
            "4352/4352 [==============================] - 0s 14us/step - loss: 0.0209 - acc: 0.9933 - val_loss: 0.1276 - val_acc: 0.9744\n",
            "Epoch 2893/3500\n",
            "4352/4352 [==============================] - 0s 14us/step - loss: 0.0267 - acc: 0.9917 - val_loss: 0.0735 - val_acc: 0.9860\n",
            "Epoch 2894/3500\n",
            "4352/4352 [==============================] - 0s 13us/step - loss: 0.0217 - acc: 0.9945 - val_loss: 0.0707 - val_acc: 0.9902\n",
            "Epoch 2895/3500\n",
            "4352/4352 [==============================] - 0s 13us/step - loss: 0.0182 - acc: 0.9952 - val_loss: 0.0658 - val_acc: 0.9897\n",
            "Epoch 2896/3500\n",
            "4352/4352 [==============================] - 0s 13us/step - loss: 0.0181 - acc: 0.9952 - val_loss: 0.0732 - val_acc: 0.9902\n",
            "Epoch 2897/3500\n",
            "4352/4352 [==============================] - 0s 13us/step - loss: 0.0200 - acc: 0.9943 - val_loss: 0.0684 - val_acc: 0.9897\n",
            "Epoch 2898/3500\n",
            "4352/4352 [==============================] - 0s 14us/step - loss: 0.0157 - acc: 0.9956 - val_loss: 0.0596 - val_acc: 0.9902\n",
            "Epoch 2899/3500\n",
            "4352/4352 [==============================] - 0s 13us/step - loss: 0.0157 - acc: 0.9956 - val_loss: 0.0641 - val_acc: 0.9902\n",
            "Epoch 2900/3500\n",
            "4352/4352 [==============================] - 0s 13us/step - loss: 0.0155 - acc: 0.9959 - val_loss: 0.0721 - val_acc: 0.9897\n",
            "Epoch 2901/3500\n",
            "4352/4352 [==============================] - 0s 15us/step - loss: 0.0170 - acc: 0.9954 - val_loss: 0.0693 - val_acc: 0.9897\n",
            "Epoch 2902/3500\n",
            "4352/4352 [==============================] - 0s 14us/step - loss: 0.0158 - acc: 0.9959 - val_loss: 0.0594 - val_acc: 0.9911\n",
            "Epoch 2903/3500\n",
            "4352/4352 [==============================] - 0s 13us/step - loss: 0.0152 - acc: 0.9966 - val_loss: 0.0627 - val_acc: 0.9911\n",
            "Epoch 2904/3500\n",
            "4352/4352 [==============================] - 0s 13us/step - loss: 0.0151 - acc: 0.9961 - val_loss: 0.0685 - val_acc: 0.9897\n",
            "Epoch 2905/3500\n",
            "4352/4352 [==============================] - 0s 15us/step - loss: 0.0157 - acc: 0.9963 - val_loss: 0.0604 - val_acc: 0.9921\n",
            "Epoch 2906/3500\n",
            "4352/4352 [==============================] - 0s 13us/step - loss: 0.0153 - acc: 0.9961 - val_loss: 0.0623 - val_acc: 0.9916\n",
            "Epoch 2907/3500\n",
            "4352/4352 [==============================] - 0s 14us/step - loss: 0.0147 - acc: 0.9966 - val_loss: 0.0653 - val_acc: 0.9907\n",
            "Epoch 2908/3500\n",
            "4352/4352 [==============================] - 0s 14us/step - loss: 0.0148 - acc: 0.9968 - val_loss: 0.0619 - val_acc: 0.9916\n",
            "Epoch 2909/3500\n",
            "4352/4352 [==============================] - 0s 13us/step - loss: 0.0150 - acc: 0.9966 - val_loss: 0.0680 - val_acc: 0.9902\n",
            "Epoch 2910/3500\n",
            "4352/4352 [==============================] - 0s 13us/step - loss: 0.0157 - acc: 0.9959 - val_loss: 0.0666 - val_acc: 0.9902\n",
            "Epoch 2911/3500\n",
            "4352/4352 [==============================] - 0s 13us/step - loss: 0.0149 - acc: 0.9963 - val_loss: 0.0595 - val_acc: 0.9921\n",
            "Epoch 2912/3500\n",
            "4352/4352 [==============================] - 0s 13us/step - loss: 0.0163 - acc: 0.9956 - val_loss: 0.0586 - val_acc: 0.9916\n",
            "Epoch 2913/3500\n",
            "4352/4352 [==============================] - 0s 14us/step - loss: 0.0164 - acc: 0.9947 - val_loss: 0.0584 - val_acc: 0.9907\n",
            "Epoch 2914/3500\n",
            "4352/4352 [==============================] - 0s 14us/step - loss: 0.0161 - acc: 0.9954 - val_loss: 0.0637 - val_acc: 0.9911\n",
            "Epoch 2915/3500\n",
            "4352/4352 [==============================] - 0s 13us/step - loss: 0.0151 - acc: 0.9966 - val_loss: 0.0650 - val_acc: 0.9911\n",
            "Epoch 2916/3500\n",
            "4352/4352 [==============================] - 0s 14us/step - loss: 0.0157 - acc: 0.9966 - val_loss: 0.0638 - val_acc: 0.9907\n",
            "Epoch 2917/3500\n",
            "4352/4352 [==============================] - 0s 14us/step - loss: 0.0159 - acc: 0.9959 - val_loss: 0.0644 - val_acc: 0.9902\n",
            "Epoch 2918/3500\n",
            "4352/4352 [==============================] - 0s 15us/step - loss: 0.0151 - acc: 0.9968 - val_loss: 0.0612 - val_acc: 0.9916\n",
            "Epoch 2919/3500\n",
            "4352/4352 [==============================] - 0s 15us/step - loss: 0.0155 - acc: 0.9959 - val_loss: 0.0660 - val_acc: 0.9907\n",
            "Epoch 2920/3500\n",
            "4352/4352 [==============================] - 0s 14us/step - loss: 0.0149 - acc: 0.9961 - val_loss: 0.0653 - val_acc: 0.9907\n",
            "Epoch 2921/3500\n",
            "4352/4352 [==============================] - 0s 14us/step - loss: 0.0158 - acc: 0.9966 - val_loss: 0.0643 - val_acc: 0.9907\n",
            "Epoch 2922/3500\n",
            "4352/4352 [==============================] - 0s 15us/step - loss: 0.0156 - acc: 0.9956 - val_loss: 0.0607 - val_acc: 0.9916\n",
            "Epoch 2923/3500\n",
            "4352/4352 [==============================] - 0s 14us/step - loss: 0.0149 - acc: 0.9968 - val_loss: 0.0647 - val_acc: 0.9907\n",
            "Epoch 2924/3500\n",
            "4352/4352 [==============================] - 0s 15us/step - loss: 0.0159 - acc: 0.9963 - val_loss: 0.0656 - val_acc: 0.9911\n",
            "Epoch 2925/3500\n",
            "4352/4352 [==============================] - 0s 13us/step - loss: 0.0160 - acc: 0.9956 - val_loss: 0.0618 - val_acc: 0.9916\n",
            "Epoch 2926/3500\n",
            "4352/4352 [==============================] - 0s 14us/step - loss: 0.0154 - acc: 0.9961 - val_loss: 0.0624 - val_acc: 0.9911\n",
            "Epoch 2927/3500\n",
            "4352/4352 [==============================] - 0s 14us/step - loss: 0.0164 - acc: 0.9954 - val_loss: 0.0613 - val_acc: 0.9916\n",
            "Epoch 2928/3500\n",
            "4352/4352 [==============================] - 0s 14us/step - loss: 0.0164 - acc: 0.9954 - val_loss: 0.0659 - val_acc: 0.9902\n",
            "Epoch 2929/3500\n",
            "4352/4352 [==============================] - 0s 15us/step - loss: 0.0163 - acc: 0.9959 - val_loss: 0.0754 - val_acc: 0.9879\n",
            "Epoch 2930/3500\n",
            "4352/4352 [==============================] - 0s 14us/step - loss: 0.0177 - acc: 0.9954 - val_loss: 0.0656 - val_acc: 0.9907\n",
            "Epoch 2931/3500\n",
            "4352/4352 [==============================] - 0s 13us/step - loss: 0.0155 - acc: 0.9959 - val_loss: 0.0726 - val_acc: 0.9893\n",
            "Epoch 2932/3500\n",
            "4352/4352 [==============================] - 0s 16us/step - loss: 0.0175 - acc: 0.9954 - val_loss: 0.0661 - val_acc: 0.9902\n",
            "Epoch 2933/3500\n",
            "4352/4352 [==============================] - 0s 17us/step - loss: 0.0151 - acc: 0.9961 - val_loss: 0.0608 - val_acc: 0.9911\n",
            "Epoch 2934/3500\n",
            "4352/4352 [==============================] - 0s 14us/step - loss: 0.0154 - acc: 0.9963 - val_loss: 0.0608 - val_acc: 0.9916\n",
            "Epoch 2935/3500\n",
            "4352/4352 [==============================] - 0s 14us/step - loss: 0.0163 - acc: 0.9961 - val_loss: 0.0617 - val_acc: 0.9916\n",
            "Epoch 2936/3500\n",
            "4352/4352 [==============================] - 0s 13us/step - loss: 0.0153 - acc: 0.9959 - val_loss: 0.0944 - val_acc: 0.9828\n",
            "Epoch 2937/3500\n",
            "4352/4352 [==============================] - 0s 12us/step - loss: 0.0231 - acc: 0.9929 - val_loss: 0.0920 - val_acc: 0.9832\n",
            "Epoch 2938/3500\n",
            "4352/4352 [==============================] - 0s 16us/step - loss: 0.0221 - acc: 0.9933 - val_loss: 0.0647 - val_acc: 0.9911\n",
            "Epoch 2939/3500\n",
            "4352/4352 [==============================] - 0s 14us/step - loss: 0.0161 - acc: 0.9961 - val_loss: 0.0592 - val_acc: 0.9916\n",
            "Epoch 2940/3500\n",
            "4352/4352 [==============================] - 0s 15us/step - loss: 0.0150 - acc: 0.9963 - val_loss: 0.0586 - val_acc: 0.9902\n",
            "Epoch 2941/3500\n",
            "4352/4352 [==============================] - 0s 18us/step - loss: 0.0156 - acc: 0.9959 - val_loss: 0.0583 - val_acc: 0.9907\n",
            "Epoch 2942/3500\n",
            "4352/4352 [==============================] - 0s 13us/step - loss: 0.0156 - acc: 0.9961 - val_loss: 0.0584 - val_acc: 0.9911\n",
            "Epoch 2943/3500\n",
            "4352/4352 [==============================] - 0s 14us/step - loss: 0.0160 - acc: 0.9956 - val_loss: 0.0573 - val_acc: 0.9893\n",
            "Epoch 2944/3500\n",
            "4352/4352 [==============================] - 0s 16us/step - loss: 0.0199 - acc: 0.9933 - val_loss: 0.0584 - val_acc: 0.9911\n",
            "Epoch 2945/3500\n",
            "4352/4352 [==============================] - 0s 14us/step - loss: 0.0190 - acc: 0.9945 - val_loss: 0.0633 - val_acc: 0.9907\n",
            "Epoch 2946/3500\n",
            "4352/4352 [==============================] - 0s 15us/step - loss: 0.0187 - acc: 0.9943 - val_loss: 0.0655 - val_acc: 0.9911\n",
            "Epoch 2947/3500\n",
            "4352/4352 [==============================] - 0s 14us/step - loss: 0.0182 - acc: 0.9943 - val_loss: 0.0772 - val_acc: 0.9893\n",
            "Epoch 2948/3500\n",
            "4352/4352 [==============================] - 0s 15us/step - loss: 0.0186 - acc: 0.9938 - val_loss: 0.0778 - val_acc: 0.9888\n",
            "Epoch 2949/3500\n",
            "4352/4352 [==============================] - 0s 15us/step - loss: 0.0185 - acc: 0.9943 - val_loss: 0.0697 - val_acc: 0.9907\n",
            "Epoch 2950/3500\n",
            "4352/4352 [==============================] - 0s 15us/step - loss: 0.0173 - acc: 0.9956 - val_loss: 0.0562 - val_acc: 0.9907\n",
            "Epoch 2951/3500\n",
            "4352/4352 [==============================] - 0s 15us/step - loss: 0.0152 - acc: 0.9959 - val_loss: 0.0605 - val_acc: 0.9921\n",
            "Epoch 2952/3500\n",
            "4352/4352 [==============================] - 0s 15us/step - loss: 0.0157 - acc: 0.9961 - val_loss: 0.0697 - val_acc: 0.9902\n",
            "Epoch 2953/3500\n",
            "4352/4352 [==============================] - 0s 13us/step - loss: 0.0160 - acc: 0.9961 - val_loss: 0.0647 - val_acc: 0.9907\n",
            "Epoch 2954/3500\n",
            "4352/4352 [==============================] - 0s 14us/step - loss: 0.0149 - acc: 0.9961 - val_loss: 0.0622 - val_acc: 0.9902\n",
            "Epoch 2955/3500\n",
            "4352/4352 [==============================] - 0s 14us/step - loss: 0.0146 - acc: 0.9966 - val_loss: 0.0616 - val_acc: 0.9921\n",
            "Epoch 2956/3500\n",
            "4352/4352 [==============================] - 0s 16us/step - loss: 0.0147 - acc: 0.9961 - val_loss: 0.0696 - val_acc: 0.9897\n",
            "Epoch 2957/3500\n",
            "4352/4352 [==============================] - 0s 14us/step - loss: 0.0154 - acc: 0.9963 - val_loss: 0.0636 - val_acc: 0.9921\n",
            "Epoch 2958/3500\n",
            "4352/4352 [==============================] - 0s 14us/step - loss: 0.0151 - acc: 0.9966 - val_loss: 0.0601 - val_acc: 0.9921\n",
            "Epoch 2959/3500\n",
            "4352/4352 [==============================] - 0s 14us/step - loss: 0.0149 - acc: 0.9968 - val_loss: 0.0627 - val_acc: 0.9911\n",
            "Epoch 2960/3500\n",
            "4352/4352 [==============================] - 0s 15us/step - loss: 0.0144 - acc: 0.9966 - val_loss: 0.0634 - val_acc: 0.9911\n",
            "Epoch 2961/3500\n",
            "4352/4352 [==============================] - 0s 15us/step - loss: 0.0149 - acc: 0.9966 - val_loss: 0.0686 - val_acc: 0.9902\n",
            "Epoch 2962/3500\n",
            "4352/4352 [==============================] - 0s 15us/step - loss: 0.0163 - acc: 0.9959 - val_loss: 0.0689 - val_acc: 0.9897\n",
            "Epoch 2963/3500\n",
            "4352/4352 [==============================] - 0s 14us/step - loss: 0.0182 - acc: 0.9938 - val_loss: 0.0608 - val_acc: 0.9911\n",
            "Epoch 2964/3500\n",
            "4352/4352 [==============================] - 0s 14us/step - loss: 0.0161 - acc: 0.9956 - val_loss: 0.0617 - val_acc: 0.9916\n",
            "Epoch 2965/3500\n",
            "4352/4352 [==============================] - 0s 16us/step - loss: 0.0165 - acc: 0.9949 - val_loss: 0.0585 - val_acc: 0.9897\n",
            "Epoch 2966/3500\n",
            "4352/4352 [==============================] - 0s 16us/step - loss: 0.0159 - acc: 0.9954 - val_loss: 0.0583 - val_acc: 0.9911\n",
            "Epoch 2967/3500\n",
            "4352/4352 [==============================] - 0s 14us/step - loss: 0.0164 - acc: 0.9954 - val_loss: 0.0626 - val_acc: 0.9911\n",
            "Epoch 2968/3500\n",
            "4352/4352 [==============================] - 0s 14us/step - loss: 0.0162 - acc: 0.9954 - val_loss: 0.0696 - val_acc: 0.9902\n",
            "Epoch 2969/3500\n",
            "4352/4352 [==============================] - 0s 20us/step - loss: 0.0148 - acc: 0.9966 - val_loss: 0.0660 - val_acc: 0.9907\n",
            "Epoch 2970/3500\n",
            "4352/4352 [==============================] - 0s 14us/step - loss: 0.0145 - acc: 0.9966 - val_loss: 0.0618 - val_acc: 0.9911\n",
            "Epoch 2971/3500\n",
            "4352/4352 [==============================] - 0s 14us/step - loss: 0.0145 - acc: 0.9966 - val_loss: 0.0630 - val_acc: 0.9916\n",
            "Epoch 2972/3500\n",
            "4352/4352 [==============================] - 0s 14us/step - loss: 0.0149 - acc: 0.9963 - val_loss: 0.0617 - val_acc: 0.9916\n",
            "Epoch 2973/3500\n",
            "4352/4352 [==============================] - 0s 14us/step - loss: 0.0146 - acc: 0.9968 - val_loss: 0.0625 - val_acc: 0.9911\n",
            "Epoch 2974/3500\n",
            "4352/4352 [==============================] - 0s 14us/step - loss: 0.0148 - acc: 0.9966 - val_loss: 0.0606 - val_acc: 0.9921\n",
            "Epoch 2975/3500\n",
            "4352/4352 [==============================] - 0s 14us/step - loss: 0.0147 - acc: 0.9966 - val_loss: 0.0612 - val_acc: 0.9921\n",
            "Epoch 2976/3500\n",
            "4352/4352 [==============================] - 0s 14us/step - loss: 0.0144 - acc: 0.9963 - val_loss: 0.0713 - val_acc: 0.9893\n",
            "Epoch 2977/3500\n",
            "4352/4352 [==============================] - 0s 14us/step - loss: 0.0161 - acc: 0.9956 - val_loss: 0.0672 - val_acc: 0.9907\n",
            "Epoch 2978/3500\n",
            "4352/4352 [==============================] - 0s 14us/step - loss: 0.0198 - acc: 0.9938 - val_loss: 0.0902 - val_acc: 0.9837\n",
            "Epoch 2979/3500\n",
            "4352/4352 [==============================] - 0s 13us/step - loss: 0.0230 - acc: 0.9922 - val_loss: 0.0683 - val_acc: 0.9907\n",
            "Epoch 2980/3500\n",
            "4352/4352 [==============================] - 0s 14us/step - loss: 0.0239 - acc: 0.9926 - val_loss: 0.0892 - val_acc: 0.9846\n",
            "Epoch 2981/3500\n",
            "4352/4352 [==============================] - 0s 14us/step - loss: 0.0259 - acc: 0.9924 - val_loss: 0.0598 - val_acc: 0.9911\n",
            "Epoch 2982/3500\n",
            "4352/4352 [==============================] - 0s 14us/step - loss: 0.0183 - acc: 0.9947 - val_loss: 0.0583 - val_acc: 0.9916\n",
            "Epoch 2983/3500\n",
            "4352/4352 [==============================] - 0s 14us/step - loss: 0.0165 - acc: 0.9956 - val_loss: 0.0601 - val_acc: 0.9921\n",
            "Epoch 2984/3500\n",
            "4352/4352 [==============================] - 0s 12us/step - loss: 0.0169 - acc: 0.9956 - val_loss: 0.0665 - val_acc: 0.9897\n",
            "Epoch 2985/3500\n",
            "4352/4352 [==============================] - 0s 16us/step - loss: 0.0176 - acc: 0.9966 - val_loss: 0.0653 - val_acc: 0.9911\n",
            "Epoch 2986/3500\n",
            "4352/4352 [==============================] - 0s 13us/step - loss: 0.0167 - acc: 0.9956 - val_loss: 0.0657 - val_acc: 0.9911\n",
            "Epoch 2987/3500\n",
            "4352/4352 [==============================] - 0s 14us/step - loss: 0.0165 - acc: 0.9949 - val_loss: 0.0604 - val_acc: 0.9925\n",
            "Epoch 2988/3500\n",
            "4352/4352 [==============================] - 0s 15us/step - loss: 0.0145 - acc: 0.9970 - val_loss: 0.0589 - val_acc: 0.9921\n",
            "Epoch 2989/3500\n",
            "4352/4352 [==============================] - 0s 14us/step - loss: 0.0153 - acc: 0.9961 - val_loss: 0.0600 - val_acc: 0.9925\n",
            "Epoch 2990/3500\n",
            "4352/4352 [==============================] - 0s 15us/step - loss: 0.0155 - acc: 0.9959 - val_loss: 0.0581 - val_acc: 0.9921\n",
            "Epoch 2991/3500\n",
            "4352/4352 [==============================] - 0s 14us/step - loss: 0.0193 - acc: 0.9936 - val_loss: 0.0635 - val_acc: 0.9916\n",
            "Epoch 2992/3500\n",
            "4352/4352 [==============================] - 0s 15us/step - loss: 0.0163 - acc: 0.9954 - val_loss: 0.0727 - val_acc: 0.9902\n",
            "Epoch 2993/3500\n",
            "4352/4352 [==============================] - 0s 13us/step - loss: 0.0166 - acc: 0.9959 - val_loss: 0.0669 - val_acc: 0.9907\n",
            "Epoch 2994/3500\n",
            "4352/4352 [==============================] - 0s 13us/step - loss: 0.0160 - acc: 0.9961 - val_loss: 0.0586 - val_acc: 0.9921\n",
            "Epoch 2995/3500\n",
            "4352/4352 [==============================] - 0s 13us/step - loss: 0.0160 - acc: 0.9956 - val_loss: 0.0585 - val_acc: 0.9902\n",
            "Epoch 2996/3500\n",
            "4352/4352 [==============================] - 0s 13us/step - loss: 0.0199 - acc: 0.9940 - val_loss: 0.0584 - val_acc: 0.9916\n",
            "Epoch 2997/3500\n",
            "4352/4352 [==============================] - 0s 13us/step - loss: 0.0162 - acc: 0.9952 - val_loss: 0.0666 - val_acc: 0.9911\n",
            "Epoch 2998/3500\n",
            "4352/4352 [==============================] - 0s 13us/step - loss: 0.0148 - acc: 0.9968 - val_loss: 0.0625 - val_acc: 0.9916\n",
            "Epoch 2999/3500\n",
            "4352/4352 [==============================] - 0s 14us/step - loss: 0.0150 - acc: 0.9968 - val_loss: 0.0633 - val_acc: 0.9916\n",
            "Epoch 3000/3500\n",
            "4352/4352 [==============================] - 0s 13us/step - loss: 0.0147 - acc: 0.9968 - val_loss: 0.0621 - val_acc: 0.9925\n",
            "Epoch 3001/3500\n",
            "4352/4352 [==============================] - 0s 14us/step - loss: 0.0149 - acc: 0.9963 - val_loss: 0.0609 - val_acc: 0.9921\n",
            "Epoch 3002/3500\n",
            "4352/4352 [==============================] - 0s 16us/step - loss: 0.0148 - acc: 0.9968 - val_loss: 0.0608 - val_acc: 0.9925\n",
            "Epoch 3003/3500\n",
            "4352/4352 [==============================] - 0s 14us/step - loss: 0.0148 - acc: 0.9961 - val_loss: 0.0668 - val_acc: 0.9907\n",
            "Epoch 3004/3500\n",
            "4352/4352 [==============================] - 0s 13us/step - loss: 0.0150 - acc: 0.9963 - val_loss: 0.0655 - val_acc: 0.9916\n",
            "Epoch 3005/3500\n",
            "4352/4352 [==============================] - 0s 13us/step - loss: 0.0149 - acc: 0.9966 - val_loss: 0.0631 - val_acc: 0.9911\n",
            "Epoch 3006/3500\n",
            "4352/4352 [==============================] - 0s 14us/step - loss: 0.0144 - acc: 0.9966 - val_loss: 0.0631 - val_acc: 0.9921\n",
            "Epoch 3007/3500\n",
            "4352/4352 [==============================] - 0s 14us/step - loss: 0.0151 - acc: 0.9968 - val_loss: 0.0680 - val_acc: 0.9911\n",
            "Epoch 3008/3500\n",
            "4352/4352 [==============================] - 0s 13us/step - loss: 0.0157 - acc: 0.9963 - val_loss: 0.0618 - val_acc: 0.9921\n",
            "Epoch 3009/3500\n",
            "4352/4352 [==============================] - 0s 13us/step - loss: 0.0147 - acc: 0.9961 - val_loss: 0.0592 - val_acc: 0.9921\n",
            "Epoch 3010/3500\n",
            "4352/4352 [==============================] - 0s 12us/step - loss: 0.0154 - acc: 0.9959 - val_loss: 0.0662 - val_acc: 0.9907\n",
            "Epoch 3011/3500\n",
            "4352/4352 [==============================] - 0s 14us/step - loss: 0.0153 - acc: 0.9963 - val_loss: 0.0744 - val_acc: 0.9893\n",
            "Epoch 3012/3500\n",
            "4352/4352 [==============================] - 0s 14us/step - loss: 0.0169 - acc: 0.9954 - val_loss: 0.0609 - val_acc: 0.9921\n",
            "Epoch 3013/3500\n",
            "4352/4352 [==============================] - 0s 13us/step - loss: 0.0150 - acc: 0.9968 - val_loss: 0.0601 - val_acc: 0.9921\n",
            "Epoch 3014/3500\n",
            "4352/4352 [==============================] - 0s 13us/step - loss: 0.0144 - acc: 0.9968 - val_loss: 0.0715 - val_acc: 0.9902\n",
            "Epoch 3015/3500\n",
            "4352/4352 [==============================] - 0s 13us/step - loss: 0.0154 - acc: 0.9961 - val_loss: 0.0618 - val_acc: 0.9925\n",
            "Epoch 3016/3500\n",
            "4352/4352 [==============================] - 0s 15us/step - loss: 0.0163 - acc: 0.9952 - val_loss: 0.0596 - val_acc: 0.9925\n",
            "Epoch 3017/3500\n",
            "4352/4352 [==============================] - 0s 13us/step - loss: 0.0147 - acc: 0.9966 - val_loss: 0.0623 - val_acc: 0.9916\n",
            "Epoch 3018/3500\n",
            "4352/4352 [==============================] - 0s 16us/step - loss: 0.0144 - acc: 0.9966 - val_loss: 0.0717 - val_acc: 0.9902\n",
            "Epoch 3019/3500\n",
            "4352/4352 [==============================] - 0s 14us/step - loss: 0.0162 - acc: 0.9949 - val_loss: 0.0629 - val_acc: 0.9916\n",
            "Epoch 3020/3500\n",
            "4352/4352 [==============================] - 0s 14us/step - loss: 0.0163 - acc: 0.9959 - val_loss: 0.0677 - val_acc: 0.9911\n",
            "Epoch 3021/3500\n",
            "4352/4352 [==============================] - 0s 13us/step - loss: 0.0156 - acc: 0.9959 - val_loss: 0.0622 - val_acc: 0.9916\n",
            "Epoch 3022/3500\n",
            "4352/4352 [==============================] - 0s 13us/step - loss: 0.0153 - acc: 0.9970 - val_loss: 0.0598 - val_acc: 0.9916\n",
            "Epoch 3023/3500\n",
            "4352/4352 [==============================] - 0s 14us/step - loss: 0.0178 - acc: 0.9952 - val_loss: 0.0587 - val_acc: 0.9921\n",
            "Epoch 3024/3500\n",
            "4352/4352 [==============================] - 0s 14us/step - loss: 0.0160 - acc: 0.9959 - val_loss: 0.0568 - val_acc: 0.9916\n",
            "Epoch 3025/3500\n",
            "4352/4352 [==============================] - 0s 13us/step - loss: 0.0177 - acc: 0.9949 - val_loss: 0.0647 - val_acc: 0.9911\n",
            "Epoch 3026/3500\n",
            "4352/4352 [==============================] - 0s 13us/step - loss: 0.0160 - acc: 0.9959 - val_loss: 0.0635 - val_acc: 0.9911\n",
            "Epoch 3027/3500\n",
            "4352/4352 [==============================] - 0s 14us/step - loss: 0.0150 - acc: 0.9961 - val_loss: 0.0601 - val_acc: 0.9925\n",
            "Epoch 3028/3500\n",
            "4352/4352 [==============================] - 0s 13us/step - loss: 0.0156 - acc: 0.9956 - val_loss: 0.0659 - val_acc: 0.9897\n",
            "Epoch 3029/3500\n",
            "4352/4352 [==============================] - 0s 14us/step - loss: 0.0155 - acc: 0.9963 - val_loss: 0.0650 - val_acc: 0.9916\n",
            "Epoch 3030/3500\n",
            "4352/4352 [==============================] - 0s 14us/step - loss: 0.0147 - acc: 0.9961 - val_loss: 0.0638 - val_acc: 0.9911\n",
            "Epoch 3031/3500\n",
            "4352/4352 [==============================] - 0s 14us/step - loss: 0.0154 - acc: 0.9963 - val_loss: 0.0627 - val_acc: 0.9911\n",
            "Epoch 3032/3500\n",
            "4352/4352 [==============================] - 0s 16us/step - loss: 0.0176 - acc: 0.9952 - val_loss: 0.0615 - val_acc: 0.9921\n",
            "Epoch 3033/3500\n",
            "4352/4352 [==============================] - 0s 14us/step - loss: 0.0178 - acc: 0.9949 - val_loss: 0.0594 - val_acc: 0.9916\n",
            "Epoch 3034/3500\n",
            "4352/4352 [==============================] - 0s 13us/step - loss: 0.0163 - acc: 0.9956 - val_loss: 0.0689 - val_acc: 0.9907\n",
            "Epoch 3035/3500\n",
            "4352/4352 [==============================] - 0s 17us/step - loss: 0.0152 - acc: 0.9968 - val_loss: 0.0630 - val_acc: 0.9916\n",
            "Epoch 3036/3500\n",
            "4352/4352 [==============================] - 0s 13us/step - loss: 0.0161 - acc: 0.9963 - val_loss: 0.0593 - val_acc: 0.9921\n",
            "Epoch 3037/3500\n",
            "4352/4352 [==============================] - 0s 14us/step - loss: 0.0147 - acc: 0.9968 - val_loss: 0.0597 - val_acc: 0.9911\n",
            "Epoch 3038/3500\n",
            "4352/4352 [==============================] - 0s 15us/step - loss: 0.0157 - acc: 0.9966 - val_loss: 0.0626 - val_acc: 0.9916\n",
            "Epoch 3039/3500\n",
            "4352/4352 [==============================] - 0s 12us/step - loss: 0.0160 - acc: 0.9947 - val_loss: 0.0597 - val_acc: 0.9916\n",
            "Epoch 3040/3500\n",
            "4352/4352 [==============================] - 0s 14us/step - loss: 0.0167 - acc: 0.9949 - val_loss: 0.0629 - val_acc: 0.9916\n",
            "Epoch 3041/3500\n",
            "4352/4352 [==============================] - 0s 14us/step - loss: 0.0149 - acc: 0.9972 - val_loss: 0.0607 - val_acc: 0.9907\n",
            "Epoch 3042/3500\n",
            "4352/4352 [==============================] - 0s 14us/step - loss: 0.0154 - acc: 0.9968 - val_loss: 0.0658 - val_acc: 0.9902\n",
            "Epoch 3043/3500\n",
            "4352/4352 [==============================] - 0s 12us/step - loss: 0.0151 - acc: 0.9966 - val_loss: 0.0649 - val_acc: 0.9911\n",
            "Epoch 3044/3500\n",
            "4352/4352 [==============================] - 0s 13us/step - loss: 0.0143 - acc: 0.9963 - val_loss: 0.0615 - val_acc: 0.9921\n",
            "Epoch 3045/3500\n",
            "4352/4352 [==============================] - 0s 13us/step - loss: 0.0147 - acc: 0.9970 - val_loss: 0.0685 - val_acc: 0.9911\n",
            "Epoch 3046/3500\n",
            "4352/4352 [==============================] - 0s 13us/step - loss: 0.0149 - acc: 0.9963 - val_loss: 0.0670 - val_acc: 0.9911\n",
            "Epoch 3047/3500\n",
            "4352/4352 [==============================] - 0s 14us/step - loss: 0.0150 - acc: 0.9966 - val_loss: 0.0653 - val_acc: 0.9907\n",
            "Epoch 3048/3500\n",
            "4352/4352 [==============================] - 0s 14us/step - loss: 0.0148 - acc: 0.9968 - val_loss: 0.0648 - val_acc: 0.9907\n",
            "Epoch 3049/3500\n",
            "4352/4352 [==============================] - 0s 14us/step - loss: 0.0155 - acc: 0.9961 - val_loss: 0.0606 - val_acc: 0.9925\n",
            "Epoch 3050/3500\n",
            "4352/4352 [==============================] - 0s 14us/step - loss: 0.0171 - acc: 0.9949 - val_loss: 0.0643 - val_acc: 0.9907\n",
            "Epoch 3051/3500\n",
            "4352/4352 [==============================] - 0s 15us/step - loss: 0.0161 - acc: 0.9959 - val_loss: 0.0592 - val_acc: 0.9902\n",
            "Epoch 3052/3500\n",
            "4352/4352 [==============================] - 0s 13us/step - loss: 0.0170 - acc: 0.9949 - val_loss: 0.0597 - val_acc: 0.9897\n",
            "Epoch 3053/3500\n",
            "4352/4352 [==============================] - 0s 14us/step - loss: 0.0171 - acc: 0.9956 - val_loss: 0.0633 - val_acc: 0.9907\n",
            "Epoch 3054/3500\n",
            "4352/4352 [==============================] - 0s 14us/step - loss: 0.0150 - acc: 0.9959 - val_loss: 0.0615 - val_acc: 0.9921\n",
            "Epoch 3055/3500\n",
            "4352/4352 [==============================] - 0s 14us/step - loss: 0.0155 - acc: 0.9963 - val_loss: 0.0700 - val_acc: 0.9893\n",
            "Epoch 3056/3500\n",
            "4352/4352 [==============================] - 0s 14us/step - loss: 0.0152 - acc: 0.9954 - val_loss: 0.0698 - val_acc: 0.9893\n",
            "Epoch 3057/3500\n",
            "4352/4352 [==============================] - 0s 14us/step - loss: 0.0159 - acc: 0.9956 - val_loss: 0.0609 - val_acc: 0.9907\n",
            "Epoch 3058/3500\n",
            "4352/4352 [==============================] - 0s 13us/step - loss: 0.0157 - acc: 0.9970 - val_loss: 0.0598 - val_acc: 0.9902\n",
            "Epoch 3059/3500\n",
            "4352/4352 [==============================] - 0s 13us/step - loss: 0.0158 - acc: 0.9963 - val_loss: 0.0642 - val_acc: 0.9911\n",
            "Epoch 3060/3500\n",
            "4352/4352 [==============================] - 0s 13us/step - loss: 0.0152 - acc: 0.9961 - val_loss: 0.0659 - val_acc: 0.9907\n",
            "Epoch 3061/3500\n",
            "4352/4352 [==============================] - 0s 13us/step - loss: 0.0156 - acc: 0.9963 - val_loss: 0.0682 - val_acc: 0.9907\n",
            "Epoch 3062/3500\n",
            "4352/4352 [==============================] - 0s 13us/step - loss: 0.0175 - acc: 0.9947 - val_loss: 0.0840 - val_acc: 0.9874\n",
            "Epoch 3063/3500\n",
            "4352/4352 [==============================] - 0s 15us/step - loss: 0.0166 - acc: 0.9952 - val_loss: 0.0770 - val_acc: 0.9897\n",
            "Epoch 3064/3500\n",
            "4352/4352 [==============================] - 0s 12us/step - loss: 0.0160 - acc: 0.9963 - val_loss: 0.0650 - val_acc: 0.9911\n",
            "Epoch 3065/3500\n",
            "4352/4352 [==============================] - 0s 14us/step - loss: 0.0178 - acc: 0.9947 - val_loss: 0.0637 - val_acc: 0.9911\n",
            "Epoch 3066/3500\n",
            "4352/4352 [==============================] - 0s 13us/step - loss: 0.0158 - acc: 0.9952 - val_loss: 0.0619 - val_acc: 0.9916\n",
            "Epoch 3067/3500\n",
            "4352/4352 [==============================] - 0s 13us/step - loss: 0.0152 - acc: 0.9963 - val_loss: 0.0607 - val_acc: 0.9907\n",
            "Epoch 3068/3500\n",
            "4352/4352 [==============================] - 0s 15us/step - loss: 0.0151 - acc: 0.9961 - val_loss: 0.0650 - val_acc: 0.9902\n",
            "Epoch 3069/3500\n",
            "4352/4352 [==============================] - 0s 13us/step - loss: 0.0143 - acc: 0.9970 - val_loss: 0.0610 - val_acc: 0.9916\n",
            "Epoch 3070/3500\n",
            "4352/4352 [==============================] - 0s 13us/step - loss: 0.0155 - acc: 0.9959 - val_loss: 0.0646 - val_acc: 0.9907\n",
            "Epoch 3071/3500\n",
            "4352/4352 [==============================] - 0s 14us/step - loss: 0.0178 - acc: 0.9959 - val_loss: 0.0615 - val_acc: 0.9907\n",
            "Epoch 3072/3500\n",
            "4352/4352 [==============================] - 0s 15us/step - loss: 0.0201 - acc: 0.9943 - val_loss: 0.0693 - val_acc: 0.9902\n",
            "Epoch 3073/3500\n",
            "4352/4352 [==============================] - 0s 13us/step - loss: 0.0167 - acc: 0.9954 - val_loss: 0.0658 - val_acc: 0.9911\n",
            "Epoch 3074/3500\n",
            "4352/4352 [==============================] - 0s 15us/step - loss: 0.0148 - acc: 0.9966 - val_loss: 0.0675 - val_acc: 0.9907\n",
            "Epoch 3075/3500\n",
            "4352/4352 [==============================] - 0s 14us/step - loss: 0.0156 - acc: 0.9959 - val_loss: 0.0689 - val_acc: 0.9907\n",
            "Epoch 3076/3500\n",
            "4352/4352 [==============================] - 0s 15us/step - loss: 0.0155 - acc: 0.9966 - val_loss: 0.0754 - val_acc: 0.9897\n",
            "Epoch 3077/3500\n",
            "4352/4352 [==============================] - 0s 16us/step - loss: 0.0163 - acc: 0.9954 - val_loss: 0.0634 - val_acc: 0.9911\n",
            "Epoch 3078/3500\n",
            "4352/4352 [==============================] - 0s 14us/step - loss: 0.0147 - acc: 0.9963 - val_loss: 0.0591 - val_acc: 0.9911\n",
            "Epoch 3079/3500\n",
            "4352/4352 [==============================] - 0s 14us/step - loss: 0.0163 - acc: 0.9954 - val_loss: 0.0635 - val_acc: 0.9911\n",
            "Epoch 3080/3500\n",
            "4352/4352 [==============================] - 0s 14us/step - loss: 0.0175 - acc: 0.9952 - val_loss: 0.0662 - val_acc: 0.9911\n",
            "Epoch 3081/3500\n",
            "4352/4352 [==============================] - 0s 14us/step - loss: 0.0146 - acc: 0.9966 - val_loss: 0.0673 - val_acc: 0.9897\n",
            "Epoch 3082/3500\n",
            "4352/4352 [==============================] - 0s 13us/step - loss: 0.0154 - acc: 0.9961 - val_loss: 0.0655 - val_acc: 0.9907\n",
            "Epoch 3083/3500\n",
            "4352/4352 [==============================] - 0s 14us/step - loss: 0.0149 - acc: 0.9966 - val_loss: 0.0697 - val_acc: 0.9897\n",
            "Epoch 3084/3500\n",
            "4352/4352 [==============================] - 0s 16us/step - loss: 0.0148 - acc: 0.9961 - val_loss: 0.0673 - val_acc: 0.9902\n",
            "Epoch 3085/3500\n",
            "4352/4352 [==============================] - 0s 15us/step - loss: 0.0160 - acc: 0.9954 - val_loss: 0.0674 - val_acc: 0.9902\n",
            "Epoch 3086/3500\n",
            "4352/4352 [==============================] - 0s 12us/step - loss: 0.0145 - acc: 0.9961 - val_loss: 0.0624 - val_acc: 0.9911\n",
            "Epoch 3087/3500\n",
            "4352/4352 [==============================] - 0s 14us/step - loss: 0.0143 - acc: 0.9968 - val_loss: 0.0630 - val_acc: 0.9911\n",
            "Epoch 3088/3500\n",
            "4352/4352 [==============================] - 0s 13us/step - loss: 0.0147 - acc: 0.9961 - val_loss: 0.0675 - val_acc: 0.9907\n",
            "Epoch 3089/3500\n",
            "4352/4352 [==============================] - 0s 13us/step - loss: 0.0150 - acc: 0.9968 - val_loss: 0.0644 - val_acc: 0.9907\n",
            "Epoch 3090/3500\n",
            "4352/4352 [==============================] - 0s 14us/step - loss: 0.0146 - acc: 0.9961 - val_loss: 0.0639 - val_acc: 0.9911\n",
            "Epoch 3091/3500\n",
            "4352/4352 [==============================] - 0s 14us/step - loss: 0.0144 - acc: 0.9966 - val_loss: 0.0659 - val_acc: 0.9907\n",
            "Epoch 3092/3500\n",
            "4352/4352 [==============================] - 0s 13us/step - loss: 0.0147 - acc: 0.9968 - val_loss: 0.0631 - val_acc: 0.9911\n",
            "Epoch 3093/3500\n",
            "4352/4352 [==============================] - 0s 14us/step - loss: 0.0148 - acc: 0.9963 - val_loss: 0.0700 - val_acc: 0.9897\n",
            "Epoch 3094/3500\n",
            "4352/4352 [==============================] - 0s 13us/step - loss: 0.0156 - acc: 0.9963 - val_loss: 0.0772 - val_acc: 0.9893\n",
            "Epoch 3095/3500\n",
            "4352/4352 [==============================] - 0s 13us/step - loss: 0.0168 - acc: 0.9963 - val_loss: 0.0732 - val_acc: 0.9893\n",
            "Epoch 3096/3500\n",
            "4352/4352 [==============================] - 0s 14us/step - loss: 0.0152 - acc: 0.9954 - val_loss: 0.0609 - val_acc: 0.9902\n",
            "Epoch 3097/3500\n",
            "4352/4352 [==============================] - 0s 13us/step - loss: 0.0176 - acc: 0.9945 - val_loss: 0.0680 - val_acc: 0.9911\n",
            "Epoch 3098/3500\n",
            "4352/4352 [==============================] - 0s 14us/step - loss: 0.0199 - acc: 0.9943 - val_loss: 0.0667 - val_acc: 0.9902\n",
            "Epoch 3099/3500\n",
            "4352/4352 [==============================] - 0s 13us/step - loss: 0.0184 - acc: 0.9947 - val_loss: 0.0600 - val_acc: 0.9916\n",
            "Epoch 3100/3500\n",
            "4352/4352 [==============================] - 0s 14us/step - loss: 0.0175 - acc: 0.9943 - val_loss: 0.0609 - val_acc: 0.9911\n",
            "Epoch 3101/3500\n",
            "4352/4352 [==============================] - 0s 16us/step - loss: 0.0154 - acc: 0.9963 - val_loss: 0.0599 - val_acc: 0.9911\n",
            "Epoch 3102/3500\n",
            "4352/4352 [==============================] - 0s 14us/step - loss: 0.0163 - acc: 0.9949 - val_loss: 0.0642 - val_acc: 0.9911\n",
            "Epoch 3103/3500\n",
            "4352/4352 [==============================] - 0s 17us/step - loss: 0.0157 - acc: 0.9947 - val_loss: 0.0588 - val_acc: 0.9916\n",
            "Epoch 3104/3500\n",
            "4352/4352 [==============================] - 0s 14us/step - loss: 0.0174 - acc: 0.9947 - val_loss: 0.0664 - val_acc: 0.9911\n",
            "Epoch 3105/3500\n",
            "4352/4352 [==============================] - 0s 14us/step - loss: 0.0162 - acc: 0.9954 - val_loss: 0.0725 - val_acc: 0.9902\n",
            "Epoch 3106/3500\n",
            "4352/4352 [==============================] - 0s 15us/step - loss: 0.0163 - acc: 0.9956 - val_loss: 0.0751 - val_acc: 0.9897\n",
            "Epoch 3107/3500\n",
            "4352/4352 [==============================] - 0s 14us/step - loss: 0.0150 - acc: 0.9966 - val_loss: 0.0675 - val_acc: 0.9907\n",
            "Epoch 3108/3500\n",
            "4352/4352 [==============================] - 0s 15us/step - loss: 0.0155 - acc: 0.9963 - val_loss: 0.0618 - val_acc: 0.9911\n",
            "Epoch 3109/3500\n",
            "4352/4352 [==============================] - 0s 15us/step - loss: 0.0148 - acc: 0.9961 - val_loss: 0.0648 - val_acc: 0.9907\n",
            "Epoch 3110/3500\n",
            "4352/4352 [==============================] - 0s 14us/step - loss: 0.0172 - acc: 0.9954 - val_loss: 0.0631 - val_acc: 0.9916\n",
            "Epoch 3111/3500\n",
            "4352/4352 [==============================] - 0s 15us/step - loss: 0.0170 - acc: 0.9945 - val_loss: 0.0751 - val_acc: 0.9897\n",
            "Epoch 3112/3500\n",
            "4352/4352 [==============================] - 0s 16us/step - loss: 0.0166 - acc: 0.9961 - val_loss: 0.0745 - val_acc: 0.9897\n",
            "Epoch 3113/3500\n",
            "4352/4352 [==============================] - 0s 15us/step - loss: 0.0156 - acc: 0.9961 - val_loss: 0.0663 - val_acc: 0.9911\n",
            "Epoch 3114/3500\n",
            "4352/4352 [==============================] - 0s 15us/step - loss: 0.0144 - acc: 0.9968 - val_loss: 0.0625 - val_acc: 0.9907\n",
            "Epoch 3115/3500\n",
            "4352/4352 [==============================] - 0s 14us/step - loss: 0.0168 - acc: 0.9949 - val_loss: 0.0611 - val_acc: 0.9911\n",
            "Epoch 3116/3500\n",
            "4352/4352 [==============================] - 0s 15us/step - loss: 0.0211 - acc: 0.9929 - val_loss: 0.0605 - val_acc: 0.9907\n",
            "Epoch 3117/3500\n",
            "4352/4352 [==============================] - 0s 15us/step - loss: 0.0197 - acc: 0.9945 - val_loss: 0.0613 - val_acc: 0.9916\n",
            "Epoch 3118/3500\n",
            "4352/4352 [==============================] - 0s 15us/step - loss: 0.0163 - acc: 0.9959 - val_loss: 0.0630 - val_acc: 0.9916\n",
            "Epoch 3119/3500\n",
            "4352/4352 [==============================] - 0s 14us/step - loss: 0.0176 - acc: 0.9945 - val_loss: 0.0613 - val_acc: 0.9907\n",
            "Epoch 3120/3500\n",
            "4352/4352 [==============================] - 0s 15us/step - loss: 0.0180 - acc: 0.9949 - val_loss: 0.0716 - val_acc: 0.9907\n",
            "Epoch 3121/3500\n",
            "4352/4352 [==============================] - 0s 14us/step - loss: 0.0174 - acc: 0.9952 - val_loss: 0.0907 - val_acc: 0.9855\n",
            "Epoch 3122/3500\n",
            "4352/4352 [==============================] - 0s 14us/step - loss: 0.0199 - acc: 0.9945 - val_loss: 0.0808 - val_acc: 0.9879\n",
            "Epoch 3123/3500\n",
            "4352/4352 [==============================] - 0s 14us/step - loss: 0.0168 - acc: 0.9947 - val_loss: 0.0705 - val_acc: 0.9897\n",
            "Epoch 3124/3500\n",
            "4352/4352 [==============================] - 0s 15us/step - loss: 0.0151 - acc: 0.9954 - val_loss: 0.0653 - val_acc: 0.9911\n",
            "Epoch 3125/3500\n",
            "4352/4352 [==============================] - 0s 14us/step - loss: 0.0159 - acc: 0.9961 - val_loss: 0.0642 - val_acc: 0.9902\n",
            "Epoch 3126/3500\n",
            "4352/4352 [==============================] - 0s 14us/step - loss: 0.0171 - acc: 0.9952 - val_loss: 0.0754 - val_acc: 0.9893\n",
            "Epoch 3127/3500\n",
            "4352/4352 [==============================] - 0s 14us/step - loss: 0.0167 - acc: 0.9956 - val_loss: 0.0793 - val_acc: 0.9879\n",
            "Epoch 3128/3500\n",
            "4352/4352 [==============================] - 0s 14us/step - loss: 0.0213 - acc: 0.9940 - val_loss: 0.0949 - val_acc: 0.9841\n",
            "Epoch 3129/3500\n",
            "4352/4352 [==============================] - 0s 13us/step - loss: 0.0189 - acc: 0.9949 - val_loss: 0.0842 - val_acc: 0.9865\n",
            "Epoch 3130/3500\n",
            "4352/4352 [==============================] - 0s 12us/step - loss: 0.0203 - acc: 0.9936 - val_loss: 0.0791 - val_acc: 0.9888\n",
            "Epoch 3131/3500\n",
            "4352/4352 [==============================] - 0s 14us/step - loss: 0.0170 - acc: 0.9949 - val_loss: 0.0678 - val_acc: 0.9897\n",
            "Epoch 3132/3500\n",
            "4352/4352 [==============================] - 0s 15us/step - loss: 0.0149 - acc: 0.9970 - val_loss: 0.0611 - val_acc: 0.9911\n",
            "Epoch 3133/3500\n",
            "4352/4352 [==============================] - 0s 16us/step - loss: 0.0150 - acc: 0.9961 - val_loss: 0.0596 - val_acc: 0.9916\n",
            "Epoch 3134/3500\n",
            "4352/4352 [==============================] - 0s 14us/step - loss: 0.0170 - acc: 0.9949 - val_loss: 0.0591 - val_acc: 0.9907\n",
            "Epoch 3135/3500\n",
            "4352/4352 [==============================] - 0s 15us/step - loss: 0.0188 - acc: 0.9936 - val_loss: 0.0605 - val_acc: 0.9888\n",
            "Epoch 3136/3500\n",
            "4352/4352 [==============================] - 0s 13us/step - loss: 0.0211 - acc: 0.9947 - val_loss: 0.0619 - val_acc: 0.9893\n",
            "Epoch 3137/3500\n",
            "4352/4352 [==============================] - 0s 13us/step - loss: 0.0189 - acc: 0.9938 - val_loss: 0.0623 - val_acc: 0.9911\n",
            "Epoch 3138/3500\n",
            "4352/4352 [==============================] - 0s 14us/step - loss: 0.0158 - acc: 0.9949 - val_loss: 0.0674 - val_acc: 0.9902\n",
            "Epoch 3139/3500\n",
            "4352/4352 [==============================] - 0s 14us/step - loss: 0.0146 - acc: 0.9966 - val_loss: 0.0652 - val_acc: 0.9911\n",
            "Epoch 3140/3500\n",
            "4352/4352 [==============================] - 0s 13us/step - loss: 0.0150 - acc: 0.9961 - val_loss: 0.0736 - val_acc: 0.9893\n",
            "Epoch 3141/3500\n",
            "4352/4352 [==============================] - 0s 14us/step - loss: 0.0155 - acc: 0.9956 - val_loss: 0.0744 - val_acc: 0.9902\n",
            "Epoch 3142/3500\n",
            "4352/4352 [==============================] - 0s 13us/step - loss: 0.0170 - acc: 0.9956 - val_loss: 0.0765 - val_acc: 0.9893\n",
            "Epoch 3143/3500\n",
            "4352/4352 [==============================] - 0s 14us/step - loss: 0.0172 - acc: 0.9945 - val_loss: 0.0695 - val_acc: 0.9897\n",
            "Epoch 3144/3500\n",
            "4352/4352 [==============================] - 0s 12us/step - loss: 0.0195 - acc: 0.9945 - val_loss: 0.0648 - val_acc: 0.9902\n",
            "Epoch 3145/3500\n",
            "4352/4352 [==============================] - 0s 13us/step - loss: 0.0155 - acc: 0.9966 - val_loss: 0.0615 - val_acc: 0.9902\n",
            "Epoch 3146/3500\n",
            "4352/4352 [==============================] - 0s 13us/step - loss: 0.0209 - acc: 0.9926 - val_loss: 0.0609 - val_acc: 0.9897\n",
            "Epoch 3147/3500\n",
            "4352/4352 [==============================] - 0s 15us/step - loss: 0.0192 - acc: 0.9938 - val_loss: 0.0583 - val_acc: 0.9902\n",
            "Epoch 3148/3500\n",
            "4352/4352 [==============================] - 0s 14us/step - loss: 0.0198 - acc: 0.9938 - val_loss: 0.0603 - val_acc: 0.9902\n",
            "Epoch 3149/3500\n",
            "4352/4352 [==============================] - 0s 15us/step - loss: 0.0163 - acc: 0.9959 - val_loss: 0.0655 - val_acc: 0.9907\n",
            "Epoch 3150/3500\n",
            "4352/4352 [==============================] - 0s 15us/step - loss: 0.0150 - acc: 0.9966 - val_loss: 0.0627 - val_acc: 0.9907\n",
            "Epoch 3151/3500\n",
            "4352/4352 [==============================] - 0s 15us/step - loss: 0.0145 - acc: 0.9970 - val_loss: 0.0709 - val_acc: 0.9897\n",
            "Epoch 3152/3500\n",
            "4352/4352 [==============================] - 0s 13us/step - loss: 0.0146 - acc: 0.9956 - val_loss: 0.0615 - val_acc: 0.9911\n",
            "Epoch 3153/3500\n",
            "4352/4352 [==============================] - 0s 15us/step - loss: 0.0151 - acc: 0.9961 - val_loss: 0.0607 - val_acc: 0.9911\n",
            "Epoch 3154/3500\n",
            "4352/4352 [==============================] - 0s 13us/step - loss: 0.0144 - acc: 0.9968 - val_loss: 0.0652 - val_acc: 0.9911\n",
            "Epoch 3155/3500\n",
            "4352/4352 [==============================] - 0s 13us/step - loss: 0.0149 - acc: 0.9963 - val_loss: 0.0640 - val_acc: 0.9911\n",
            "Epoch 3156/3500\n",
            "4352/4352 [==============================] - 0s 14us/step - loss: 0.0147 - acc: 0.9972 - val_loss: 0.0606 - val_acc: 0.9907\n",
            "Epoch 3157/3500\n",
            "4352/4352 [==============================] - 0s 14us/step - loss: 0.0168 - acc: 0.9959 - val_loss: 0.0638 - val_acc: 0.9916\n",
            "Epoch 3158/3500\n",
            "4352/4352 [==============================] - 0s 13us/step - loss: 0.0154 - acc: 0.9961 - val_loss: 0.0669 - val_acc: 0.9907\n",
            "Epoch 3159/3500\n",
            "4352/4352 [==============================] - 0s 13us/step - loss: 0.0148 - acc: 0.9959 - val_loss: 0.0691 - val_acc: 0.9897\n",
            "Epoch 3160/3500\n",
            "4352/4352 [==============================] - 0s 13us/step - loss: 0.0146 - acc: 0.9963 - val_loss: 0.0657 - val_acc: 0.9907\n",
            "Epoch 3161/3500\n",
            "4352/4352 [==============================] - 0s 14us/step - loss: 0.0156 - acc: 0.9956 - val_loss: 0.0685 - val_acc: 0.9897\n",
            "Epoch 3162/3500\n",
            "4352/4352 [==============================] - 0s 15us/step - loss: 0.0149 - acc: 0.9956 - val_loss: 0.0610 - val_acc: 0.9911\n",
            "Epoch 3163/3500\n",
            "4352/4352 [==============================] - 0s 14us/step - loss: 0.0150 - acc: 0.9968 - val_loss: 0.0615 - val_acc: 0.9916\n",
            "Epoch 3164/3500\n",
            "4352/4352 [==============================] - 0s 14us/step - loss: 0.0156 - acc: 0.9954 - val_loss: 0.0657 - val_acc: 0.9911\n",
            "Epoch 3165/3500\n",
            "4352/4352 [==============================] - 0s 14us/step - loss: 0.0147 - acc: 0.9966 - val_loss: 0.0623 - val_acc: 0.9916\n",
            "Epoch 3166/3500\n",
            "4352/4352 [==============================] - 0s 16us/step - loss: 0.0151 - acc: 0.9959 - val_loss: 0.0609 - val_acc: 0.9911\n",
            "Epoch 3167/3500\n",
            "4352/4352 [==============================] - 0s 14us/step - loss: 0.0159 - acc: 0.9961 - val_loss: 0.0626 - val_acc: 0.9911\n",
            "Epoch 3168/3500\n",
            "4352/4352 [==============================] - 0s 14us/step - loss: 0.0167 - acc: 0.9945 - val_loss: 0.0667 - val_acc: 0.9897\n",
            "Epoch 3169/3500\n",
            "4352/4352 [==============================] - 0s 15us/step - loss: 0.0161 - acc: 0.9954 - val_loss: 0.0655 - val_acc: 0.9911\n",
            "Epoch 3170/3500\n",
            "4352/4352 [==============================] - 0s 14us/step - loss: 0.0142 - acc: 0.9961 - val_loss: 0.0733 - val_acc: 0.9897\n",
            "Epoch 3171/3500\n",
            "4352/4352 [==============================] - 0s 14us/step - loss: 0.0153 - acc: 0.9963 - val_loss: 0.0679 - val_acc: 0.9902\n",
            "Epoch 3172/3500\n",
            "4352/4352 [==============================] - 0s 13us/step - loss: 0.0163 - acc: 0.9959 - val_loss: 0.0647 - val_acc: 0.9902\n",
            "Epoch 3173/3500\n",
            "4352/4352 [==============================] - 0s 13us/step - loss: 0.0158 - acc: 0.9959 - val_loss: 0.0675 - val_acc: 0.9902\n",
            "Epoch 3174/3500\n",
            "4352/4352 [==============================] - 0s 15us/step - loss: 0.0158 - acc: 0.9956 - val_loss: 0.0615 - val_acc: 0.9911\n",
            "Epoch 3175/3500\n",
            "4352/4352 [==============================] - 0s 14us/step - loss: 0.0145 - acc: 0.9968 - val_loss: 0.0664 - val_acc: 0.9897\n",
            "Epoch 3176/3500\n",
            "4352/4352 [==============================] - 0s 15us/step - loss: 0.0143 - acc: 0.9970 - val_loss: 0.0611 - val_acc: 0.9911\n",
            "Epoch 3177/3500\n",
            "4352/4352 [==============================] - 0s 14us/step - loss: 0.0149 - acc: 0.9968 - val_loss: 0.0651 - val_acc: 0.9911\n",
            "Epoch 3178/3500\n",
            "4352/4352 [==============================] - 0s 16us/step - loss: 0.0149 - acc: 0.9966 - val_loss: 0.0623 - val_acc: 0.9916\n",
            "Epoch 3179/3500\n",
            "4352/4352 [==============================] - 0s 15us/step - loss: 0.0140 - acc: 0.9963 - val_loss: 0.0813 - val_acc: 0.9879\n",
            "Epoch 3180/3500\n",
            "4352/4352 [==============================] - 0s 14us/step - loss: 0.0150 - acc: 0.9966 - val_loss: 0.0663 - val_acc: 0.9902\n",
            "Epoch 3181/3500\n",
            "4352/4352 [==============================] - 0s 14us/step - loss: 0.0140 - acc: 0.9970 - val_loss: 0.0619 - val_acc: 0.9911\n",
            "Epoch 3182/3500\n",
            "4352/4352 [==============================] - 0s 16us/step - loss: 0.0139 - acc: 0.9966 - val_loss: 0.0743 - val_acc: 0.9897\n",
            "Epoch 3183/3500\n",
            "4352/4352 [==============================] - 0s 15us/step - loss: 0.0154 - acc: 0.9961 - val_loss: 0.0676 - val_acc: 0.9893\n",
            "Epoch 3184/3500\n",
            "4352/4352 [==============================] - 0s 16us/step - loss: 0.0158 - acc: 0.9949 - val_loss: 0.0691 - val_acc: 0.9897\n",
            "Epoch 3185/3500\n",
            "4352/4352 [==============================] - 0s 12us/step - loss: 0.0161 - acc: 0.9961 - val_loss: 0.0685 - val_acc: 0.9897\n",
            "Epoch 3186/3500\n",
            "4352/4352 [==============================] - 0s 15us/step - loss: 0.0154 - acc: 0.9963 - val_loss: 0.0620 - val_acc: 0.9911\n",
            "Epoch 3187/3500\n",
            "4352/4352 [==============================] - 0s 13us/step - loss: 0.0147 - acc: 0.9963 - val_loss: 0.0621 - val_acc: 0.9916\n",
            "Epoch 3188/3500\n",
            "4352/4352 [==============================] - 0s 14us/step - loss: 0.0152 - acc: 0.9963 - val_loss: 0.0618 - val_acc: 0.9911\n",
            "Epoch 3189/3500\n",
            "4352/4352 [==============================] - 0s 14us/step - loss: 0.0163 - acc: 0.9954 - val_loss: 0.0712 - val_acc: 0.9897\n",
            "Epoch 3190/3500\n",
            "4352/4352 [==============================] - 0s 13us/step - loss: 0.0167 - acc: 0.9947 - val_loss: 0.0710 - val_acc: 0.9897\n",
            "Epoch 3191/3500\n",
            "4352/4352 [==============================] - 0s 14us/step - loss: 0.0167 - acc: 0.9945 - val_loss: 0.0674 - val_acc: 0.9907\n",
            "Epoch 3192/3500\n",
            "4352/4352 [==============================] - 0s 13us/step - loss: 0.0161 - acc: 0.9949 - val_loss: 0.0714 - val_acc: 0.9897\n",
            "Epoch 3193/3500\n",
            "4352/4352 [==============================] - 0s 14us/step - loss: 0.0162 - acc: 0.9954 - val_loss: 0.0793 - val_acc: 0.9888\n",
            "Epoch 3194/3500\n",
            "4352/4352 [==============================] - 0s 14us/step - loss: 0.0152 - acc: 0.9963 - val_loss: 0.0708 - val_acc: 0.9902\n",
            "Epoch 3195/3500\n",
            "4352/4352 [==============================] - 0s 14us/step - loss: 0.0164 - acc: 0.9956 - val_loss: 0.0693 - val_acc: 0.9907\n",
            "Epoch 3196/3500\n",
            "4352/4352 [==============================] - 0s 13us/step - loss: 0.0168 - acc: 0.9961 - val_loss: 0.0730 - val_acc: 0.9897\n",
            "Epoch 3197/3500\n",
            "4352/4352 [==============================] - 0s 14us/step - loss: 0.0147 - acc: 0.9970 - val_loss: 0.0671 - val_acc: 0.9907\n",
            "Epoch 3198/3500\n",
            "4352/4352 [==============================] - 0s 15us/step - loss: 0.0146 - acc: 0.9963 - val_loss: 0.0667 - val_acc: 0.9911\n",
            "Epoch 3199/3500\n",
            "4352/4352 [==============================] - 0s 13us/step - loss: 0.0146 - acc: 0.9968 - val_loss: 0.0669 - val_acc: 0.9907\n",
            "Epoch 3200/3500\n",
            "4352/4352 [==============================] - 0s 14us/step - loss: 0.0147 - acc: 0.9963 - val_loss: 0.0644 - val_acc: 0.9911\n",
            "Epoch 3201/3500\n",
            "4352/4352 [==============================] - 0s 15us/step - loss: 0.0140 - acc: 0.9970 - val_loss: 0.0627 - val_acc: 0.9911\n",
            "Epoch 3202/3500\n",
            "4352/4352 [==============================] - 0s 14us/step - loss: 0.0144 - acc: 0.9963 - val_loss: 0.0672 - val_acc: 0.9911\n",
            "Epoch 3203/3500\n",
            "4352/4352 [==============================] - 0s 13us/step - loss: 0.0142 - acc: 0.9972 - val_loss: 0.0707 - val_acc: 0.9902\n",
            "Epoch 3204/3500\n",
            "4352/4352 [==============================] - 0s 14us/step - loss: 0.0151 - acc: 0.9963 - val_loss: 0.0694 - val_acc: 0.9902\n",
            "Epoch 3205/3500\n",
            "4352/4352 [==============================] - 0s 14us/step - loss: 0.0152 - acc: 0.9961 - val_loss: 0.0677 - val_acc: 0.9897\n",
            "Epoch 3206/3500\n",
            "4352/4352 [==============================] - 0s 14us/step - loss: 0.0150 - acc: 0.9954 - val_loss: 0.0658 - val_acc: 0.9907\n",
            "Epoch 3207/3500\n",
            "4352/4352 [==============================] - 0s 14us/step - loss: 0.0148 - acc: 0.9968 - val_loss: 0.0624 - val_acc: 0.9911\n",
            "Epoch 3208/3500\n",
            "4352/4352 [==============================] - 0s 14us/step - loss: 0.0149 - acc: 0.9961 - val_loss: 0.0700 - val_acc: 0.9897\n",
            "Epoch 3209/3500\n",
            "4352/4352 [==============================] - 0s 15us/step - loss: 0.0143 - acc: 0.9968 - val_loss: 0.0716 - val_acc: 0.9902\n",
            "Epoch 3210/3500\n",
            "4352/4352 [==============================] - 0s 15us/step - loss: 0.0146 - acc: 0.9959 - val_loss: 0.0687 - val_acc: 0.9902\n",
            "Epoch 3211/3500\n",
            "4352/4352 [==============================] - 0s 16us/step - loss: 0.0146 - acc: 0.9963 - val_loss: 0.0632 - val_acc: 0.9911\n",
            "Epoch 3212/3500\n",
            "4352/4352 [==============================] - 0s 13us/step - loss: 0.0144 - acc: 0.9970 - val_loss: 0.0733 - val_acc: 0.9893\n",
            "Epoch 3213/3500\n",
            "4352/4352 [==============================] - 0s 14us/step - loss: 0.0149 - acc: 0.9968 - val_loss: 0.0712 - val_acc: 0.9902\n",
            "Epoch 3214/3500\n",
            "4352/4352 [==============================] - 0s 16us/step - loss: 0.0156 - acc: 0.9963 - val_loss: 0.0618 - val_acc: 0.9916\n",
            "Epoch 3215/3500\n",
            "4352/4352 [==============================] - 0s 15us/step - loss: 0.0172 - acc: 0.9954 - val_loss: 0.0637 - val_acc: 0.9911\n",
            "Epoch 3216/3500\n",
            "4352/4352 [==============================] - 0s 15us/step - loss: 0.0149 - acc: 0.9959 - val_loss: 0.0643 - val_acc: 0.9893\n",
            "Epoch 3217/3500\n",
            "4352/4352 [==============================] - 0s 13us/step - loss: 0.0272 - acc: 0.9910 - val_loss: 0.0604 - val_acc: 0.9897\n",
            "Epoch 3218/3500\n",
            "4352/4352 [==============================] - 0s 12us/step - loss: 0.0261 - acc: 0.9917 - val_loss: 0.0597 - val_acc: 0.9907\n",
            "Epoch 3219/3500\n",
            "4352/4352 [==============================] - 0s 13us/step - loss: 0.0201 - acc: 0.9952 - val_loss: 0.0685 - val_acc: 0.9902\n",
            "Epoch 3220/3500\n",
            "4352/4352 [==============================] - 0s 13us/step - loss: 0.0181 - acc: 0.9952 - val_loss: 0.0706 - val_acc: 0.9888\n",
            "Epoch 3221/3500\n",
            "4352/4352 [==============================] - 0s 14us/step - loss: 0.0183 - acc: 0.9952 - val_loss: 0.0671 - val_acc: 0.9902\n",
            "Epoch 3222/3500\n",
            "4352/4352 [==============================] - 0s 13us/step - loss: 0.0178 - acc: 0.9956 - val_loss: 0.0685 - val_acc: 0.9902\n",
            "Epoch 3223/3500\n",
            "4352/4352 [==============================] - 0s 13us/step - loss: 0.0200 - acc: 0.9945 - val_loss: 0.0674 - val_acc: 0.9907\n",
            "Epoch 3224/3500\n",
            "4352/4352 [==============================] - 0s 15us/step - loss: 0.0170 - acc: 0.9945 - val_loss: 0.0619 - val_acc: 0.9907\n",
            "Epoch 3225/3500\n",
            "4352/4352 [==============================] - 0s 13us/step - loss: 0.0143 - acc: 0.9966 - val_loss: 0.0614 - val_acc: 0.9911\n",
            "Epoch 3226/3500\n",
            "4352/4352 [==============================] - 0s 14us/step - loss: 0.0148 - acc: 0.9966 - val_loss: 0.0611 - val_acc: 0.9902\n",
            "Epoch 3227/3500\n",
            "4352/4352 [==============================] - 0s 14us/step - loss: 0.0162 - acc: 0.9954 - val_loss: 0.0606 - val_acc: 0.9916\n",
            "Epoch 3228/3500\n",
            "4352/4352 [==============================] - 0s 14us/step - loss: 0.0160 - acc: 0.9966 - val_loss: 0.0595 - val_acc: 0.9911\n",
            "Epoch 3229/3500\n",
            "4352/4352 [==============================] - 0s 14us/step - loss: 0.0153 - acc: 0.9956 - val_loss: 0.0591 - val_acc: 0.9911\n",
            "Epoch 3230/3500\n",
            "4352/4352 [==============================] - 0s 13us/step - loss: 0.0164 - acc: 0.9959 - val_loss: 0.0626 - val_acc: 0.9907\n",
            "Epoch 3231/3500\n",
            "4352/4352 [==============================] - 0s 16us/step - loss: 0.0147 - acc: 0.9959 - val_loss: 0.0685 - val_acc: 0.9897\n",
            "Epoch 3232/3500\n",
            "4352/4352 [==============================] - 0s 15us/step - loss: 0.0148 - acc: 0.9968 - val_loss: 0.0643 - val_acc: 0.9907\n",
            "Epoch 3233/3500\n",
            "4352/4352 [==============================] - 0s 14us/step - loss: 0.0140 - acc: 0.9968 - val_loss: 0.0647 - val_acc: 0.9907\n",
            "Epoch 3234/3500\n",
            "4352/4352 [==============================] - 0s 14us/step - loss: 0.0150 - acc: 0.9966 - val_loss: 0.0667 - val_acc: 0.9902\n",
            "Epoch 3235/3500\n",
            "4352/4352 [==============================] - 0s 14us/step - loss: 0.0154 - acc: 0.9963 - val_loss: 0.0641 - val_acc: 0.9907\n",
            "Epoch 3236/3500\n",
            "4352/4352 [==============================] - 0s 14us/step - loss: 0.0168 - acc: 0.9949 - val_loss: 0.0663 - val_acc: 0.9907\n",
            "Epoch 3237/3500\n",
            "4352/4352 [==============================] - 0s 16us/step - loss: 0.0174 - acc: 0.9952 - val_loss: 0.0716 - val_acc: 0.9893\n",
            "Epoch 3238/3500\n",
            "4352/4352 [==============================] - 0s 13us/step - loss: 0.0153 - acc: 0.9963 - val_loss: 0.0642 - val_acc: 0.9907\n",
            "Epoch 3239/3500\n",
            "4352/4352 [==============================] - 0s 14us/step - loss: 0.0141 - acc: 0.9968 - val_loss: 0.0703 - val_acc: 0.9897\n",
            "Epoch 3240/3500\n",
            "4352/4352 [==============================] - 0s 14us/step - loss: 0.0158 - acc: 0.9956 - val_loss: 0.0710 - val_acc: 0.9893\n",
            "Epoch 3241/3500\n",
            "4352/4352 [==============================] - 0s 13us/step - loss: 0.0165 - acc: 0.9952 - val_loss: 0.0810 - val_acc: 0.9883\n",
            "Epoch 3242/3500\n",
            "4352/4352 [==============================] - 0s 13us/step - loss: 0.0167 - acc: 0.9949 - val_loss: 0.0849 - val_acc: 0.9874\n",
            "Epoch 3243/3500\n",
            "4352/4352 [==============================] - 0s 13us/step - loss: 0.0160 - acc: 0.9952 - val_loss: 0.0697 - val_acc: 0.9897\n",
            "Epoch 3244/3500\n",
            "4352/4352 [==============================] - 0s 13us/step - loss: 0.0182 - acc: 0.9947 - val_loss: 0.0792 - val_acc: 0.9893\n",
            "Epoch 3245/3500\n",
            "4352/4352 [==============================] - 0s 15us/step - loss: 0.0248 - acc: 0.9933 - val_loss: 0.0695 - val_acc: 0.9902\n",
            "Epoch 3246/3500\n",
            "4352/4352 [==============================] - 0s 13us/step - loss: 0.0207 - acc: 0.9936 - val_loss: 0.0618 - val_acc: 0.9916\n",
            "Epoch 3247/3500\n",
            "4352/4352 [==============================] - 0s 17us/step - loss: 0.0169 - acc: 0.9949 - val_loss: 0.0618 - val_acc: 0.9907\n",
            "Epoch 3248/3500\n",
            "4352/4352 [==============================] - 0s 15us/step - loss: 0.0175 - acc: 0.9947 - val_loss: 0.0606 - val_acc: 0.9911\n",
            "Epoch 3249/3500\n",
            "4352/4352 [==============================] - 0s 14us/step - loss: 0.0169 - acc: 0.9949 - val_loss: 0.0604 - val_acc: 0.9911\n",
            "Epoch 3250/3500\n",
            "4352/4352 [==============================] - 0s 14us/step - loss: 0.0173 - acc: 0.9952 - val_loss: 0.0602 - val_acc: 0.9893\n",
            "Epoch 3251/3500\n",
            "4352/4352 [==============================] - 0s 14us/step - loss: 0.0165 - acc: 0.9954 - val_loss: 0.0627 - val_acc: 0.9916\n",
            "Epoch 3252/3500\n",
            "4352/4352 [==============================] - 0s 14us/step - loss: 0.0152 - acc: 0.9959 - val_loss: 0.0821 - val_acc: 0.9879\n",
            "Epoch 3253/3500\n",
            "4352/4352 [==============================] - 0s 15us/step - loss: 0.0168 - acc: 0.9952 - val_loss: 0.0732 - val_acc: 0.9902\n",
            "Epoch 3254/3500\n",
            "4352/4352 [==============================] - 0s 14us/step - loss: 0.0144 - acc: 0.9968 - val_loss: 0.0670 - val_acc: 0.9902\n",
            "Epoch 3255/3500\n",
            "4352/4352 [==============================] - 0s 16us/step - loss: 0.0147 - acc: 0.9963 - val_loss: 0.0625 - val_acc: 0.9916\n",
            "Epoch 3256/3500\n",
            "4352/4352 [==============================] - 0s 15us/step - loss: 0.0145 - acc: 0.9963 - val_loss: 0.0617 - val_acc: 0.9911\n",
            "Epoch 3257/3500\n",
            "4352/4352 [==============================] - 0s 14us/step - loss: 0.0148 - acc: 0.9959 - val_loss: 0.0682 - val_acc: 0.9897\n",
            "Epoch 3258/3500\n",
            "4352/4352 [==============================] - 0s 14us/step - loss: 0.0143 - acc: 0.9968 - val_loss: 0.0678 - val_acc: 0.9907\n",
            "Epoch 3259/3500\n",
            "4352/4352 [==============================] - 0s 13us/step - loss: 0.0142 - acc: 0.9966 - val_loss: 0.0631 - val_acc: 0.9911\n",
            "Epoch 3260/3500\n",
            "4352/4352 [==============================] - 0s 14us/step - loss: 0.0145 - acc: 0.9963 - val_loss: 0.0813 - val_acc: 0.9883\n",
            "Epoch 3261/3500\n",
            "4352/4352 [==============================] - 0s 15us/step - loss: 0.0158 - acc: 0.9954 - val_loss: 0.0650 - val_acc: 0.9902\n",
            "Epoch 3262/3500\n",
            "4352/4352 [==============================] - 0s 13us/step - loss: 0.0154 - acc: 0.9959 - val_loss: 0.0646 - val_acc: 0.9911\n",
            "Epoch 3263/3500\n",
            "4352/4352 [==============================] - 0s 15us/step - loss: 0.0145 - acc: 0.9961 - val_loss: 0.0628 - val_acc: 0.9911\n",
            "Epoch 3264/3500\n",
            "4352/4352 [==============================] - 0s 16us/step - loss: 0.0149 - acc: 0.9968 - val_loss: 0.0650 - val_acc: 0.9911\n",
            "Epoch 3265/3500\n",
            "4352/4352 [==============================] - 0s 13us/step - loss: 0.0146 - acc: 0.9963 - val_loss: 0.0780 - val_acc: 0.9888\n",
            "Epoch 3266/3500\n",
            "4352/4352 [==============================] - 0s 14us/step - loss: 0.0150 - acc: 0.9956 - val_loss: 0.0637 - val_acc: 0.9916\n",
            "Epoch 3267/3500\n",
            "4352/4352 [==============================] - 0s 13us/step - loss: 0.0145 - acc: 0.9963 - val_loss: 0.0679 - val_acc: 0.9893\n",
            "Epoch 3268/3500\n",
            "4352/4352 [==============================] - 0s 13us/step - loss: 0.0144 - acc: 0.9968 - val_loss: 0.0637 - val_acc: 0.9911\n",
            "Epoch 3269/3500\n",
            "4352/4352 [==============================] - 0s 13us/step - loss: 0.0140 - acc: 0.9968 - val_loss: 0.0729 - val_acc: 0.9893\n",
            "Epoch 3270/3500\n",
            "4352/4352 [==============================] - 0s 13us/step - loss: 0.0142 - acc: 0.9961 - val_loss: 0.0654 - val_acc: 0.9911\n",
            "Epoch 3271/3500\n",
            "4352/4352 [==============================] - 0s 14us/step - loss: 0.0143 - acc: 0.9968 - val_loss: 0.0639 - val_acc: 0.9911\n",
            "Epoch 3272/3500\n",
            "4352/4352 [==============================] - 0s 14us/step - loss: 0.0145 - acc: 0.9961 - val_loss: 0.0624 - val_acc: 0.9916\n",
            "Epoch 3273/3500\n",
            "4352/4352 [==============================] - 0s 14us/step - loss: 0.0141 - acc: 0.9966 - val_loss: 0.0669 - val_acc: 0.9902\n",
            "Epoch 3274/3500\n",
            "4352/4352 [==============================] - 0s 14us/step - loss: 0.0145 - acc: 0.9966 - val_loss: 0.0667 - val_acc: 0.9911\n",
            "Epoch 3275/3500\n",
            "4352/4352 [==============================] - 0s 13us/step - loss: 0.0138 - acc: 0.9970 - val_loss: 0.0616 - val_acc: 0.9907\n",
            "Epoch 3276/3500\n",
            "4352/4352 [==============================] - 0s 16us/step - loss: 0.0148 - acc: 0.9959 - val_loss: 0.0615 - val_acc: 0.9897\n",
            "Epoch 3277/3500\n",
            "4352/4352 [==============================] - 0s 15us/step - loss: 0.0190 - acc: 0.9940 - val_loss: 0.0617 - val_acc: 0.9907\n",
            "Epoch 3278/3500\n",
            "4352/4352 [==============================] - 0s 15us/step - loss: 0.0186 - acc: 0.9943 - val_loss: 0.0611 - val_acc: 0.9902\n",
            "Epoch 3279/3500\n",
            "4352/4352 [==============================] - 0s 17us/step - loss: 0.0181 - acc: 0.9943 - val_loss: 0.0680 - val_acc: 0.9907\n",
            "Epoch 3280/3500\n",
            "4352/4352 [==============================] - 0s 13us/step - loss: 0.0153 - acc: 0.9959 - val_loss: 0.0737 - val_acc: 0.9897\n",
            "Epoch 3281/3500\n",
            "4352/4352 [==============================] - 0s 14us/step - loss: 0.0144 - acc: 0.9968 - val_loss: 0.0658 - val_acc: 0.9911\n",
            "Epoch 3282/3500\n",
            "4352/4352 [==============================] - 0s 14us/step - loss: 0.0156 - acc: 0.9956 - val_loss: 0.0699 - val_acc: 0.9897\n",
            "Epoch 3283/3500\n",
            "4352/4352 [==============================] - 0s 14us/step - loss: 0.0155 - acc: 0.9959 - val_loss: 0.0669 - val_acc: 0.9907\n",
            "Epoch 3284/3500\n",
            "4352/4352 [==============================] - 0s 16us/step - loss: 0.0150 - acc: 0.9963 - val_loss: 0.0609 - val_acc: 0.9916\n",
            "Epoch 3285/3500\n",
            "4352/4352 [==============================] - 0s 15us/step - loss: 0.0153 - acc: 0.9966 - val_loss: 0.0676 - val_acc: 0.9907\n",
            "Epoch 3286/3500\n",
            "4352/4352 [==============================] - 0s 13us/step - loss: 0.0141 - acc: 0.9966 - val_loss: 0.0705 - val_acc: 0.9897\n",
            "Epoch 3287/3500\n",
            "4352/4352 [==============================] - 0s 13us/step - loss: 0.0141 - acc: 0.9961 - val_loss: 0.0652 - val_acc: 0.9911\n",
            "Epoch 3288/3500\n",
            "4352/4352 [==============================] - 0s 13us/step - loss: 0.0147 - acc: 0.9966 - val_loss: 0.0631 - val_acc: 0.9916\n",
            "Epoch 3289/3500\n",
            "4352/4352 [==============================] - 0s 13us/step - loss: 0.0139 - acc: 0.9968 - val_loss: 0.0728 - val_acc: 0.9893\n",
            "Epoch 3290/3500\n",
            "4352/4352 [==============================] - 0s 14us/step - loss: 0.0155 - acc: 0.9963 - val_loss: 0.0755 - val_acc: 0.9897\n",
            "Epoch 3291/3500\n",
            "4352/4352 [==============================] - 0s 14us/step - loss: 0.0209 - acc: 0.9933 - val_loss: 0.0736 - val_acc: 0.9893\n",
            "Epoch 3292/3500\n",
            "4352/4352 [==============================] - 0s 14us/step - loss: 0.0152 - acc: 0.9961 - val_loss: 0.0671 - val_acc: 0.9907\n",
            "Epoch 3293/3500\n",
            "4352/4352 [==============================] - 0s 14us/step - loss: 0.0145 - acc: 0.9963 - val_loss: 0.0621 - val_acc: 0.9916\n",
            "Epoch 3294/3500\n",
            "4352/4352 [==============================] - 0s 15us/step - loss: 0.0148 - acc: 0.9956 - val_loss: 0.0714 - val_acc: 0.9897\n",
            "Epoch 3295/3500\n",
            "4352/4352 [==============================] - 0s 16us/step - loss: 0.0141 - acc: 0.9968 - val_loss: 0.0674 - val_acc: 0.9902\n",
            "Epoch 3296/3500\n",
            "4352/4352 [==============================] - 0s 14us/step - loss: 0.0141 - acc: 0.9968 - val_loss: 0.0676 - val_acc: 0.9897\n",
            "Epoch 3297/3500\n",
            "4352/4352 [==============================] - 0s 14us/step - loss: 0.0144 - acc: 0.9968 - val_loss: 0.0644 - val_acc: 0.9911\n",
            "Epoch 3298/3500\n",
            "4352/4352 [==============================] - 0s 13us/step - loss: 0.0147 - acc: 0.9968 - val_loss: 0.0665 - val_acc: 0.9907\n",
            "Epoch 3299/3500\n",
            "4352/4352 [==============================] - 0s 13us/step - loss: 0.0139 - acc: 0.9970 - val_loss: 0.0676 - val_acc: 0.9902\n",
            "Epoch 3300/3500\n",
            "4352/4352 [==============================] - 0s 15us/step - loss: 0.0144 - acc: 0.9970 - val_loss: 0.0630 - val_acc: 0.9916\n",
            "Epoch 3301/3500\n",
            "4352/4352 [==============================] - 0s 13us/step - loss: 0.0142 - acc: 0.9968 - val_loss: 0.0687 - val_acc: 0.9893\n",
            "Epoch 3302/3500\n",
            "4352/4352 [==============================] - 0s 14us/step - loss: 0.0141 - acc: 0.9961 - val_loss: 0.0650 - val_acc: 0.9902\n",
            "Epoch 3303/3500\n",
            "4352/4352 [==============================] - 0s 15us/step - loss: 0.0147 - acc: 0.9972 - val_loss: 0.0687 - val_acc: 0.9902\n",
            "Epoch 3304/3500\n",
            "4352/4352 [==============================] - 0s 13us/step - loss: 0.0145 - acc: 0.9961 - val_loss: 0.0676 - val_acc: 0.9902\n",
            "Epoch 3305/3500\n",
            "4352/4352 [==============================] - 0s 13us/step - loss: 0.0137 - acc: 0.9970 - val_loss: 0.0695 - val_acc: 0.9902\n",
            "Epoch 3306/3500\n",
            "4352/4352 [==============================] - 0s 13us/step - loss: 0.0138 - acc: 0.9970 - val_loss: 0.0605 - val_acc: 0.9907\n",
            "Epoch 3307/3500\n",
            "4352/4352 [==============================] - 0s 13us/step - loss: 0.0167 - acc: 0.9940 - val_loss: 0.0623 - val_acc: 0.9888\n",
            "Epoch 3308/3500\n",
            "4352/4352 [==============================] - 0s 14us/step - loss: 0.0204 - acc: 0.9931 - val_loss: 0.0620 - val_acc: 0.9911\n",
            "Epoch 3309/3500\n",
            "4352/4352 [==============================] - 0s 14us/step - loss: 0.0163 - acc: 0.9949 - val_loss: 0.0624 - val_acc: 0.9911\n",
            "Epoch 3310/3500\n",
            "4352/4352 [==============================] - 0s 13us/step - loss: 0.0151 - acc: 0.9963 - val_loss: 0.0607 - val_acc: 0.9911\n",
            "Epoch 3311/3500\n",
            "4352/4352 [==============================] - 0s 15us/step - loss: 0.0144 - acc: 0.9963 - val_loss: 0.0647 - val_acc: 0.9907\n",
            "Epoch 3312/3500\n",
            "4352/4352 [==============================] - 0s 16us/step - loss: 0.0146 - acc: 0.9963 - val_loss: 0.0654 - val_acc: 0.9902\n",
            "Epoch 3313/3500\n",
            "4352/4352 [==============================] - 0s 14us/step - loss: 0.0149 - acc: 0.9959 - val_loss: 0.0711 - val_acc: 0.9902\n",
            "Epoch 3314/3500\n",
            "4352/4352 [==============================] - 0s 13us/step - loss: 0.0145 - acc: 0.9963 - val_loss: 0.0686 - val_acc: 0.9902\n",
            "Epoch 3315/3500\n",
            "4352/4352 [==============================] - 0s 14us/step - loss: 0.0145 - acc: 0.9966 - val_loss: 0.0678 - val_acc: 0.9902\n",
            "Epoch 3316/3500\n",
            "4352/4352 [==============================] - 0s 15us/step - loss: 0.0134 - acc: 0.9966 - val_loss: 0.0640 - val_acc: 0.9907\n",
            "Epoch 3317/3500\n",
            "4352/4352 [==============================] - 0s 14us/step - loss: 0.0140 - acc: 0.9963 - val_loss: 0.0686 - val_acc: 0.9897\n",
            "Epoch 3318/3500\n",
            "4352/4352 [==============================] - 0s 14us/step - loss: 0.0172 - acc: 0.9940 - val_loss: 0.0710 - val_acc: 0.9893\n",
            "Epoch 3319/3500\n",
            "4352/4352 [==============================] - 0s 13us/step - loss: 0.0207 - acc: 0.9936 - val_loss: 0.0816 - val_acc: 0.9874\n",
            "Epoch 3320/3500\n",
            "4352/4352 [==============================] - 0s 13us/step - loss: 0.0208 - acc: 0.9931 - val_loss: 0.0637 - val_acc: 0.9916\n",
            "Epoch 3321/3500\n",
            "4352/4352 [==============================] - 0s 13us/step - loss: 0.0150 - acc: 0.9961 - val_loss: 0.0575 - val_acc: 0.9925\n",
            "Epoch 3322/3500\n",
            "4352/4352 [==============================] - 0s 13us/step - loss: 0.0152 - acc: 0.9963 - val_loss: 0.0580 - val_acc: 0.9916\n",
            "Epoch 3323/3500\n",
            "4352/4352 [==============================] - 0s 13us/step - loss: 0.0154 - acc: 0.9959 - val_loss: 0.0639 - val_acc: 0.9911\n",
            "Epoch 3324/3500\n",
            "4352/4352 [==============================] - 0s 14us/step - loss: 0.0154 - acc: 0.9954 - val_loss: 0.0596 - val_acc: 0.9921\n",
            "Epoch 3325/3500\n",
            "4352/4352 [==============================] - 0s 15us/step - loss: 0.0151 - acc: 0.9956 - val_loss: 0.0694 - val_acc: 0.9897\n",
            "Epoch 3326/3500\n",
            "4352/4352 [==============================] - 0s 14us/step - loss: 0.0146 - acc: 0.9966 - val_loss: 0.0713 - val_acc: 0.9897\n",
            "Epoch 3327/3500\n",
            "4352/4352 [==============================] - 0s 14us/step - loss: 0.0155 - acc: 0.9961 - val_loss: 0.0785 - val_acc: 0.9883\n",
            "Epoch 3328/3500\n",
            "4352/4352 [==============================] - 0s 17us/step - loss: 0.0178 - acc: 0.9949 - val_loss: 0.0779 - val_acc: 0.9883\n",
            "Epoch 3329/3500\n",
            "4352/4352 [==============================] - 0s 13us/step - loss: 0.0185 - acc: 0.9952 - val_loss: 0.0788 - val_acc: 0.9888\n",
            "Epoch 3330/3500\n",
            "4352/4352 [==============================] - 0s 14us/step - loss: 0.0157 - acc: 0.9961 - val_loss: 0.0617 - val_acc: 0.9911\n",
            "Epoch 3331/3500\n",
            "4352/4352 [==============================] - 0s 14us/step - loss: 0.0160 - acc: 0.9954 - val_loss: 0.0645 - val_acc: 0.9907\n",
            "Epoch 3332/3500\n",
            "4352/4352 [==============================] - 0s 14us/step - loss: 0.0162 - acc: 0.9959 - val_loss: 0.0604 - val_acc: 0.9907\n",
            "Epoch 3333/3500\n",
            "4352/4352 [==============================] - 0s 16us/step - loss: 0.0154 - acc: 0.9959 - val_loss: 0.0610 - val_acc: 0.9911\n",
            "Epoch 3334/3500\n",
            "4352/4352 [==============================] - 0s 15us/step - loss: 0.0157 - acc: 0.9961 - val_loss: 0.0644 - val_acc: 0.9907\n",
            "Epoch 3335/3500\n",
            "4352/4352 [==============================] - 0s 14us/step - loss: 0.0151 - acc: 0.9959 - val_loss: 0.0631 - val_acc: 0.9916\n",
            "Epoch 3336/3500\n",
            "4352/4352 [==============================] - 0s 12us/step - loss: 0.0164 - acc: 0.9954 - val_loss: 0.0691 - val_acc: 0.9893\n",
            "Epoch 3337/3500\n",
            "4352/4352 [==============================] - 0s 13us/step - loss: 0.0140 - acc: 0.9968 - val_loss: 0.0664 - val_acc: 0.9902\n",
            "Epoch 3338/3500\n",
            "4352/4352 [==============================] - 0s 14us/step - loss: 0.0144 - acc: 0.9961 - val_loss: 0.0650 - val_acc: 0.9911\n",
            "Epoch 3339/3500\n",
            "4352/4352 [==============================] - 0s 13us/step - loss: 0.0145 - acc: 0.9963 - val_loss: 0.0641 - val_acc: 0.9907\n",
            "Epoch 3340/3500\n",
            "4352/4352 [==============================] - 0s 13us/step - loss: 0.0147 - acc: 0.9961 - val_loss: 0.0770 - val_acc: 0.9883\n",
            "Epoch 3341/3500\n",
            "4352/4352 [==============================] - 0s 14us/step - loss: 0.0150 - acc: 0.9968 - val_loss: 0.0690 - val_acc: 0.9902\n",
            "Epoch 3342/3500\n",
            "4352/4352 [==============================] - 0s 14us/step - loss: 0.0151 - acc: 0.9963 - val_loss: 0.0628 - val_acc: 0.9916\n",
            "Epoch 3343/3500\n",
            "4352/4352 [==============================] - 0s 13us/step - loss: 0.0141 - acc: 0.9963 - val_loss: 0.0624 - val_acc: 0.9925\n",
            "Epoch 3344/3500\n",
            "4352/4352 [==============================] - 0s 12us/step - loss: 0.0151 - acc: 0.9961 - val_loss: 0.0591 - val_acc: 0.9907\n",
            "Epoch 3345/3500\n",
            "4352/4352 [==============================] - 0s 15us/step - loss: 0.0199 - acc: 0.9940 - val_loss: 0.0608 - val_acc: 0.9902\n",
            "Epoch 3346/3500\n",
            "4352/4352 [==============================] - 0s 12us/step - loss: 0.0210 - acc: 0.9936 - val_loss: 0.0598 - val_acc: 0.9911\n",
            "Epoch 3347/3500\n",
            "4352/4352 [==============================] - 0s 13us/step - loss: 0.0224 - acc: 0.9929 - val_loss: 0.0636 - val_acc: 0.9907\n",
            "Epoch 3348/3500\n",
            "4352/4352 [==============================] - 0s 13us/step - loss: 0.0235 - acc: 0.9926 - val_loss: 0.0611 - val_acc: 0.9897\n",
            "Epoch 3349/3500\n",
            "4352/4352 [==============================] - 0s 13us/step - loss: 0.0198 - acc: 0.9936 - val_loss: 0.0621 - val_acc: 0.9902\n",
            "Epoch 3350/3500\n",
            "4352/4352 [==============================] - 0s 14us/step - loss: 0.0237 - acc: 0.9931 - val_loss: 0.0857 - val_acc: 0.9869\n",
            "Epoch 3351/3500\n",
            "4352/4352 [==============================] - 0s 15us/step - loss: 0.0193 - acc: 0.9943 - val_loss: 0.0787 - val_acc: 0.9869\n",
            "Epoch 3352/3500\n",
            "4352/4352 [==============================] - 0s 14us/step - loss: 0.0154 - acc: 0.9966 - val_loss: 0.0619 - val_acc: 0.9930\n",
            "Epoch 3353/3500\n",
            "4352/4352 [==============================] - 0s 13us/step - loss: 0.0162 - acc: 0.9949 - val_loss: 0.0576 - val_acc: 0.9921\n",
            "Epoch 3354/3500\n",
            "4352/4352 [==============================] - 0s 14us/step - loss: 0.0211 - acc: 0.9938 - val_loss: 0.0619 - val_acc: 0.9916\n",
            "Epoch 3355/3500\n",
            "4352/4352 [==============================] - 0s 13us/step - loss: 0.0149 - acc: 0.9956 - val_loss: 0.0629 - val_acc: 0.9907\n",
            "Epoch 3356/3500\n",
            "4352/4352 [==============================] - 0s 14us/step - loss: 0.0168 - acc: 0.9952 - val_loss: 0.0611 - val_acc: 0.9925\n",
            "Epoch 3357/3500\n",
            "4352/4352 [==============================] - 0s 13us/step - loss: 0.0182 - acc: 0.9954 - val_loss: 0.0694 - val_acc: 0.9911\n",
            "Epoch 3358/3500\n",
            "4352/4352 [==============================] - 0s 13us/step - loss: 0.0175 - acc: 0.9952 - val_loss: 0.0807 - val_acc: 0.9874\n",
            "Epoch 3359/3500\n",
            "4352/4352 [==============================] - 0s 14us/step - loss: 0.0212 - acc: 0.9929 - val_loss: 0.0994 - val_acc: 0.9823\n",
            "Epoch 3360/3500\n",
            "4352/4352 [==============================] - 0s 13us/step - loss: 0.0249 - acc: 0.9922 - val_loss: 0.0803 - val_acc: 0.9879\n",
            "Epoch 3361/3500\n",
            "4352/4352 [==============================] - 0s 15us/step - loss: 0.0189 - acc: 0.9936 - val_loss: 0.0661 - val_acc: 0.9907\n",
            "Epoch 3362/3500\n",
            "4352/4352 [==============================] - 0s 14us/step - loss: 0.0178 - acc: 0.9952 - val_loss: 0.0611 - val_acc: 0.9907\n",
            "Epoch 3363/3500\n",
            "4352/4352 [==============================] - 0s 13us/step - loss: 0.0146 - acc: 0.9963 - val_loss: 0.0622 - val_acc: 0.9911\n",
            "Epoch 3364/3500\n",
            "4352/4352 [==============================] - 0s 15us/step - loss: 0.0138 - acc: 0.9972 - val_loss: 0.0689 - val_acc: 0.9916\n",
            "Epoch 3365/3500\n",
            "4352/4352 [==============================] - 0s 14us/step - loss: 0.0149 - acc: 0.9961 - val_loss: 0.0672 - val_acc: 0.9911\n",
            "Epoch 3366/3500\n",
            "4352/4352 [==============================] - 0s 13us/step - loss: 0.0154 - acc: 0.9956 - val_loss: 0.0741 - val_acc: 0.9907\n",
            "Epoch 3367/3500\n",
            "4352/4352 [==============================] - 0s 14us/step - loss: 0.0158 - acc: 0.9956 - val_loss: 0.0720 - val_acc: 0.9911\n",
            "Epoch 3368/3500\n",
            "4352/4352 [==============================] - 0s 13us/step - loss: 0.0151 - acc: 0.9956 - val_loss: 0.0658 - val_acc: 0.9907\n",
            "Epoch 3369/3500\n",
            "4352/4352 [==============================] - 0s 14us/step - loss: 0.0147 - acc: 0.9963 - val_loss: 0.0697 - val_acc: 0.9902\n",
            "Epoch 3370/3500\n",
            "4352/4352 [==============================] - 0s 13us/step - loss: 0.0149 - acc: 0.9961 - val_loss: 0.0696 - val_acc: 0.9911\n",
            "Epoch 3371/3500\n",
            "4352/4352 [==============================] - 0s 13us/step - loss: 0.0143 - acc: 0.9961 - val_loss: 0.0600 - val_acc: 0.9916\n",
            "Epoch 3372/3500\n",
            "4352/4352 [==============================] - 0s 13us/step - loss: 0.0166 - acc: 0.9956 - val_loss: 0.0603 - val_acc: 0.9911\n",
            "Epoch 3373/3500\n",
            "4352/4352 [==============================] - 0s 16us/step - loss: 0.0156 - acc: 0.9959 - val_loss: 0.0630 - val_acc: 0.9911\n",
            "Epoch 3374/3500\n",
            "4352/4352 [==============================] - 0s 16us/step - loss: 0.0158 - acc: 0.9961 - val_loss: 0.0631 - val_acc: 0.9921\n",
            "Epoch 3375/3500\n",
            "4352/4352 [==============================] - 0s 14us/step - loss: 0.0146 - acc: 0.9963 - val_loss: 0.0658 - val_acc: 0.9907\n",
            "Epoch 3376/3500\n",
            "4352/4352 [==============================] - 0s 13us/step - loss: 0.0149 - acc: 0.9961 - val_loss: 0.0732 - val_acc: 0.9897\n",
            "Epoch 3377/3500\n",
            "4352/4352 [==============================] - 0s 13us/step - loss: 0.0149 - acc: 0.9966 - val_loss: 0.0629 - val_acc: 0.9921\n",
            "Epoch 3378/3500\n",
            "4352/4352 [==============================] - 0s 16us/step - loss: 0.0142 - acc: 0.9963 - val_loss: 0.0603 - val_acc: 0.9916\n",
            "Epoch 3379/3500\n",
            "4352/4352 [==============================] - 0s 15us/step - loss: 0.0140 - acc: 0.9968 - val_loss: 0.0637 - val_acc: 0.9921\n",
            "Epoch 3380/3500\n",
            "4352/4352 [==============================] - 0s 15us/step - loss: 0.0143 - acc: 0.9961 - val_loss: 0.0778 - val_acc: 0.9879\n",
            "Epoch 3381/3500\n",
            "4352/4352 [==============================] - 0s 13us/step - loss: 0.0166 - acc: 0.9949 - val_loss: 0.0764 - val_acc: 0.9902\n",
            "Epoch 3382/3500\n",
            "4352/4352 [==============================] - 0s 14us/step - loss: 0.0152 - acc: 0.9961 - val_loss: 0.0621 - val_acc: 0.9921\n",
            "Epoch 3383/3500\n",
            "4352/4352 [==============================] - 0s 14us/step - loss: 0.0138 - acc: 0.9970 - val_loss: 0.0636 - val_acc: 0.9921\n",
            "Epoch 3384/3500\n",
            "4352/4352 [==============================] - 0s 14us/step - loss: 0.0137 - acc: 0.9968 - val_loss: 0.0651 - val_acc: 0.9911\n",
            "Epoch 3385/3500\n",
            "4352/4352 [==============================] - 0s 15us/step - loss: 0.0137 - acc: 0.9968 - val_loss: 0.0676 - val_acc: 0.9911\n",
            "Epoch 3386/3500\n",
            "4352/4352 [==============================] - 0s 14us/step - loss: 0.0138 - acc: 0.9970 - val_loss: 0.0635 - val_acc: 0.9916\n",
            "Epoch 3387/3500\n",
            "4352/4352 [==============================] - 0s 15us/step - loss: 0.0141 - acc: 0.9966 - val_loss: 0.0631 - val_acc: 0.9921\n",
            "Epoch 3388/3500\n",
            "4352/4352 [==============================] - 0s 15us/step - loss: 0.0153 - acc: 0.9959 - val_loss: 0.0615 - val_acc: 0.9925\n",
            "Epoch 3389/3500\n",
            "4352/4352 [==============================] - 0s 13us/step - loss: 0.0139 - acc: 0.9963 - val_loss: 0.0664 - val_acc: 0.9911\n",
            "Epoch 3390/3500\n",
            "4352/4352 [==============================] - 0s 13us/step - loss: 0.0163 - acc: 0.9956 - val_loss: 0.0725 - val_acc: 0.9893\n",
            "Epoch 3391/3500\n",
            "4352/4352 [==============================] - 0s 14us/step - loss: 0.0152 - acc: 0.9963 - val_loss: 0.0692 - val_acc: 0.9911\n",
            "Epoch 3392/3500\n",
            "4352/4352 [==============================] - 0s 14us/step - loss: 0.0147 - acc: 0.9961 - val_loss: 0.0623 - val_acc: 0.9921\n",
            "Epoch 3393/3500\n",
            "4352/4352 [==============================] - 0s 13us/step - loss: 0.0141 - acc: 0.9968 - val_loss: 0.0661 - val_acc: 0.9911\n",
            "Epoch 3394/3500\n",
            "4352/4352 [==============================] - 0s 16us/step - loss: 0.0142 - acc: 0.9961 - val_loss: 0.0676 - val_acc: 0.9911\n",
            "Epoch 3395/3500\n",
            "4352/4352 [==============================] - 0s 15us/step - loss: 0.0151 - acc: 0.9956 - val_loss: 0.0698 - val_acc: 0.9916\n",
            "Epoch 3396/3500\n",
            "4352/4352 [==============================] - 0s 25us/step - loss: 0.0153 - acc: 0.9961 - val_loss: 0.0669 - val_acc: 0.9902\n",
            "Epoch 3397/3500\n",
            "4352/4352 [==============================] - 0s 14us/step - loss: 0.0138 - acc: 0.9966 - val_loss: 0.0611 - val_acc: 0.9911\n",
            "Epoch 3398/3500\n",
            "4352/4352 [==============================] - 0s 14us/step - loss: 0.0175 - acc: 0.9952 - val_loss: 0.0635 - val_acc: 0.9888\n",
            "Epoch 3399/3500\n",
            "4352/4352 [==============================] - 0s 13us/step - loss: 0.0172 - acc: 0.9952 - val_loss: 0.0641 - val_acc: 0.9911\n",
            "Epoch 3400/3500\n",
            "4352/4352 [==============================] - 0s 14us/step - loss: 0.0150 - acc: 0.9956 - val_loss: 0.0688 - val_acc: 0.9907\n",
            "Epoch 3401/3500\n",
            "4352/4352 [==============================] - 0s 13us/step - loss: 0.0138 - acc: 0.9966 - val_loss: 0.0635 - val_acc: 0.9921\n",
            "Epoch 3402/3500\n",
            "4352/4352 [==============================] - 0s 14us/step - loss: 0.0168 - acc: 0.9949 - val_loss: 0.0668 - val_acc: 0.9902\n",
            "Epoch 3403/3500\n",
            "4352/4352 [==============================] - 0s 15us/step - loss: 0.0174 - acc: 0.9947 - val_loss: 0.0727 - val_acc: 0.9893\n",
            "Epoch 3404/3500\n",
            "4352/4352 [==============================] - 0s 14us/step - loss: 0.0197 - acc: 0.9936 - val_loss: 0.0916 - val_acc: 0.9860\n",
            "Epoch 3405/3500\n",
            "4352/4352 [==============================] - 0s 14us/step - loss: 0.0245 - acc: 0.9931 - val_loss: 0.0739 - val_acc: 0.9907\n",
            "Epoch 3406/3500\n",
            "4352/4352 [==============================] - 0s 13us/step - loss: 0.0204 - acc: 0.9945 - val_loss: 0.1056 - val_acc: 0.9809\n",
            "Epoch 3407/3500\n",
            "4352/4352 [==============================] - 0s 14us/step - loss: 0.0262 - acc: 0.9917 - val_loss: 0.1022 - val_acc: 0.9823\n",
            "Epoch 3408/3500\n",
            "4352/4352 [==============================] - 0s 13us/step - loss: 0.0189 - acc: 0.9938 - val_loss: 0.0728 - val_acc: 0.9893\n",
            "Epoch 3409/3500\n",
            "4352/4352 [==============================] - 0s 13us/step - loss: 0.0161 - acc: 0.9961 - val_loss: 0.0635 - val_acc: 0.9907\n",
            "Epoch 3410/3500\n",
            "4352/4352 [==============================] - 0s 15us/step - loss: 0.0168 - acc: 0.9949 - val_loss: 0.0649 - val_acc: 0.9897\n",
            "Epoch 3411/3500\n",
            "4352/4352 [==============================] - 0s 15us/step - loss: 0.0234 - acc: 0.9929 - val_loss: 0.0632 - val_acc: 0.9907\n",
            "Epoch 3412/3500\n",
            "4352/4352 [==============================] - 0s 14us/step - loss: 0.0192 - acc: 0.9938 - val_loss: 0.0736 - val_acc: 0.9902\n",
            "Epoch 3413/3500\n",
            "4352/4352 [==============================] - 0s 15us/step - loss: 0.0157 - acc: 0.9954 - val_loss: 0.0822 - val_acc: 0.9874\n",
            "Epoch 3414/3500\n",
            "4352/4352 [==============================] - 0s 14us/step - loss: 0.0175 - acc: 0.9940 - val_loss: 0.0647 - val_acc: 0.9911\n",
            "Epoch 3415/3500\n",
            "4352/4352 [==============================] - 0s 14us/step - loss: 0.0156 - acc: 0.9959 - val_loss: 0.0696 - val_acc: 0.9902\n",
            "Epoch 3416/3500\n",
            "4352/4352 [==============================] - 0s 15us/step - loss: 0.0153 - acc: 0.9966 - val_loss: 0.0652 - val_acc: 0.9911\n",
            "Epoch 3417/3500\n",
            "4352/4352 [==============================] - 0s 14us/step - loss: 0.0153 - acc: 0.9956 - val_loss: 0.0647 - val_acc: 0.9911\n",
            "Epoch 3418/3500\n",
            "4352/4352 [==============================] - 0s 15us/step - loss: 0.0145 - acc: 0.9963 - val_loss: 0.0719 - val_acc: 0.9902\n",
            "Epoch 3419/3500\n",
            "4352/4352 [==============================] - 0s 13us/step - loss: 0.0145 - acc: 0.9961 - val_loss: 0.0650 - val_acc: 0.9911\n",
            "Epoch 3420/3500\n",
            "4352/4352 [==============================] - 0s 21us/step - loss: 0.0143 - acc: 0.9963 - val_loss: 0.0627 - val_acc: 0.9907\n",
            "Epoch 3421/3500\n",
            "4352/4352 [==============================] - 0s 13us/step - loss: 0.0159 - acc: 0.9956 - val_loss: 0.0654 - val_acc: 0.9911\n",
            "Epoch 3422/3500\n",
            "4352/4352 [==============================] - 0s 13us/step - loss: 0.0145 - acc: 0.9961 - val_loss: 0.0665 - val_acc: 0.9911\n",
            "Epoch 3423/3500\n",
            "4352/4352 [==============================] - 0s 14us/step - loss: 0.0144 - acc: 0.9963 - val_loss: 0.0615 - val_acc: 0.9916\n",
            "Epoch 3424/3500\n",
            "4352/4352 [==============================] - 0s 13us/step - loss: 0.0198 - acc: 0.9933 - val_loss: 0.0657 - val_acc: 0.9911\n",
            "Epoch 3425/3500\n",
            "4352/4352 [==============================] - 0s 16us/step - loss: 0.0157 - acc: 0.9954 - val_loss: 0.0811 - val_acc: 0.9879\n",
            "Epoch 3426/3500\n",
            "4352/4352 [==============================] - 0s 15us/step - loss: 0.0155 - acc: 0.9954 - val_loss: 0.0704 - val_acc: 0.9902\n",
            "Epoch 3427/3500\n",
            "4352/4352 [==============================] - 0s 16us/step - loss: 0.0160 - acc: 0.9961 - val_loss: 0.0660 - val_acc: 0.9902\n",
            "Epoch 3428/3500\n",
            "4352/4352 [==============================] - 0s 13us/step - loss: 0.0145 - acc: 0.9966 - val_loss: 0.0673 - val_acc: 0.9911\n",
            "Epoch 3429/3500\n",
            "4352/4352 [==============================] - 0s 13us/step - loss: 0.0143 - acc: 0.9970 - val_loss: 0.0692 - val_acc: 0.9897\n",
            "Epoch 3430/3500\n",
            "4352/4352 [==============================] - 0s 14us/step - loss: 0.0138 - acc: 0.9966 - val_loss: 0.0664 - val_acc: 0.9911\n",
            "Epoch 3431/3500\n",
            "4352/4352 [==============================] - 0s 14us/step - loss: 0.0139 - acc: 0.9968 - val_loss: 0.0700 - val_acc: 0.9902\n",
            "Epoch 3432/3500\n",
            "4352/4352 [==============================] - 0s 14us/step - loss: 0.0142 - acc: 0.9968 - val_loss: 0.0653 - val_acc: 0.9907\n",
            "Epoch 3433/3500\n",
            "4352/4352 [==============================] - 0s 13us/step - loss: 0.0138 - acc: 0.9970 - val_loss: 0.0715 - val_acc: 0.9897\n",
            "Epoch 3434/3500\n",
            "4352/4352 [==============================] - 0s 14us/step - loss: 0.0135 - acc: 0.9968 - val_loss: 0.0659 - val_acc: 0.9911\n",
            "Epoch 3435/3500\n",
            "4352/4352 [==============================] - 0s 13us/step - loss: 0.0140 - acc: 0.9968 - val_loss: 0.0684 - val_acc: 0.9911\n",
            "Epoch 3436/3500\n",
            "4352/4352 [==============================] - 0s 15us/step - loss: 0.0139 - acc: 0.9966 - val_loss: 0.0722 - val_acc: 0.9897\n",
            "Epoch 3437/3500\n",
            "4352/4352 [==============================] - 0s 14us/step - loss: 0.0143 - acc: 0.9968 - val_loss: 0.0708 - val_acc: 0.9907\n",
            "Epoch 3438/3500\n",
            "4352/4352 [==============================] - 0s 13us/step - loss: 0.0155 - acc: 0.9961 - val_loss: 0.0759 - val_acc: 0.9888\n",
            "Epoch 3439/3500\n",
            "4352/4352 [==============================] - 0s 14us/step - loss: 0.0167 - acc: 0.9949 - val_loss: 0.0801 - val_acc: 0.9888\n",
            "Epoch 3440/3500\n",
            "4352/4352 [==============================] - 0s 12us/step - loss: 0.0147 - acc: 0.9961 - val_loss: 0.0627 - val_acc: 0.9911\n",
            "Epoch 3441/3500\n",
            "4352/4352 [==============================] - 0s 15us/step - loss: 0.0139 - acc: 0.9970 - val_loss: 0.0672 - val_acc: 0.9907\n",
            "Epoch 3442/3500\n",
            "4352/4352 [==============================] - 0s 15us/step - loss: 0.0139 - acc: 0.9963 - val_loss: 0.0717 - val_acc: 0.9902\n",
            "Epoch 3443/3500\n",
            "4352/4352 [==============================] - 0s 15us/step - loss: 0.0134 - acc: 0.9970 - val_loss: 0.0637 - val_acc: 0.9916\n",
            "Epoch 3444/3500\n",
            "4352/4352 [==============================] - 0s 18us/step - loss: 0.0141 - acc: 0.9966 - val_loss: 0.0655 - val_acc: 0.9916\n",
            "Epoch 3445/3500\n",
            "4352/4352 [==============================] - 0s 13us/step - loss: 0.0150 - acc: 0.9954 - val_loss: 0.0629 - val_acc: 0.9897\n",
            "Epoch 3446/3500\n",
            "4352/4352 [==============================] - 0s 14us/step - loss: 0.0154 - acc: 0.9954 - val_loss: 0.0638 - val_acc: 0.9907\n",
            "Epoch 3447/3500\n",
            "4352/4352 [==============================] - 0s 13us/step - loss: 0.0138 - acc: 0.9970 - val_loss: 0.0702 - val_acc: 0.9897\n",
            "Epoch 3448/3500\n",
            "4352/4352 [==============================] - 0s 17us/step - loss: 0.0142 - acc: 0.9966 - val_loss: 0.0690 - val_acc: 0.9897\n",
            "Epoch 3449/3500\n",
            "4352/4352 [==============================] - 0s 14us/step - loss: 0.0148 - acc: 0.9959 - val_loss: 0.0673 - val_acc: 0.9907\n",
            "Epoch 3450/3500\n",
            "4352/4352 [==============================] - 0s 16us/step - loss: 0.0140 - acc: 0.9968 - val_loss: 0.0701 - val_acc: 0.9907\n",
            "Epoch 3451/3500\n",
            "4352/4352 [==============================] - 0s 15us/step - loss: 0.0142 - acc: 0.9968 - val_loss: 0.0665 - val_acc: 0.9911\n",
            "Epoch 3452/3500\n",
            "4352/4352 [==============================] - 0s 15us/step - loss: 0.0135 - acc: 0.9972 - val_loss: 0.0708 - val_acc: 0.9893\n",
            "Epoch 3453/3500\n",
            "4352/4352 [==============================] - 0s 16us/step - loss: 0.0133 - acc: 0.9968 - val_loss: 0.0673 - val_acc: 0.9907\n",
            "Epoch 3454/3500\n",
            "4352/4352 [==============================] - 0s 13us/step - loss: 0.0134 - acc: 0.9968 - val_loss: 0.0691 - val_acc: 0.9902\n",
            "Epoch 3455/3500\n",
            "4352/4352 [==============================] - 0s 14us/step - loss: 0.0134 - acc: 0.9966 - val_loss: 0.0659 - val_acc: 0.9907\n",
            "Epoch 3456/3500\n",
            "4352/4352 [==============================] - 0s 15us/step - loss: 0.0142 - acc: 0.9968 - val_loss: 0.0651 - val_acc: 0.9911\n",
            "Epoch 3457/3500\n",
            "4352/4352 [==============================] - 0s 16us/step - loss: 0.0138 - acc: 0.9968 - val_loss: 0.0675 - val_acc: 0.9902\n",
            "Epoch 3458/3500\n",
            "4352/4352 [==============================] - 0s 13us/step - loss: 0.0136 - acc: 0.9970 - val_loss: 0.0639 - val_acc: 0.9911\n",
            "Epoch 3459/3500\n",
            "4352/4352 [==============================] - 0s 14us/step - loss: 0.0151 - acc: 0.9954 - val_loss: 0.0627 - val_acc: 0.9921\n",
            "Epoch 3460/3500\n",
            "4352/4352 [==============================] - 0s 14us/step - loss: 0.0195 - acc: 0.9947 - val_loss: 0.0644 - val_acc: 0.9902\n",
            "Epoch 3461/3500\n",
            "4352/4352 [==============================] - 0s 14us/step - loss: 0.0242 - acc: 0.9931 - val_loss: 0.0956 - val_acc: 0.9841\n",
            "Epoch 3462/3500\n",
            "4352/4352 [==============================] - 0s 14us/step - loss: 0.0229 - acc: 0.9924 - val_loss: 0.0831 - val_acc: 0.9883\n",
            "Epoch 3463/3500\n",
            "4352/4352 [==============================] - 0s 14us/step - loss: 0.0192 - acc: 0.9940 - val_loss: 0.0888 - val_acc: 0.9874\n",
            "Epoch 3464/3500\n",
            "4352/4352 [==============================] - 0s 14us/step - loss: 0.0215 - acc: 0.9936 - val_loss: 0.0744 - val_acc: 0.9897\n",
            "Epoch 3465/3500\n",
            "4352/4352 [==============================] - 0s 13us/step - loss: 0.0182 - acc: 0.9945 - val_loss: 0.0756 - val_acc: 0.9897\n",
            "Epoch 3466/3500\n",
            "4352/4352 [==============================] - 0s 14us/step - loss: 0.0157 - acc: 0.9959 - val_loss: 0.0662 - val_acc: 0.9916\n",
            "Epoch 3467/3500\n",
            "4352/4352 [==============================] - 0s 14us/step - loss: 0.0141 - acc: 0.9968 - val_loss: 0.0657 - val_acc: 0.9911\n",
            "Epoch 3468/3500\n",
            "4352/4352 [==============================] - 0s 14us/step - loss: 0.0140 - acc: 0.9968 - val_loss: 0.0703 - val_acc: 0.9897\n",
            "Epoch 3469/3500\n",
            "4352/4352 [==============================] - 0s 14us/step - loss: 0.0135 - acc: 0.9968 - val_loss: 0.0705 - val_acc: 0.9902\n",
            "Epoch 3470/3500\n",
            "4352/4352 [==============================] - 0s 14us/step - loss: 0.0140 - acc: 0.9968 - val_loss: 0.0696 - val_acc: 0.9902\n",
            "Epoch 3471/3500\n",
            "4352/4352 [==============================] - 0s 13us/step - loss: 0.0139 - acc: 0.9970 - val_loss: 0.0696 - val_acc: 0.9897\n",
            "Epoch 3472/3500\n",
            "4352/4352 [==============================] - 0s 13us/step - loss: 0.0145 - acc: 0.9966 - val_loss: 0.0693 - val_acc: 0.9897\n",
            "Epoch 3473/3500\n",
            "4352/4352 [==============================] - 0s 18us/step - loss: 0.0146 - acc: 0.9961 - val_loss: 0.0669 - val_acc: 0.9907\n",
            "Epoch 3474/3500\n",
            "4352/4352 [==============================] - 0s 14us/step - loss: 0.0144 - acc: 0.9963 - val_loss: 0.0754 - val_acc: 0.9897\n",
            "Epoch 3475/3500\n",
            "4352/4352 [==============================] - 0s 13us/step - loss: 0.0142 - acc: 0.9963 - val_loss: 0.0652 - val_acc: 0.9911\n",
            "Epoch 3476/3500\n",
            "4352/4352 [==============================] - 0s 13us/step - loss: 0.0142 - acc: 0.9966 - val_loss: 0.0691 - val_acc: 0.9902\n",
            "Epoch 3477/3500\n",
            "4352/4352 [==============================] - 0s 14us/step - loss: 0.0139 - acc: 0.9963 - val_loss: 0.0653 - val_acc: 0.9907\n",
            "Epoch 3478/3500\n",
            "4352/4352 [==============================] - 0s 14us/step - loss: 0.0139 - acc: 0.9972 - val_loss: 0.0641 - val_acc: 0.9907\n",
            "Epoch 3479/3500\n",
            "4352/4352 [==============================] - 0s 14us/step - loss: 0.0139 - acc: 0.9966 - val_loss: 0.0681 - val_acc: 0.9897\n",
            "Epoch 3480/3500\n",
            "4352/4352 [==============================] - 0s 14us/step - loss: 0.0137 - acc: 0.9966 - val_loss: 0.0672 - val_acc: 0.9907\n",
            "Epoch 3481/3500\n",
            "4352/4352 [==============================] - 0s 17us/step - loss: 0.0139 - acc: 0.9963 - val_loss: 0.0636 - val_acc: 0.9921\n",
            "Epoch 3482/3500\n",
            "4352/4352 [==============================] - 0s 15us/step - loss: 0.0153 - acc: 0.9954 - val_loss: 0.0657 - val_acc: 0.9911\n",
            "Epoch 3483/3500\n",
            "4352/4352 [==============================] - 0s 14us/step - loss: 0.0197 - acc: 0.9933 - val_loss: 0.0630 - val_acc: 0.9907\n",
            "Epoch 3484/3500\n",
            "4352/4352 [==============================] - 0s 14us/step - loss: 0.0258 - acc: 0.9924 - val_loss: 0.0680 - val_acc: 0.9893\n",
            "Epoch 3485/3500\n",
            "4352/4352 [==============================] - 0s 14us/step - loss: 0.0184 - acc: 0.9938 - val_loss: 0.0786 - val_acc: 0.9893\n",
            "Epoch 3486/3500\n",
            "4352/4352 [==============================] - 0s 14us/step - loss: 0.0198 - acc: 0.9947 - val_loss: 0.0967 - val_acc: 0.9828\n",
            "Epoch 3487/3500\n",
            "4352/4352 [==============================] - 0s 14us/step - loss: 0.0171 - acc: 0.9954 - val_loss: 0.0668 - val_acc: 0.9897\n",
            "Epoch 3488/3500\n",
            "4352/4352 [==============================] - 0s 14us/step - loss: 0.0148 - acc: 0.9963 - val_loss: 0.0621 - val_acc: 0.9911\n",
            "Epoch 3489/3500\n",
            "4352/4352 [==============================] - 0s 16us/step - loss: 0.0145 - acc: 0.9961 - val_loss: 0.0688 - val_acc: 0.9902\n",
            "Epoch 3490/3500\n",
            "4352/4352 [==============================] - 0s 16us/step - loss: 0.0137 - acc: 0.9963 - val_loss: 0.0713 - val_acc: 0.9897\n",
            "Epoch 3491/3500\n",
            "4352/4352 [==============================] - 0s 15us/step - loss: 0.0141 - acc: 0.9963 - val_loss: 0.0635 - val_acc: 0.9911\n",
            "Epoch 3492/3500\n",
            "4352/4352 [==============================] - 0s 13us/step - loss: 0.0144 - acc: 0.9961 - val_loss: 0.0634 - val_acc: 0.9911\n",
            "Epoch 3493/3500\n",
            "4352/4352 [==============================] - 0s 14us/step - loss: 0.0132 - acc: 0.9963 - val_loss: 0.0749 - val_acc: 0.9897\n",
            "Epoch 3494/3500\n",
            "4352/4352 [==============================] - 0s 14us/step - loss: 0.0141 - acc: 0.9966 - val_loss: 0.0735 - val_acc: 0.9897\n",
            "Epoch 3495/3500\n",
            "4352/4352 [==============================] - 0s 13us/step - loss: 0.0138 - acc: 0.9968 - val_loss: 0.0655 - val_acc: 0.9911\n",
            "Epoch 3496/3500\n",
            "4352/4352 [==============================] - 0s 13us/step - loss: 0.0138 - acc: 0.9966 - val_loss: 0.0628 - val_acc: 0.9916\n",
            "Epoch 3497/3500\n",
            "4352/4352 [==============================] - 0s 13us/step - loss: 0.0152 - acc: 0.9961 - val_loss: 0.0661 - val_acc: 0.9911\n",
            "Epoch 3498/3500\n",
            "4352/4352 [==============================] - 0s 15us/step - loss: 0.0139 - acc: 0.9968 - val_loss: 0.0746 - val_acc: 0.9897\n",
            "Epoch 3499/3500\n",
            "4352/4352 [==============================] - 0s 14us/step - loss: 0.0145 - acc: 0.9966 - val_loss: 0.0715 - val_acc: 0.9897\n",
            "Epoch 3500/3500\n",
            "4352/4352 [==============================] - 0s 15us/step - loss: 0.0141 - acc: 0.9968 - val_loss: 0.0697 - val_acc: 0.9893\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXQAAAD4CAYAAAD8Zh1EAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nIqFAFxGJhAJdRCQSCnQRkUj8P0tGiEVEUaDJAAAAAElFTkSuQmCC\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8Hr_vX5FpcdR",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "823fdf12-18cc-4ccb-978e-9731ad73bcfd"
      },
      "source": [
        "# 학습의 자동 중단 적용\n",
        "# 좀전의 학습이 도저히 끝날생각을 안한다(epoch 3500)\n",
        "# 학습이 진행될수록 학습셋의 정확도는 올라가지만 과적합으로 인해 테스트셋의 실험 결과는 점점 나빠지게 됩니다.\n",
        "# 케라스에는 학습이 진행되어도 테스트셋 오차가 줄지 않으면 학습을 멈추게 하는 함수가 있습니다.\n",
        "from keras.callbacks import EarlyStopping\n",
        "\n",
        "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "# 자동 중단 설정(EarlyStopping() 함수에 모니터할 값과 테스트 오차가 좋아지지 않아도 몇 번까지 기다릴지를 정합니다.)\n",
        "early_stopping_callback = EarlyStopping(monitor='val_loss', patience=100)\n",
        "# 모델 실행\n",
        "model.fit(X, Y, validation_split=0.2, epochs=2000, batch_size=500, callbacks=[early_stopping_callback])\n",
        "# 결과 출력\n",
        "print(\"\\n Accuracy: %.4f\" % (model.evaluate(X, Y)[1]))"
      ],
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train on 5197 samples, validate on 1300 samples\n",
            "Epoch 1/2000\n",
            "5197/5197 [==============================] - 1s 268us/step - loss: 0.0244 - acc: 0.9946 - val_loss: 0.0773 - val_acc: 0.9900\n",
            "Epoch 2/2000\n",
            "5197/5197 [==============================] - 0s 14us/step - loss: 0.0245 - acc: 0.9933 - val_loss: 0.0779 - val_acc: 0.9900\n",
            "Epoch 3/2000\n",
            "5197/5197 [==============================] - 0s 12us/step - loss: 0.0205 - acc: 0.9944 - val_loss: 0.0746 - val_acc: 0.9908\n",
            "Epoch 4/2000\n",
            "5197/5197 [==============================] - 0s 12us/step - loss: 0.0192 - acc: 0.9952 - val_loss: 0.0740 - val_acc: 0.9915\n",
            "Epoch 5/2000\n",
            "5197/5197 [==============================] - 0s 12us/step - loss: 0.0191 - acc: 0.9950 - val_loss: 0.0715 - val_acc: 0.9908\n",
            "Epoch 6/2000\n",
            "5197/5197 [==============================] - 0s 14us/step - loss: 0.0193 - acc: 0.9952 - val_loss: 0.0723 - val_acc: 0.9915\n",
            "Epoch 7/2000\n",
            "5197/5197 [==============================] - 0s 13us/step - loss: 0.0197 - acc: 0.9956 - val_loss: 0.0731 - val_acc: 0.9908\n",
            "Epoch 8/2000\n",
            "5197/5197 [==============================] - 0s 16us/step - loss: 0.0182 - acc: 0.9952 - val_loss: 0.0763 - val_acc: 0.9908\n",
            "Epoch 9/2000\n",
            "5197/5197 [==============================] - 0s 14us/step - loss: 0.0191 - acc: 0.9952 - val_loss: 0.0712 - val_acc: 0.9908\n",
            "Epoch 10/2000\n",
            "5197/5197 [==============================] - 0s 14us/step - loss: 0.0176 - acc: 0.9962 - val_loss: 0.0733 - val_acc: 0.9908\n",
            "Epoch 11/2000\n",
            "5197/5197 [==============================] - 0s 13us/step - loss: 0.0173 - acc: 0.9956 - val_loss: 0.0740 - val_acc: 0.9915\n",
            "Epoch 12/2000\n",
            "5197/5197 [==============================] - 0s 14us/step - loss: 0.0182 - acc: 0.9956 - val_loss: 0.0749 - val_acc: 0.9915\n",
            "Epoch 13/2000\n",
            "5197/5197 [==============================] - 0s 13us/step - loss: 0.0171 - acc: 0.9963 - val_loss: 0.0746 - val_acc: 0.9908\n",
            "Epoch 14/2000\n",
            "5197/5197 [==============================] - 0s 14us/step - loss: 0.0194 - acc: 0.9950 - val_loss: 0.0770 - val_acc: 0.9885\n",
            "Epoch 15/2000\n",
            "5197/5197 [==============================] - 0s 13us/step - loss: 0.0293 - acc: 0.9917 - val_loss: 0.0756 - val_acc: 0.9877\n",
            "Epoch 16/2000\n",
            "5197/5197 [==============================] - 0s 13us/step - loss: 0.0217 - acc: 0.9950 - val_loss: 0.0776 - val_acc: 0.9900\n",
            "Epoch 17/2000\n",
            "5197/5197 [==============================] - 0s 12us/step - loss: 0.0222 - acc: 0.9938 - val_loss: 0.0700 - val_acc: 0.9915\n",
            "Epoch 18/2000\n",
            "5197/5197 [==============================] - 0s 16us/step - loss: 0.0255 - acc: 0.9931 - val_loss: 0.0814 - val_acc: 0.9892\n",
            "Epoch 19/2000\n",
            "5197/5197 [==============================] - 0s 13us/step - loss: 0.0206 - acc: 0.9944 - val_loss: 0.0743 - val_acc: 0.9908\n",
            "Epoch 20/2000\n",
            "5197/5197 [==============================] - 0s 13us/step - loss: 0.0187 - acc: 0.9950 - val_loss: 0.0819 - val_acc: 0.9908\n",
            "Epoch 21/2000\n",
            "5197/5197 [==============================] - 0s 12us/step - loss: 0.0204 - acc: 0.9946 - val_loss: 0.0813 - val_acc: 0.9908\n",
            "Epoch 22/2000\n",
            "5197/5197 [==============================] - 0s 14us/step - loss: 0.0203 - acc: 0.9944 - val_loss: 0.0741 - val_acc: 0.9900\n",
            "Epoch 23/2000\n",
            "5197/5197 [==============================] - 0s 13us/step - loss: 0.0183 - acc: 0.9956 - val_loss: 0.0710 - val_acc: 0.9892\n",
            "Epoch 24/2000\n",
            "5197/5197 [==============================] - 0s 12us/step - loss: 0.0218 - acc: 0.9946 - val_loss: 0.0686 - val_acc: 0.9908\n",
            "Epoch 25/2000\n",
            "5197/5197 [==============================] - 0s 12us/step - loss: 0.0220 - acc: 0.9929 - val_loss: 0.0834 - val_acc: 0.9885\n",
            "Epoch 26/2000\n",
            "5197/5197 [==============================] - 0s 13us/step - loss: 0.0230 - acc: 0.9935 - val_loss: 0.0821 - val_acc: 0.9900\n",
            "Epoch 27/2000\n",
            "5197/5197 [==============================] - 0s 15us/step - loss: 0.0205 - acc: 0.9946 - val_loss: 0.0816 - val_acc: 0.9877\n",
            "Epoch 28/2000\n",
            "5197/5197 [==============================] - 0s 15us/step - loss: 0.0195 - acc: 0.9950 - val_loss: 0.0721 - val_acc: 0.9900\n",
            "Epoch 29/2000\n",
            "5197/5197 [==============================] - 0s 13us/step - loss: 0.0179 - acc: 0.9950 - val_loss: 0.0729 - val_acc: 0.9908\n",
            "Epoch 30/2000\n",
            "5197/5197 [==============================] - 0s 14us/step - loss: 0.0180 - acc: 0.9952 - val_loss: 0.0714 - val_acc: 0.9915\n",
            "Epoch 31/2000\n",
            "5197/5197 [==============================] - 0s 13us/step - loss: 0.0205 - acc: 0.9940 - val_loss: 0.0723 - val_acc: 0.9908\n",
            "Epoch 32/2000\n",
            "5197/5197 [==============================] - 0s 13us/step - loss: 0.0203 - acc: 0.9946 - val_loss: 0.0767 - val_acc: 0.9900\n",
            "Epoch 33/2000\n",
            "5197/5197 [==============================] - 0s 14us/step - loss: 0.0174 - acc: 0.9950 - val_loss: 0.0772 - val_acc: 0.9908\n",
            "Epoch 34/2000\n",
            "5197/5197 [==============================] - 0s 12us/step - loss: 0.0187 - acc: 0.9954 - val_loss: 0.0774 - val_acc: 0.9900\n",
            "Epoch 35/2000\n",
            "5197/5197 [==============================] - 0s 13us/step - loss: 0.0179 - acc: 0.9960 - val_loss: 0.0809 - val_acc: 0.9892\n",
            "Epoch 36/2000\n",
            "5197/5197 [==============================] - 0s 13us/step - loss: 0.0191 - acc: 0.9950 - val_loss: 0.0776 - val_acc: 0.9908\n",
            "Epoch 37/2000\n",
            "5197/5197 [==============================] - 0s 17us/step - loss: 0.0181 - acc: 0.9954 - val_loss: 0.0793 - val_acc: 0.9908\n",
            "Epoch 38/2000\n",
            "5197/5197 [==============================] - 0s 12us/step - loss: 0.0171 - acc: 0.9952 - val_loss: 0.0770 - val_acc: 0.9908\n",
            "Epoch 39/2000\n",
            "5197/5197 [==============================] - 0s 12us/step - loss: 0.0195 - acc: 0.9946 - val_loss: 0.0726 - val_acc: 0.9908\n",
            "Epoch 40/2000\n",
            "5197/5197 [==============================] - 0s 12us/step - loss: 0.0220 - acc: 0.9938 - val_loss: 0.0778 - val_acc: 0.9892\n",
            "Epoch 41/2000\n",
            "5197/5197 [==============================] - 0s 14us/step - loss: 0.0205 - acc: 0.9944 - val_loss: 0.0777 - val_acc: 0.9900\n",
            "Epoch 42/2000\n",
            "5197/5197 [==============================] - 0s 15us/step - loss: 0.0256 - acc: 0.9931 - val_loss: 0.0779 - val_acc: 0.9900\n",
            "Epoch 43/2000\n",
            "5197/5197 [==============================] - 0s 14us/step - loss: 0.0223 - acc: 0.9933 - val_loss: 0.0760 - val_acc: 0.9900\n",
            "Epoch 44/2000\n",
            "5197/5197 [==============================] - 0s 14us/step - loss: 0.0230 - acc: 0.9938 - val_loss: 0.0713 - val_acc: 0.9915\n",
            "Epoch 45/2000\n",
            "5197/5197 [==============================] - 0s 15us/step - loss: 0.0194 - acc: 0.9937 - val_loss: 0.0734 - val_acc: 0.9900\n",
            "Epoch 46/2000\n",
            "5197/5197 [==============================] - 0s 12us/step - loss: 0.0181 - acc: 0.9952 - val_loss: 0.0695 - val_acc: 0.9915\n",
            "Epoch 47/2000\n",
            "5197/5197 [==============================] - 0s 15us/step - loss: 0.0186 - acc: 0.9948 - val_loss: 0.0711 - val_acc: 0.9923\n",
            "Epoch 48/2000\n",
            "5197/5197 [==============================] - 0s 13us/step - loss: 0.0164 - acc: 0.9956 - val_loss: 0.0737 - val_acc: 0.9908\n",
            "Epoch 49/2000\n",
            "5197/5197 [==============================] - 0s 12us/step - loss: 0.0171 - acc: 0.9948 - val_loss: 0.0674 - val_acc: 0.9923\n",
            "Epoch 50/2000\n",
            "5197/5197 [==============================] - 0s 15us/step - loss: 0.0171 - acc: 0.9963 - val_loss: 0.0706 - val_acc: 0.9915\n",
            "Epoch 51/2000\n",
            "5197/5197 [==============================] - 0s 16us/step - loss: 0.0238 - acc: 0.9929 - val_loss: 0.0775 - val_acc: 0.9885\n",
            "Epoch 52/2000\n",
            "5197/5197 [==============================] - 0s 13us/step - loss: 0.0264 - acc: 0.9917 - val_loss: 0.0709 - val_acc: 0.9908\n",
            "Epoch 53/2000\n",
            "5197/5197 [==============================] - 0s 12us/step - loss: 0.0209 - acc: 0.9944 - val_loss: 0.0682 - val_acc: 0.9915\n",
            "Epoch 54/2000\n",
            "5197/5197 [==============================] - 0s 12us/step - loss: 0.0181 - acc: 0.9950 - val_loss: 0.0729 - val_acc: 0.9908\n",
            "Epoch 55/2000\n",
            "5197/5197 [==============================] - 0s 13us/step - loss: 0.0207 - acc: 0.9948 - val_loss: 0.0723 - val_acc: 0.9908\n",
            "Epoch 56/2000\n",
            "5197/5197 [==============================] - 0s 13us/step - loss: 0.0270 - acc: 0.9919 - val_loss: 0.0730 - val_acc: 0.9900\n",
            "Epoch 57/2000\n",
            "5197/5197 [==============================] - 0s 12us/step - loss: 0.0182 - acc: 0.9948 - val_loss: 0.0717 - val_acc: 0.9915\n",
            "Epoch 58/2000\n",
            "5197/5197 [==============================] - 0s 13us/step - loss: 0.0168 - acc: 0.9956 - val_loss: 0.0771 - val_acc: 0.9908\n",
            "Epoch 59/2000\n",
            "5197/5197 [==============================] - 0s 13us/step - loss: 0.0170 - acc: 0.9948 - val_loss: 0.0721 - val_acc: 0.9915\n",
            "Epoch 60/2000\n",
            "5197/5197 [==============================] - 0s 17us/step - loss: 0.0163 - acc: 0.9960 - val_loss: 0.0708 - val_acc: 0.9923\n",
            "Epoch 61/2000\n",
            "5197/5197 [==============================] - 0s 12us/step - loss: 0.0176 - acc: 0.9962 - val_loss: 0.0702 - val_acc: 0.9915\n",
            "Epoch 62/2000\n",
            "5197/5197 [==============================] - 0s 12us/step - loss: 0.0160 - acc: 0.9963 - val_loss: 0.0781 - val_acc: 0.9877\n",
            "Epoch 63/2000\n",
            "5197/5197 [==============================] - 0s 13us/step - loss: 0.0242 - acc: 0.9923 - val_loss: 0.0789 - val_acc: 0.9892\n",
            "Epoch 64/2000\n",
            "5197/5197 [==============================] - 0s 14us/step - loss: 0.0187 - acc: 0.9950 - val_loss: 0.0730 - val_acc: 0.9908\n",
            "Epoch 65/2000\n",
            "5197/5197 [==============================] - 0s 18us/step - loss: 0.0168 - acc: 0.9950 - val_loss: 0.0766 - val_acc: 0.9885\n",
            "Epoch 66/2000\n",
            "5197/5197 [==============================] - 0s 14us/step - loss: 0.0174 - acc: 0.9956 - val_loss: 0.0748 - val_acc: 0.9908\n",
            "Epoch 67/2000\n",
            "5197/5197 [==============================] - 0s 13us/step - loss: 0.0173 - acc: 0.9956 - val_loss: 0.0741 - val_acc: 0.9915\n",
            "Epoch 68/2000\n",
            "5197/5197 [==============================] - 0s 14us/step - loss: 0.0183 - acc: 0.9952 - val_loss: 0.0738 - val_acc: 0.9915\n",
            "Epoch 69/2000\n",
            "5197/5197 [==============================] - 0s 14us/step - loss: 0.0166 - acc: 0.9954 - val_loss: 0.0727 - val_acc: 0.9915\n",
            "Epoch 70/2000\n",
            "5197/5197 [==============================] - 0s 13us/step - loss: 0.0163 - acc: 0.9956 - val_loss: 0.0756 - val_acc: 0.9915\n",
            "Epoch 71/2000\n",
            "5197/5197 [==============================] - 0s 15us/step - loss: 0.0178 - acc: 0.9950 - val_loss: 0.0710 - val_acc: 0.9915\n",
            "Epoch 72/2000\n",
            "5197/5197 [==============================] - 0s 13us/step - loss: 0.0170 - acc: 0.9956 - val_loss: 0.0913 - val_acc: 0.9862\n",
            "Epoch 73/2000\n",
            "5197/5197 [==============================] - 0s 12us/step - loss: 0.0187 - acc: 0.9948 - val_loss: 0.0757 - val_acc: 0.9908\n",
            "Epoch 74/2000\n",
            "5197/5197 [==============================] - 0s 13us/step - loss: 0.0161 - acc: 0.9963 - val_loss: 0.0715 - val_acc: 0.9915\n",
            "Epoch 75/2000\n",
            "5197/5197 [==============================] - 0s 13us/step - loss: 0.0172 - acc: 0.9958 - val_loss: 0.0718 - val_acc: 0.9923\n",
            "Epoch 76/2000\n",
            "5197/5197 [==============================] - 0s 12us/step - loss: 0.0165 - acc: 0.9958 - val_loss: 0.0724 - val_acc: 0.9915\n",
            "Epoch 77/2000\n",
            "5197/5197 [==============================] - 0s 13us/step - loss: 0.0186 - acc: 0.9946 - val_loss: 0.0721 - val_acc: 0.9923\n",
            "Epoch 78/2000\n",
            "5197/5197 [==============================] - 0s 15us/step - loss: 0.0162 - acc: 0.9960 - val_loss: 0.0717 - val_acc: 0.9915\n",
            "Epoch 79/2000\n",
            "5197/5197 [==============================] - 0s 14us/step - loss: 0.0185 - acc: 0.9948 - val_loss: 0.0732 - val_acc: 0.9900\n",
            "Epoch 80/2000\n",
            "5197/5197 [==============================] - 0s 13us/step - loss: 0.0163 - acc: 0.9962 - val_loss: 0.0750 - val_acc: 0.9908\n",
            "Epoch 81/2000\n",
            "5197/5197 [==============================] - 0s 12us/step - loss: 0.0173 - acc: 0.9950 - val_loss: 0.0763 - val_acc: 0.9900\n",
            "Epoch 82/2000\n",
            "5197/5197 [==============================] - 0s 13us/step - loss: 0.0174 - acc: 0.9956 - val_loss: 0.0761 - val_acc: 0.9900\n",
            "Epoch 83/2000\n",
            "5197/5197 [==============================] - 0s 12us/step - loss: 0.0166 - acc: 0.9958 - val_loss: 0.0770 - val_acc: 0.9908\n",
            "Epoch 84/2000\n",
            "5197/5197 [==============================] - 0s 12us/step - loss: 0.0176 - acc: 0.9954 - val_loss: 0.0752 - val_acc: 0.9900\n",
            "Epoch 85/2000\n",
            "5197/5197 [==============================] - 0s 13us/step - loss: 0.0179 - acc: 0.9944 - val_loss: 0.0719 - val_acc: 0.9908\n",
            "Epoch 86/2000\n",
            "5197/5197 [==============================] - 0s 15us/step - loss: 0.0178 - acc: 0.9954 - val_loss: 0.0725 - val_acc: 0.9908\n",
            "Epoch 87/2000\n",
            "5197/5197 [==============================] - 0s 13us/step - loss: 0.0194 - acc: 0.9933 - val_loss: 0.0738 - val_acc: 0.9900\n",
            "Epoch 88/2000\n",
            "5197/5197 [==============================] - 0s 13us/step - loss: 0.0208 - acc: 0.9935 - val_loss: 0.0670 - val_acc: 0.9908\n",
            "Epoch 89/2000\n",
            "5197/5197 [==============================] - 0s 13us/step - loss: 0.0178 - acc: 0.9956 - val_loss: 0.0691 - val_acc: 0.9915\n",
            "Epoch 90/2000\n",
            "5197/5197 [==============================] - 0s 12us/step - loss: 0.0163 - acc: 0.9954 - val_loss: 0.0769 - val_acc: 0.9908\n",
            "Epoch 91/2000\n",
            "5197/5197 [==============================] - 0s 12us/step - loss: 0.0183 - acc: 0.9952 - val_loss: 0.0733 - val_acc: 0.9900\n",
            "Epoch 92/2000\n",
            "5197/5197 [==============================] - 0s 13us/step - loss: 0.0169 - acc: 0.9958 - val_loss: 0.0691 - val_acc: 0.9915\n",
            "Epoch 93/2000\n",
            "5197/5197 [==============================] - 0s 13us/step - loss: 0.0171 - acc: 0.9952 - val_loss: 0.0704 - val_acc: 0.9915\n",
            "Epoch 94/2000\n",
            "5197/5197 [==============================] - 0s 14us/step - loss: 0.0177 - acc: 0.9948 - val_loss: 0.0708 - val_acc: 0.9915\n",
            "Epoch 95/2000\n",
            "5197/5197 [==============================] - 0s 15us/step - loss: 0.0197 - acc: 0.9944 - val_loss: 0.0769 - val_acc: 0.9877\n",
            "Epoch 96/2000\n",
            "5197/5197 [==============================] - 0s 15us/step - loss: 0.0172 - acc: 0.9958 - val_loss: 0.0715 - val_acc: 0.9908\n",
            "Epoch 97/2000\n",
            "5197/5197 [==============================] - 0s 12us/step - loss: 0.0168 - acc: 0.9958 - val_loss: 0.0709 - val_acc: 0.9908\n",
            "Epoch 98/2000\n",
            "5197/5197 [==============================] - 0s 14us/step - loss: 0.0165 - acc: 0.9958 - val_loss: 0.0695 - val_acc: 0.9908\n",
            "Epoch 99/2000\n",
            "5197/5197 [==============================] - 0s 12us/step - loss: 0.0188 - acc: 0.9942 - val_loss: 0.0710 - val_acc: 0.9908\n",
            "Epoch 100/2000\n",
            "5197/5197 [==============================] - 0s 13us/step - loss: 0.0203 - acc: 0.9944 - val_loss: 0.0731 - val_acc: 0.9908\n",
            "Epoch 101/2000\n",
            "5197/5197 [==============================] - 0s 14us/step - loss: 0.0249 - acc: 0.9927 - val_loss: 0.0787 - val_acc: 0.9892\n",
            "Epoch 102/2000\n",
            "5197/5197 [==============================] - 0s 12us/step - loss: 0.0228 - acc: 0.9927 - val_loss: 0.0854 - val_acc: 0.9877\n",
            "Epoch 103/2000\n",
            "5197/5197 [==============================] - 0s 12us/step - loss: 0.0226 - acc: 0.9933 - val_loss: 0.0873 - val_acc: 0.9885\n",
            "Epoch 104/2000\n",
            "5197/5197 [==============================] - 0s 14us/step - loss: 0.0223 - acc: 0.9933 - val_loss: 0.0799 - val_acc: 0.9885\n",
            "Epoch 105/2000\n",
            "5197/5197 [==============================] - 0s 15us/step - loss: 0.0233 - acc: 0.9933 - val_loss: 0.0805 - val_acc: 0.9877\n",
            "Epoch 106/2000\n",
            "5197/5197 [==============================] - 0s 12us/step - loss: 0.0228 - acc: 0.9919 - val_loss: 0.0702 - val_acc: 0.9908\n",
            "Epoch 107/2000\n",
            "5197/5197 [==============================] - 0s 12us/step - loss: 0.0173 - acc: 0.9958 - val_loss: 0.0698 - val_acc: 0.9923\n",
            "Epoch 108/2000\n",
            "5197/5197 [==============================] - 0s 13us/step - loss: 0.0176 - acc: 0.9956 - val_loss: 0.0744 - val_acc: 0.9900\n",
            "Epoch 109/2000\n",
            "5197/5197 [==============================] - 0s 17us/step - loss: 0.0175 - acc: 0.9952 - val_loss: 0.0714 - val_acc: 0.9915\n",
            "Epoch 110/2000\n",
            "5197/5197 [==============================] - 0s 13us/step - loss: 0.0191 - acc: 0.9946 - val_loss: 0.0713 - val_acc: 0.9923\n",
            "Epoch 111/2000\n",
            "5197/5197 [==============================] - 0s 12us/step - loss: 0.0168 - acc: 0.9954 - val_loss: 0.0715 - val_acc: 0.9923\n",
            "Epoch 112/2000\n",
            "5197/5197 [==============================] - 0s 12us/step - loss: 0.0164 - acc: 0.9956 - val_loss: 0.0733 - val_acc: 0.9915\n",
            "Epoch 113/2000\n",
            "5197/5197 [==============================] - 0s 13us/step - loss: 0.0154 - acc: 0.9962 - val_loss: 0.0704 - val_acc: 0.9915\n",
            "Epoch 114/2000\n",
            "5197/5197 [==============================] - 0s 12us/step - loss: 0.0163 - acc: 0.9956 - val_loss: 0.0723 - val_acc: 0.9908\n",
            "Epoch 115/2000\n",
            "5197/5197 [==============================] - 0s 13us/step - loss: 0.0167 - acc: 0.9954 - val_loss: 0.0700 - val_acc: 0.9915\n",
            "Epoch 116/2000\n",
            "5197/5197 [==============================] - 0s 14us/step - loss: 0.0156 - acc: 0.9963 - val_loss: 0.0697 - val_acc: 0.9915\n",
            "Epoch 117/2000\n",
            "5197/5197 [==============================] - 0s 13us/step - loss: 0.0160 - acc: 0.9958 - val_loss: 0.0720 - val_acc: 0.9908\n",
            "Epoch 118/2000\n",
            "5197/5197 [==============================] - 0s 14us/step - loss: 0.0165 - acc: 0.9952 - val_loss: 0.0701 - val_acc: 0.9923\n",
            "Epoch 119/2000\n",
            "5197/5197 [==============================] - 0s 11us/step - loss: 0.0184 - acc: 0.9956 - val_loss: 0.0710 - val_acc: 0.9908\n",
            "Epoch 120/2000\n",
            "5197/5197 [==============================] - 0s 15us/step - loss: 0.0206 - acc: 0.9942 - val_loss: 0.0742 - val_acc: 0.9908\n",
            "Epoch 121/2000\n",
            "5197/5197 [==============================] - 0s 12us/step - loss: 0.0170 - acc: 0.9952 - val_loss: 0.0722 - val_acc: 0.9908\n",
            "Epoch 122/2000\n",
            "5197/5197 [==============================] - 0s 12us/step - loss: 0.0164 - acc: 0.9954 - val_loss: 0.0701 - val_acc: 0.9915\n",
            "Epoch 123/2000\n",
            "5197/5197 [==============================] - 0s 16us/step - loss: 0.0219 - acc: 0.9937 - val_loss: 0.0728 - val_acc: 0.9908\n",
            "Epoch 124/2000\n",
            "5197/5197 [==============================] - 0s 15us/step - loss: 0.0207 - acc: 0.9935 - val_loss: 0.0786 - val_acc: 0.9885\n",
            "Epoch 125/2000\n",
            "5197/5197 [==============================] - 0s 13us/step - loss: 0.0236 - acc: 0.9933 - val_loss: 0.0790 - val_acc: 0.9892\n",
            "Epoch 126/2000\n",
            "5197/5197 [==============================] - 0s 12us/step - loss: 0.0210 - acc: 0.9935 - val_loss: 0.0732 - val_acc: 0.9915\n",
            "Epoch 127/2000\n",
            "5197/5197 [==============================] - 0s 13us/step - loss: 0.0160 - acc: 0.9960 - val_loss: 0.0722 - val_acc: 0.9915\n",
            "Epoch 128/2000\n",
            "5197/5197 [==============================] - 0s 13us/step - loss: 0.0230 - acc: 0.9935 - val_loss: 0.0713 - val_acc: 0.9915\n",
            "Epoch 129/2000\n",
            "5197/5197 [==============================] - 0s 13us/step - loss: 0.0192 - acc: 0.9942 - val_loss: 0.0742 - val_acc: 0.9908\n",
            "Epoch 130/2000\n",
            "5197/5197 [==============================] - 0s 14us/step - loss: 0.0182 - acc: 0.9948 - val_loss: 0.0703 - val_acc: 0.9915\n",
            "Epoch 131/2000\n",
            "5197/5197 [==============================] - 0s 12us/step - loss: 0.0162 - acc: 0.9958 - val_loss: 0.0747 - val_acc: 0.9892\n",
            "Epoch 132/2000\n",
            "5197/5197 [==============================] - 0s 13us/step - loss: 0.0184 - acc: 0.9946 - val_loss: 0.0724 - val_acc: 0.9915\n",
            "Epoch 133/2000\n",
            "5197/5197 [==============================] - 0s 13us/step - loss: 0.0181 - acc: 0.9948 - val_loss: 0.0712 - val_acc: 0.9908\n",
            "Epoch 134/2000\n",
            "5197/5197 [==============================] - 0s 14us/step - loss: 0.0193 - acc: 0.9940 - val_loss: 0.0707 - val_acc: 0.9915\n",
            "Epoch 135/2000\n",
            "5197/5197 [==============================] - 0s 13us/step - loss: 0.0163 - acc: 0.9960 - val_loss: 0.0729 - val_acc: 0.9908\n",
            "Epoch 136/2000\n",
            "5197/5197 [==============================] - 0s 12us/step - loss: 0.0174 - acc: 0.9960 - val_loss: 0.0703 - val_acc: 0.9915\n",
            "Epoch 137/2000\n",
            "5197/5197 [==============================] - 0s 15us/step - loss: 0.0160 - acc: 0.9958 - val_loss: 0.0715 - val_acc: 0.9908\n",
            "Epoch 138/2000\n",
            "5197/5197 [==============================] - 0s 15us/step - loss: 0.0157 - acc: 0.9962 - val_loss: 0.0721 - val_acc: 0.9915\n",
            "Epoch 139/2000\n",
            "5197/5197 [==============================] - 0s 14us/step - loss: 0.0156 - acc: 0.9962 - val_loss: 0.0688 - val_acc: 0.9923\n",
            "Epoch 140/2000\n",
            "5197/5197 [==============================] - 0s 12us/step - loss: 0.0160 - acc: 0.9952 - val_loss: 0.0713 - val_acc: 0.9915\n",
            "Epoch 141/2000\n",
            "5197/5197 [==============================] - 0s 12us/step - loss: 0.0172 - acc: 0.9956 - val_loss: 0.0719 - val_acc: 0.9915\n",
            "Epoch 142/2000\n",
            "5197/5197 [==============================] - 0s 14us/step - loss: 0.0170 - acc: 0.9962 - val_loss: 0.0695 - val_acc: 0.9923\n",
            "Epoch 143/2000\n",
            "5197/5197 [==============================] - 0s 14us/step - loss: 0.0159 - acc: 0.9958 - val_loss: 0.0695 - val_acc: 0.9915\n",
            "Epoch 144/2000\n",
            "5197/5197 [==============================] - 0s 13us/step - loss: 0.0157 - acc: 0.9963 - val_loss: 0.0704 - val_acc: 0.9915\n",
            "Epoch 145/2000\n",
            "5197/5197 [==============================] - 0s 14us/step - loss: 0.0156 - acc: 0.9958 - val_loss: 0.0699 - val_acc: 0.9915\n",
            "Epoch 146/2000\n",
            "5197/5197 [==============================] - 0s 13us/step - loss: 0.0162 - acc: 0.9960 - val_loss: 0.0718 - val_acc: 0.9908\n",
            "Epoch 147/2000\n",
            "5197/5197 [==============================] - 0s 13us/step - loss: 0.0165 - acc: 0.9958 - val_loss: 0.0745 - val_acc: 0.9900\n",
            "Epoch 148/2000\n",
            "5197/5197 [==============================] - 0s 14us/step - loss: 0.0161 - acc: 0.9950 - val_loss: 0.0714 - val_acc: 0.9908\n",
            "Epoch 149/2000\n",
            "5197/5197 [==============================] - 0s 13us/step - loss: 0.0189 - acc: 0.9952 - val_loss: 0.0707 - val_acc: 0.9915\n",
            "Epoch 150/2000\n",
            "5197/5197 [==============================] - 0s 13us/step - loss: 0.0207 - acc: 0.9942 - val_loss: 0.0726 - val_acc: 0.9908\n",
            "Epoch 151/2000\n",
            "5197/5197 [==============================] - 0s 13us/step - loss: 0.0201 - acc: 0.9942 - val_loss: 0.0708 - val_acc: 0.9915\n",
            "Epoch 152/2000\n",
            "5197/5197 [==============================] - 0s 14us/step - loss: 0.0169 - acc: 0.9958 - val_loss: 0.0710 - val_acc: 0.9915\n",
            "Epoch 153/2000\n",
            "5197/5197 [==============================] - 0s 13us/step - loss: 0.0177 - acc: 0.9950 - val_loss: 0.0744 - val_acc: 0.9900\n",
            "Epoch 154/2000\n",
            "5197/5197 [==============================] - 0s 13us/step - loss: 0.0166 - acc: 0.9954 - val_loss: 0.0715 - val_acc: 0.9908\n",
            "Epoch 155/2000\n",
            "5197/5197 [==============================] - 0s 15us/step - loss: 0.0172 - acc: 0.9956 - val_loss: 0.0693 - val_acc: 0.9931\n",
            "Epoch 156/2000\n",
            "5197/5197 [==============================] - 0s 13us/step - loss: 0.0163 - acc: 0.9954 - val_loss: 0.0705 - val_acc: 0.9915\n",
            "Epoch 157/2000\n",
            "5197/5197 [==============================] - 0s 13us/step - loss: 0.0177 - acc: 0.9956 - val_loss: 0.0694 - val_acc: 0.9931\n",
            "Epoch 158/2000\n",
            "5197/5197 [==============================] - 0s 14us/step - loss: 0.0188 - acc: 0.9942 - val_loss: 0.0732 - val_acc: 0.9892\n",
            "Epoch 159/2000\n",
            "5197/5197 [==============================] - 0s 12us/step - loss: 0.0205 - acc: 0.9944 - val_loss: 0.0738 - val_acc: 0.9915\n",
            "Epoch 160/2000\n",
            "5197/5197 [==============================] - 0s 13us/step - loss: 0.0217 - acc: 0.9931 - val_loss: 0.0717 - val_acc: 0.9915\n",
            "Epoch 161/2000\n",
            "5197/5197 [==============================] - 0s 14us/step - loss: 0.0178 - acc: 0.9950 - val_loss: 0.0713 - val_acc: 0.9908\n",
            "Epoch 162/2000\n",
            "5197/5197 [==============================] - 0s 12us/step - loss: 0.0161 - acc: 0.9958 - val_loss: 0.0750 - val_acc: 0.9900\n",
            "Epoch 163/2000\n",
            "5197/5197 [==============================] - 0s 12us/step - loss: 0.0180 - acc: 0.9952 - val_loss: 0.0764 - val_acc: 0.9908\n",
            "Epoch 164/2000\n",
            "5197/5197 [==============================] - 0s 12us/step - loss: 0.0174 - acc: 0.9948 - val_loss: 0.0726 - val_acc: 0.9908\n",
            "Epoch 165/2000\n",
            "5197/5197 [==============================] - 0s 14us/step - loss: 0.0168 - acc: 0.9950 - val_loss: 0.0700 - val_acc: 0.9915\n",
            "Epoch 166/2000\n",
            "5197/5197 [==============================] - 0s 13us/step - loss: 0.0162 - acc: 0.9960 - val_loss: 0.0714 - val_acc: 0.9915\n",
            "Epoch 167/2000\n",
            "5197/5197 [==============================] - 0s 15us/step - loss: 0.0161 - acc: 0.9956 - val_loss: 0.0735 - val_acc: 0.9908\n",
            "Epoch 168/2000\n",
            "5197/5197 [==============================] - 0s 14us/step - loss: 0.0154 - acc: 0.9952 - val_loss: 0.0704 - val_acc: 0.9908\n",
            "Epoch 169/2000\n",
            "5197/5197 [==============================] - 0s 12us/step - loss: 0.0166 - acc: 0.9958 - val_loss: 0.0712 - val_acc: 0.9923\n",
            "Epoch 170/2000\n",
            "5197/5197 [==============================] - 0s 12us/step - loss: 0.0164 - acc: 0.9952 - val_loss: 0.0726 - val_acc: 0.9908\n",
            "Epoch 171/2000\n",
            "5197/5197 [==============================] - 0s 12us/step - loss: 0.0162 - acc: 0.9956 - val_loss: 0.0748 - val_acc: 0.9900\n",
            "Epoch 172/2000\n",
            "5197/5197 [==============================] - 0s 14us/step - loss: 0.0168 - acc: 0.9950 - val_loss: 0.0695 - val_acc: 0.9915\n",
            "Epoch 173/2000\n",
            "5197/5197 [==============================] - 0s 13us/step - loss: 0.0159 - acc: 0.9962 - val_loss: 0.0698 - val_acc: 0.9923\n",
            "Epoch 174/2000\n",
            "5197/5197 [==============================] - 0s 13us/step - loss: 0.0155 - acc: 0.9956 - val_loss: 0.0703 - val_acc: 0.9915\n",
            "Epoch 175/2000\n",
            "5197/5197 [==============================] - 0s 14us/step - loss: 0.0154 - acc: 0.9963 - val_loss: 0.0728 - val_acc: 0.9908\n",
            "Epoch 176/2000\n",
            "5197/5197 [==============================] - 0s 15us/step - loss: 0.0162 - acc: 0.9960 - val_loss: 0.0695 - val_acc: 0.9923\n",
            "Epoch 177/2000\n",
            "5197/5197 [==============================] - 0s 14us/step - loss: 0.0166 - acc: 0.9958 - val_loss: 0.0691 - val_acc: 0.9931\n",
            "Epoch 178/2000\n",
            "5197/5197 [==============================] - 0s 13us/step - loss: 0.0164 - acc: 0.9960 - val_loss: 0.0708 - val_acc: 0.9915\n",
            "Epoch 179/2000\n",
            "5197/5197 [==============================] - 0s 14us/step - loss: 0.0167 - acc: 0.9954 - val_loss: 0.0711 - val_acc: 0.9923\n",
            "Epoch 180/2000\n",
            "5197/5197 [==============================] - 0s 14us/step - loss: 0.0164 - acc: 0.9958 - val_loss: 0.0717 - val_acc: 0.9908\n",
            "Epoch 181/2000\n",
            "5197/5197 [==============================] - 0s 15us/step - loss: 0.0162 - acc: 0.9960 - val_loss: 0.0705 - val_acc: 0.9923\n",
            "Epoch 182/2000\n",
            "5197/5197 [==============================] - 0s 15us/step - loss: 0.0156 - acc: 0.9958 - val_loss: 0.0707 - val_acc: 0.9915\n",
            "Epoch 183/2000\n",
            "5197/5197 [==============================] - 0s 13us/step - loss: 0.0162 - acc: 0.9960 - val_loss: 0.0705 - val_acc: 0.9931\n",
            "Epoch 184/2000\n",
            "5197/5197 [==============================] - 0s 14us/step - loss: 0.0160 - acc: 0.9958 - val_loss: 0.0707 - val_acc: 0.9908\n",
            "Epoch 185/2000\n",
            "5197/5197 [==============================] - 0s 14us/step - loss: 0.0174 - acc: 0.9952 - val_loss: 0.0760 - val_acc: 0.9892\n",
            "Epoch 186/2000\n",
            "5197/5197 [==============================] - 0s 13us/step - loss: 0.0170 - acc: 0.9948 - val_loss: 0.0758 - val_acc: 0.9900\n",
            "Epoch 187/2000\n",
            "5197/5197 [==============================] - 0s 14us/step - loss: 0.0181 - acc: 0.9950 - val_loss: 0.0693 - val_acc: 0.9923\n",
            "Epoch 188/2000\n",
            "5197/5197 [==============================] - 0s 12us/step - loss: 0.0186 - acc: 0.9944 - val_loss: 0.0725 - val_acc: 0.9915\n",
            "6497/6497 [==============================] - 0s 59us/step\n",
            "\n",
            " Accuracy: 0.9949\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zryuj6K5qr9y",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "'''\n",
        "선형 회귀 적용- 보스턴 집값 예측 분석\n",
        " 1978년, 집값에 가장 큰 영향을 미치는 것이 ‘깨끗한 공기’라는 연구 결과가 하버드대학교 도시개발학과에\n",
        "서 발표\n",
        " 집값의 변동에 영향을 미치는 여러 가지 요인을 모아서 환경과 집값의 변동을 보여주는 데이터셋 - 머신\n",
        "러닝의 선형 회귀를 테스트하는 가장 유명한 데이터로 쓰이고 있음\n",
        " 수치를 예측하는 문제 (선형 회귀 문제)\n",
        " 머신러닝 혹은 딥러닝을 위해 주어진 데이터의 답을 구하는 문제 (여러 개 중에 정답을 맞히거나, 가격, 성\n",
        "적 같은 수치를 맞히는 것)\n",
        " 506 entries, 14 columns (13개의 속성과 1개의 클래스로 이루어졌음)\n",
        "'''\n",
        "'''\n",
        "CRIM     인구 1인당 범죄 발생 수\n",
        "ZN       25,000평방 피트 이상의 주거 구역 비중\n",
        "INDUS    소매업 외 상업이 차지하는 면적 비율\n",
        "CHAS     찰스강 위치 변수(1: 강 주변, 0: 이외)\n",
        "NOX      일산화질소 농도\n",
        "RM       집의 평균 방 수\n",
        "AGE      1940년 이전에 지어진 비율\n",
        "DIS      5가지 보스턴 시 고용 시설까지의 거리\n",
        "RAD      순환고속도로의 접근 용이성\n",
        "TAX      $10,000당 부동산 세율 총계\n",
        "PTRATIO  지역별 학생과 교사 비율\n",
        "B        지역별 흑인 비율\n",
        "LSTAT    급여가 낮은 직업에 종사하는 인구 비율(%)\n",
        "         가격(단위: $1,000)\n",
        "'''\n",
        "'''\n",
        " 선형 회귀 데이터는 마지막에 참과 거짓을 구분할 필요가 없기 때문에 출력층에 활성화 함수를 지정할 필\n",
        "요도 없습니다.\n",
        " 모델의 학습이 어느 정도 되었는지 확인하기 위해 예측 값과 실제 값을 비교하는 부분을 추가합니다.\n",
        "'''\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense\n",
        "from sklearn.model_selection import train_test_split\n",
        "import numpy\n",
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "\n",
        "seed = 0\n",
        "numpy.random.seed(seed) # seed 값 설정\n",
        "tf.set_random_seed(seed)\n",
        "\n",
        "df = pd.read_csv(\"/content/drive/My Drive/Colab Notebooks/싸이킷런 머신러닝(박태정T)/data/housing.csv\", delim_whitespace=True, header=None)\n",
        "dataset = df.values\n",
        "X = dataset[:,0:13]\n",
        "Y = dataset[:,13]\n",
        "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.3, random_state=seed)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8L4sU_x8tUqv",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "2df91ce1-2512-439f-da28-db3269ee93ec"
      },
      "source": [
        "# 모델 설정 \n",
        "model = Sequential()\n",
        "model.add(Dense(30, input_dim=13, activation='relu'))\n",
        "model.add(Dense(6, activation='relu'))\n",
        "model.add(Dense(1))\n",
        "\n",
        "model.compile(loss='mean_squared_error', optimizer='adam')\n",
        "model.fit(X_train, Y_train, epochs=200, batch_size=10)\n",
        "#flatten() - 데이터 배열이 몇 차원이든 모두 1차원으로 바꿔 읽기 쉽게 해 주는 함수\n",
        "Y_prediction = model.predict(X_test).flatten() # 예측 값과 실제 값의 비교\n",
        "for i in range(10):\n",
        "    label = Y_test[i]\n",
        "    prediction = Y_prediction[i]\n",
        "    print(\"실제가격: {:.3f}, 예상가격: {:.3f}\".format(label, prediction))\n",
        "\n"
      ],
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/200\n",
            "354/354 [==============================] - 1s 4ms/step - loss: 73.7544\n",
            "Epoch 2/200\n",
            "354/354 [==============================] - 0s 453us/step - loss: 63.4390\n",
            "Epoch 3/200\n",
            "354/354 [==============================] - 0s 418us/step - loss: 57.9993\n",
            "Epoch 4/200\n",
            "354/354 [==============================] - 0s 474us/step - loss: 53.2910\n",
            "Epoch 5/200\n",
            "354/354 [==============================] - 0s 455us/step - loss: 57.2137\n",
            "Epoch 6/200\n",
            "354/354 [==============================] - 0s 396us/step - loss: 53.4243\n",
            "Epoch 7/200\n",
            "354/354 [==============================] - 0s 399us/step - loss: 52.6130\n",
            "Epoch 8/200\n",
            "354/354 [==============================] - 0s 408us/step - loss: 48.5036\n",
            "Epoch 9/200\n",
            "354/354 [==============================] - 0s 427us/step - loss: 51.5769\n",
            "Epoch 10/200\n",
            "354/354 [==============================] - 0s 411us/step - loss: 47.3230\n",
            "Epoch 11/200\n",
            "354/354 [==============================] - 0s 415us/step - loss: 44.7591\n",
            "Epoch 12/200\n",
            "354/354 [==============================] - 0s 438us/step - loss: 44.7434\n",
            "Epoch 13/200\n",
            "354/354 [==============================] - 0s 428us/step - loss: 43.6435\n",
            "Epoch 14/200\n",
            "354/354 [==============================] - 0s 427us/step - loss: 42.2954\n",
            "Epoch 15/200\n",
            "354/354 [==============================] - 0s 432us/step - loss: 44.0695\n",
            "Epoch 16/200\n",
            "354/354 [==============================] - 0s 432us/step - loss: 41.9559\n",
            "Epoch 17/200\n",
            "354/354 [==============================] - 0s 413us/step - loss: 41.6789\n",
            "Epoch 18/200\n",
            "354/354 [==============================] - 0s 418us/step - loss: 39.0278\n",
            "Epoch 19/200\n",
            "354/354 [==============================] - 0s 476us/step - loss: 40.9333\n",
            "Epoch 20/200\n",
            "354/354 [==============================] - 0s 398us/step - loss: 41.7625\n",
            "Epoch 21/200\n",
            "354/354 [==============================] - 0s 449us/step - loss: 37.6276\n",
            "Epoch 22/200\n",
            "354/354 [==============================] - 0s 395us/step - loss: 34.9003\n",
            "Epoch 23/200\n",
            "354/354 [==============================] - 0s 435us/step - loss: 34.0182\n",
            "Epoch 24/200\n",
            "354/354 [==============================] - 0s 445us/step - loss: 34.4206\n",
            "Epoch 25/200\n",
            "354/354 [==============================] - 0s 440us/step - loss: 34.5517\n",
            "Epoch 26/200\n",
            "354/354 [==============================] - 0s 467us/step - loss: 36.9311\n",
            "Epoch 27/200\n",
            "354/354 [==============================] - 0s 415us/step - loss: 36.6692\n",
            "Epoch 28/200\n",
            "354/354 [==============================] - 0s 468us/step - loss: 35.7562\n",
            "Epoch 29/200\n",
            "354/354 [==============================] - 0s 407us/step - loss: 32.3834\n",
            "Epoch 30/200\n",
            "354/354 [==============================] - 0s 424us/step - loss: 32.1085\n",
            "Epoch 31/200\n",
            "354/354 [==============================] - 0s 445us/step - loss: 31.4843\n",
            "Epoch 32/200\n",
            "354/354 [==============================] - 0s 524us/step - loss: 31.6759\n",
            "Epoch 33/200\n",
            "354/354 [==============================] - 0s 400us/step - loss: 30.9924\n",
            "Epoch 34/200\n",
            "354/354 [==============================] - 0s 404us/step - loss: 33.7697\n",
            "Epoch 35/200\n",
            "354/354 [==============================] - 0s 423us/step - loss: 34.1075\n",
            "Epoch 36/200\n",
            "354/354 [==============================] - 0s 398us/step - loss: 30.6725\n",
            "Epoch 37/200\n",
            "354/354 [==============================] - 0s 417us/step - loss: 32.0419\n",
            "Epoch 38/200\n",
            "354/354 [==============================] - 0s 452us/step - loss: 30.4924\n",
            "Epoch 39/200\n",
            "354/354 [==============================] - 0s 495us/step - loss: 29.8982\n",
            "Epoch 40/200\n",
            "354/354 [==============================] - 0s 415us/step - loss: 30.0462\n",
            "Epoch 41/200\n",
            "354/354 [==============================] - 0s 416us/step - loss: 29.1039\n",
            "Epoch 42/200\n",
            "354/354 [==============================] - 0s 409us/step - loss: 29.4973\n",
            "Epoch 43/200\n",
            "354/354 [==============================] - 0s 443us/step - loss: 29.3906\n",
            "Epoch 44/200\n",
            "354/354 [==============================] - 0s 415us/step - loss: 28.9693\n",
            "Epoch 45/200\n",
            "354/354 [==============================] - 0s 481us/step - loss: 29.9006\n",
            "Epoch 46/200\n",
            "354/354 [==============================] - 0s 422us/step - loss: 29.1846\n",
            "Epoch 47/200\n",
            "354/354 [==============================] - 0s 425us/step - loss: 33.1674\n",
            "Epoch 48/200\n",
            "354/354 [==============================] - 0s 451us/step - loss: 29.4800\n",
            "Epoch 49/200\n",
            "354/354 [==============================] - 0s 444us/step - loss: 28.5148\n",
            "Epoch 50/200\n",
            "354/354 [==============================] - 0s 412us/step - loss: 29.2042\n",
            "Epoch 51/200\n",
            "354/354 [==============================] - 0s 455us/step - loss: 29.4609\n",
            "Epoch 52/200\n",
            "354/354 [==============================] - 0s 488us/step - loss: 28.3241\n",
            "Epoch 53/200\n",
            "354/354 [==============================] - 0s 418us/step - loss: 27.7966\n",
            "Epoch 54/200\n",
            "354/354 [==============================] - 0s 420us/step - loss: 28.0136\n",
            "Epoch 55/200\n",
            "354/354 [==============================] - 0s 459us/step - loss: 28.4587\n",
            "Epoch 56/200\n",
            "354/354 [==============================] - 0s 436us/step - loss: 30.0244\n",
            "Epoch 57/200\n",
            "354/354 [==============================] - 0s 400us/step - loss: 27.7590\n",
            "Epoch 58/200\n",
            "354/354 [==============================] - 0s 425us/step - loss: 27.9640\n",
            "Epoch 59/200\n",
            "354/354 [==============================] - 0s 446us/step - loss: 28.0204\n",
            "Epoch 60/200\n",
            "354/354 [==============================] - 0s 414us/step - loss: 28.4161\n",
            "Epoch 61/200\n",
            "354/354 [==============================] - 0s 428us/step - loss: 28.4322\n",
            "Epoch 62/200\n",
            "354/354 [==============================] - 0s 398us/step - loss: 28.0967\n",
            "Epoch 63/200\n",
            "354/354 [==============================] - 0s 438us/step - loss: 28.2922\n",
            "Epoch 64/200\n",
            "354/354 [==============================] - 0s 432us/step - loss: 28.5221\n",
            "Epoch 65/200\n",
            "354/354 [==============================] - 0s 487us/step - loss: 26.8604\n",
            "Epoch 66/200\n",
            "354/354 [==============================] - 0s 507us/step - loss: 26.1104\n",
            "Epoch 67/200\n",
            "354/354 [==============================] - 0s 458us/step - loss: 26.7527\n",
            "Epoch 68/200\n",
            "354/354 [==============================] - 0s 465us/step - loss: 26.6137\n",
            "Epoch 69/200\n",
            "354/354 [==============================] - 0s 434us/step - loss: 25.8369\n",
            "Epoch 70/200\n",
            "354/354 [==============================] - 0s 439us/step - loss: 24.9761\n",
            "Epoch 71/200\n",
            "354/354 [==============================] - 0s 465us/step - loss: 25.9647\n",
            "Epoch 72/200\n",
            "354/354 [==============================] - 0s 453us/step - loss: 24.9950\n",
            "Epoch 73/200\n",
            "354/354 [==============================] - 0s 603us/step - loss: 26.8489\n",
            "Epoch 74/200\n",
            "354/354 [==============================] - 0s 454us/step - loss: 24.8152\n",
            "Epoch 75/200\n",
            "354/354 [==============================] - 0s 431us/step - loss: 25.1502\n",
            "Epoch 76/200\n",
            "354/354 [==============================] - 0s 410us/step - loss: 23.9210\n",
            "Epoch 77/200\n",
            "354/354 [==============================] - 0s 437us/step - loss: 24.7186\n",
            "Epoch 78/200\n",
            "354/354 [==============================] - 0s 447us/step - loss: 24.1349\n",
            "Epoch 79/200\n",
            "354/354 [==============================] - 0s 483us/step - loss: 24.1915\n",
            "Epoch 80/200\n",
            "354/354 [==============================] - 0s 447us/step - loss: 23.9818\n",
            "Epoch 81/200\n",
            "354/354 [==============================] - 0s 396us/step - loss: 23.6467\n",
            "Epoch 82/200\n",
            "354/354 [==============================] - 0s 425us/step - loss: 24.1361\n",
            "Epoch 83/200\n",
            "354/354 [==============================] - 0s 429us/step - loss: 22.8347\n",
            "Epoch 84/200\n",
            "354/354 [==============================] - 0s 532us/step - loss: 25.3436\n",
            "Epoch 85/200\n",
            "354/354 [==============================] - 0s 443us/step - loss: 22.3126\n",
            "Epoch 86/200\n",
            "354/354 [==============================] - 0s 430us/step - loss: 21.2248\n",
            "Epoch 87/200\n",
            "354/354 [==============================] - 0s 428us/step - loss: 22.8337\n",
            "Epoch 88/200\n",
            "354/354 [==============================] - 0s 426us/step - loss: 20.7354\n",
            "Epoch 89/200\n",
            "354/354 [==============================] - 0s 450us/step - loss: 21.1781\n",
            "Epoch 90/200\n",
            "354/354 [==============================] - 0s 413us/step - loss: 20.9747\n",
            "Epoch 91/200\n",
            "354/354 [==============================] - 0s 449us/step - loss: 20.7938\n",
            "Epoch 92/200\n",
            "354/354 [==============================] - 0s 435us/step - loss: 23.3185\n",
            "Epoch 93/200\n",
            "354/354 [==============================] - 0s 430us/step - loss: 22.0229\n",
            "Epoch 94/200\n",
            "354/354 [==============================] - 0s 432us/step - loss: 21.1102\n",
            "Epoch 95/200\n",
            "354/354 [==============================] - 0s 420us/step - loss: 25.1986\n",
            "Epoch 96/200\n",
            "354/354 [==============================] - 0s 424us/step - loss: 19.3721\n",
            "Epoch 97/200\n",
            "354/354 [==============================] - 0s 451us/step - loss: 18.9752\n",
            "Epoch 98/200\n",
            "354/354 [==============================] - 0s 413us/step - loss: 22.4950\n",
            "Epoch 99/200\n",
            "354/354 [==============================] - 0s 441us/step - loss: 20.3537\n",
            "Epoch 100/200\n",
            "354/354 [==============================] - 0s 432us/step - loss: 20.2115\n",
            "Epoch 101/200\n",
            "354/354 [==============================] - 0s 396us/step - loss: 19.0313\n",
            "Epoch 102/200\n",
            "354/354 [==============================] - 0s 403us/step - loss: 18.1949\n",
            "Epoch 103/200\n",
            "354/354 [==============================] - 0s 427us/step - loss: 19.2051\n",
            "Epoch 104/200\n",
            "354/354 [==============================] - 0s 472us/step - loss: 26.0229\n",
            "Epoch 105/200\n",
            "354/354 [==============================] - 0s 431us/step - loss: 18.3451\n",
            "Epoch 106/200\n",
            "354/354 [==============================] - 0s 444us/step - loss: 17.9597\n",
            "Epoch 107/200\n",
            "354/354 [==============================] - 0s 434us/step - loss: 18.1660\n",
            "Epoch 108/200\n",
            "354/354 [==============================] - 0s 410us/step - loss: 18.6438\n",
            "Epoch 109/200\n",
            "354/354 [==============================] - 0s 423us/step - loss: 18.0002\n",
            "Epoch 110/200\n",
            "354/354 [==============================] - 0s 432us/step - loss: 20.6030\n",
            "Epoch 111/200\n",
            "354/354 [==============================] - 0s 437us/step - loss: 19.0841\n",
            "Epoch 112/200\n",
            "354/354 [==============================] - 0s 453us/step - loss: 17.4202\n",
            "Epoch 113/200\n",
            "354/354 [==============================] - 0s 448us/step - loss: 17.9703\n",
            "Epoch 114/200\n",
            "354/354 [==============================] - 0s 409us/step - loss: 19.7120\n",
            "Epoch 115/200\n",
            "354/354 [==============================] - 0s 416us/step - loss: 19.8909\n",
            "Epoch 116/200\n",
            "354/354 [==============================] - 0s 452us/step - loss: 16.6626\n",
            "Epoch 117/200\n",
            "354/354 [==============================] - 0s 459us/step - loss: 16.9465\n",
            "Epoch 118/200\n",
            "354/354 [==============================] - 0s 413us/step - loss: 18.1059\n",
            "Epoch 119/200\n",
            "354/354 [==============================] - 0s 428us/step - loss: 18.2125\n",
            "Epoch 120/200\n",
            "354/354 [==============================] - 0s 410us/step - loss: 18.2254\n",
            "Epoch 121/200\n",
            "354/354 [==============================] - 0s 408us/step - loss: 17.7318\n",
            "Epoch 122/200\n",
            "354/354 [==============================] - 0s 471us/step - loss: 18.5783\n",
            "Epoch 123/200\n",
            "354/354 [==============================] - 0s 398us/step - loss: 16.1749\n",
            "Epoch 124/200\n",
            "354/354 [==============================] - 0s 450us/step - loss: 17.3020\n",
            "Epoch 125/200\n",
            "354/354 [==============================] - 0s 417us/step - loss: 16.6184\n",
            "Epoch 126/200\n",
            "354/354 [==============================] - 0s 462us/step - loss: 17.8334\n",
            "Epoch 127/200\n",
            "354/354 [==============================] - 0s 443us/step - loss: 16.4057\n",
            "Epoch 128/200\n",
            "354/354 [==============================] - 0s 418us/step - loss: 15.9322\n",
            "Epoch 129/200\n",
            "354/354 [==============================] - 0s 418us/step - loss: 15.7286\n",
            "Epoch 130/200\n",
            "354/354 [==============================] - 0s 436us/step - loss: 15.8821\n",
            "Epoch 131/200\n",
            "354/354 [==============================] - 0s 483us/step - loss: 18.3416\n",
            "Epoch 132/200\n",
            "354/354 [==============================] - 0s 436us/step - loss: 17.3811\n",
            "Epoch 133/200\n",
            "354/354 [==============================] - 0s 402us/step - loss: 16.7032\n",
            "Epoch 134/200\n",
            "354/354 [==============================] - 0s 437us/step - loss: 16.4369\n",
            "Epoch 135/200\n",
            "354/354 [==============================] - 0s 407us/step - loss: 18.5134\n",
            "Epoch 136/200\n",
            "354/354 [==============================] - 0s 448us/step - loss: 15.3955\n",
            "Epoch 137/200\n",
            "354/354 [==============================] - 0s 533us/step - loss: 16.6782\n",
            "Epoch 138/200\n",
            "354/354 [==============================] - 0s 412us/step - loss: 17.1431\n",
            "Epoch 139/200\n",
            "354/354 [==============================] - 0s 438us/step - loss: 15.5836\n",
            "Epoch 140/200\n",
            "354/354 [==============================] - 0s 432us/step - loss: 15.7452\n",
            "Epoch 141/200\n",
            "354/354 [==============================] - 0s 435us/step - loss: 16.7211\n",
            "Epoch 142/200\n",
            "354/354 [==============================] - 0s 425us/step - loss: 15.4251\n",
            "Epoch 143/200\n",
            "354/354 [==============================] - 0s 424us/step - loss: 16.0068\n",
            "Epoch 144/200\n",
            "354/354 [==============================] - 0s 445us/step - loss: 14.8199\n",
            "Epoch 145/200\n",
            "354/354 [==============================] - 0s 463us/step - loss: 16.5440\n",
            "Epoch 146/200\n",
            "354/354 [==============================] - 0s 463us/step - loss: 15.8984\n",
            "Epoch 147/200\n",
            "354/354 [==============================] - 0s 436us/step - loss: 17.5028\n",
            "Epoch 148/200\n",
            "354/354 [==============================] - 0s 436us/step - loss: 15.8400\n",
            "Epoch 149/200\n",
            "354/354 [==============================] - 0s 416us/step - loss: 15.1989\n",
            "Epoch 150/200\n",
            "354/354 [==============================] - 0s 448us/step - loss: 17.1360\n",
            "Epoch 151/200\n",
            "354/354 [==============================] - 0s 398us/step - loss: 16.0161\n",
            "Epoch 152/200\n",
            "354/354 [==============================] - 0s 422us/step - loss: 15.9152\n",
            "Epoch 153/200\n",
            "354/354 [==============================] - 0s 415us/step - loss: 16.2745\n",
            "Epoch 154/200\n",
            "354/354 [==============================] - 0s 435us/step - loss: 16.1832\n",
            "Epoch 155/200\n",
            "354/354 [==============================] - 0s 405us/step - loss: 17.7479\n",
            "Epoch 156/200\n",
            "354/354 [==============================] - 0s 400us/step - loss: 16.3617\n",
            "Epoch 157/200\n",
            "354/354 [==============================] - 0s 483us/step - loss: 15.0121\n",
            "Epoch 158/200\n",
            "354/354 [==============================] - 0s 453us/step - loss: 16.5675\n",
            "Epoch 159/200\n",
            "354/354 [==============================] - 0s 400us/step - loss: 14.9477\n",
            "Epoch 160/200\n",
            "354/354 [==============================] - 0s 427us/step - loss: 14.7928\n",
            "Epoch 161/200\n",
            "354/354 [==============================] - 0s 478us/step - loss: 14.6672\n",
            "Epoch 162/200\n",
            "354/354 [==============================] - 0s 429us/step - loss: 15.2558\n",
            "Epoch 163/200\n",
            "354/354 [==============================] - 0s 425us/step - loss: 14.9345\n",
            "Epoch 164/200\n",
            "354/354 [==============================] - 0s 449us/step - loss: 17.2290\n",
            "Epoch 165/200\n",
            "354/354 [==============================] - 0s 409us/step - loss: 19.2503\n",
            "Epoch 166/200\n",
            "354/354 [==============================] - 0s 443us/step - loss: 15.2462\n",
            "Epoch 167/200\n",
            "354/354 [==============================] - 0s 425us/step - loss: 16.5213\n",
            "Epoch 168/200\n",
            "354/354 [==============================] - 0s 409us/step - loss: 15.2294\n",
            "Epoch 169/200\n",
            "354/354 [==============================] - 0s 426us/step - loss: 15.1211\n",
            "Epoch 170/200\n",
            "354/354 [==============================] - 0s 448us/step - loss: 17.2377\n",
            "Epoch 171/200\n",
            "354/354 [==============================] - 0s 420us/step - loss: 15.9851\n",
            "Epoch 172/200\n",
            "354/354 [==============================] - 0s 399us/step - loss: 15.7160\n",
            "Epoch 173/200\n",
            "354/354 [==============================] - 0s 492us/step - loss: 16.9995\n",
            "Epoch 174/200\n",
            "354/354 [==============================] - 0s 436us/step - loss: 15.2764\n",
            "Epoch 175/200\n",
            "354/354 [==============================] - 0s 449us/step - loss: 14.6813\n",
            "Epoch 176/200\n",
            "354/354 [==============================] - 0s 436us/step - loss: 14.2125\n",
            "Epoch 177/200\n",
            "354/354 [==============================] - 0s 445us/step - loss: 14.4485\n",
            "Epoch 178/200\n",
            "354/354 [==============================] - 0s 429us/step - loss: 14.8154\n",
            "Epoch 179/200\n",
            "354/354 [==============================] - 0s 429us/step - loss: 14.7145\n",
            "Epoch 180/200\n",
            "354/354 [==============================] - 0s 446us/step - loss: 17.9243\n",
            "Epoch 181/200\n",
            "354/354 [==============================] - 0s 489us/step - loss: 14.7163\n",
            "Epoch 182/200\n",
            "354/354 [==============================] - 0s 414us/step - loss: 14.7483\n",
            "Epoch 183/200\n",
            "354/354 [==============================] - 0s 478us/step - loss: 15.3389\n",
            "Epoch 184/200\n",
            "354/354 [==============================] - 0s 443us/step - loss: 14.3139\n",
            "Epoch 185/200\n",
            "354/354 [==============================] - 0s 434us/step - loss: 14.7593\n",
            "Epoch 186/200\n",
            "354/354 [==============================] - 0s 396us/step - loss: 15.0257\n",
            "Epoch 187/200\n",
            "354/354 [==============================] - 0s 413us/step - loss: 14.2592\n",
            "Epoch 188/200\n",
            "354/354 [==============================] - 0s 444us/step - loss: 14.5074\n",
            "Epoch 189/200\n",
            "354/354 [==============================] - 0s 434us/step - loss: 15.9253\n",
            "Epoch 190/200\n",
            "354/354 [==============================] - 0s 449us/step - loss: 13.9178\n",
            "Epoch 191/200\n",
            "354/354 [==============================] - 0s 477us/step - loss: 14.3513\n",
            "Epoch 192/200\n",
            "354/354 [==============================] - 0s 446us/step - loss: 14.0660\n",
            "Epoch 193/200\n",
            "354/354 [==============================] - 0s 401us/step - loss: 15.3890\n",
            "Epoch 194/200\n",
            "354/354 [==============================] - 0s 404us/step - loss: 15.3255\n",
            "Epoch 195/200\n",
            "354/354 [==============================] - 0s 400us/step - loss: 14.4819\n",
            "Epoch 196/200\n",
            "354/354 [==============================] - 0s 454us/step - loss: 15.7834\n",
            "Epoch 197/200\n",
            "354/354 [==============================] - 0s 456us/step - loss: 15.4966\n",
            "Epoch 198/200\n",
            "354/354 [==============================] - 0s 401us/step - loss: 14.4939\n",
            "Epoch 199/200\n",
            "354/354 [==============================] - 0s 425us/step - loss: 14.9785\n",
            "Epoch 200/200\n",
            "354/354 [==============================] - 0s 450us/step - loss: 14.1825\n",
            "실제가격: 22.600, 예상가격: 23.275\n",
            "실제가격: 50.000, 예상가격: 23.157\n",
            "실제가격: 23.000, 예상가격: 21.161\n",
            "실제가격: 8.300, 예상가격: 12.008\n",
            "실제가격: 21.200, 예상가격: 20.838\n",
            "실제가격: 19.900, 예상가격: 21.675\n",
            "실제가격: 20.600, 예상가격: 19.724\n",
            "실제가격: 18.700, 예상가격: 22.408\n",
            "실제가격: 16.100, 예상가격: 18.419\n",
            "실제가격: 18.600, 예상가격: 11.687\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}